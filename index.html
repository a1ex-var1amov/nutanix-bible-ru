<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta charset="UTF-8"> 
    <meta charset="utf-8">
    <title>Библия Nutanix</title>
    <meta name="description" content="Библия Nutanix - Перевод детального описания архитектуры Nutanix за авторством Стивена Пойтраса, о том как работает ПО и его функции, а также как использовать это для получения максимальной производительности."/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="keywords" content="nutanix, nutanix bible, nutanix architecture, prism, acropolis, nutanix openstack, webscale"/>
    <meta name="robots" content="index, follow"/>

    <!-- Open Graph data -->
    <meta property="og:title" content="Библия Nutanix - NutanixBible.ru"/>
    <meta property="og:locale" content="ru_RU"/>
    <meta property="og:type" content="website"/>
    <meta property="og:description" content="Библия Nutanix - Перевод детального описания архитектуры Nutanix за авторством Стивена Пойтраса, о том как работает ПО и его функции, а также как использовать это для получения максимальной производительности."/>
    <meta property="og:url" content="http://NutanixBible.ru"/>
    <meta property="og:site_name" content="NutanixBible.ru"/>
    <meta property="og:image" content="http://nutanixbible.ru/assets/Bible.png"/>

    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:url" content="http://NutanixBible.ru"/>
    <meta name="twitter:description" content="Библия Nutanix - Перевод детального описания архитектуры Nutanix за авторством Стивена Пойтраса, о том как работает ПО и его функции, а также как использовать это для получения максимальной производительности."/>
    <meta name="twitter:title" content="Библия Nutanix - NutanixBible.ru"/>
    <meta name="twitter:site" content="@a1ex_var1amov"/>
    <meta name="twitter:domain" content="NutanixBible.ru"/>
    <meta name="twitter:image:src" content="http://nutanixbible.ru/assets/Bible.png"/>
    <meta name="twitter:creator" content="@a1ex_var1amov"/>

    <!-- Google+ data -->
    <meta itemprop="name" content="Библия Nutanix - NutanixBible.ru">
    <meta itemprop="description" content="Библия Nutanix - Перевод детального описания архитектуры Nutanix за авторством Стивена Пойтраса, о том как работает ПО и его функции, а также как использовать это для получения максимальной производительности.">
    <meta itemprop="image" content="http://nutanixbible.ru/assets/Bible.png">



    <!-- <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-66778923-1', 'auto');
      ga('send', 'pageview');
    </script> -->

    <link rel="stylesheet" type="text/css" href="css/nutanixbible.css">
  </head>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script src="scripts/jquery.unveil.js"></script>

  <script>
    $(function() {
        $("li img").unveil(300);
    });
  </script>

  <body data-type="book">
    <!-- Google Tag Manager -->
    <!-- <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-TR9PVL" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> -->
    <!-- <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-TR9PVL');</script> -->
    <!-- End Google Tag Manager -->
<div class="container">
<section data-type="titlepage" class="page-title" id="the-nutanix-bible-L02Ia">
    <img src="assets/Bible.svg" alt="" class="biblesvg">
    <h1>Библия Nutanix</h1>

    <p class="author">Автор: <a href="https://www.linkedin.com/in/stevenpoitras/">Стивен Пойтрас</a></p>
    <p class="author">Перевод: <a href=https://www.linkedin.com/in/aleksandr-varlamov>Александр Варламов</a></p>

</section>

<section data-type="copyright-page" class="page-title" id="id-7ANIl">

<img src="assets/ornament1.svg" alt="" class="ornament">
<p class="small"><b>Авторские права (c) 2018:</b> Библия Nutanix и NutanixBible.com, 2018. Несанкционированное использование и/или копирование этого материала без письменного разрешения автора и / или владельца этого блога строго запрещено. Выдержки и Ссылки могут быть использованы при условии, что Стивену Пойтрасу и NutanixBible.com будет предоставлена вся информация о факте, целях и назначении их использования с обязательной отсылкой к оригинальному содержимому сайта.</p>



<p class="small"><b>От переводчика:</b> С тех пор, как я перевел <a href=http://nutanix.ru>первую версию Библии Nutanix</a> прошло немало времени и старый перевод устарел. Я принял решение освежить его. В этот раз я постараюсь поддерживать его в актуальном состоянии и своевременно исправлять недочеты перевода.</p>
<br>

<p>Отзывы, предложения и опечатки отправляйте на адрес: avarlamov@protonmail.com</p>
<br>

<p>
  Локализованные версии:
</p>

<div class="localization">
  <a href="http://nutanixbible.com/" target="_blank">
      <img src="assets/flag-united-states.svg" alt="English" class="Japanese">
  </a>
  <a href="http://nutanixbible.jp/" target="_blank">
    <img src="assets/flag-japanese.svg" alt="Japanese" class="Japanese">
  </a>
  <a href="http://www.virtual-space.co.kr/nutanix-works.html" target="_blank">
    <img src="assets/flag-korean.svg" alt="Korean" class="Japanese">
  </a>
  <a href="http://go.nutanix.com/rs/031-GVQ-112/images/Nutanix%20Bible[CN].pdf" target="_blank">
    <img src="assets/flag-chinese.svg" alt="Chinese" class="Japanese">
  </a>
</div>

<br>

<p>
  Версия в формате PDF доступна на английском языке (В данной версии могут отсутствовать последние изменения):
</p>
<a href="pdf/NutanixBible.pdf" target="_blank">
  <img src="assets/pdf.png" alt="PDF" style="width: 40px; height: 40px">
</a>

</section>

<!-- START: Ken Chen 11-17-2015-->
<div id="nav-icon"><div></div></div>
<div class="nav-title">
  Содержание
  <div id="nav-close-button"></div>
</div>
<!-- END: Ken Chen 11-17-2015-->

<nav data-type="toc">
</nav>

<section data-type="preface" class="preface" id="foreword-7kBIw">
<h1 id="anchor-foreword-1">Предисловие</h1>


<figure class="small" id="id-wntQsz">
<img alt="Dheeraj Pandey" class="iimagesv2dheeraj_pandeyjpg" src="imagesv2/Dheeraj_Pandey.jpg" style="width:60%; max-width:218px; horizontal-align:middle">
<figcaption><span class="label">Рисунок 1-1. </span>
<p class="sign">Дирай Пандей, CEO, Nutanix</p>
</figcaption>
</figure>

<blockquote>
<p>
    Для меня большая честь писать предисловие к этой книге, которую мы зовем Библия Nutanix. В первую очередь, я хотел бы обратиться к названию этой книги, которое может показаться некоторым людям не вполне уместным по отношению к их вероисповеданию, а также к агностикам и атеистам. Согласно словарю Мериам Вебстер значение слова "библия" не подразумевает по собой Писание как таковое: "выдающаяся публикация, имеющая авторитет у широкой читательской аудитории". Именно так и следует интерпретировать его корни. Данный материал был написан одним их самых скромных и сведущих сотрудников компании Nutanix - Стивеном Пойтрасом, нашим первым Архитектором решений, который является очень авторитетным, при этом, не кичась статусом одного из первых сотрудников компании. Знания не были для него его собственной прерогативой, наоборот - постоянный обмен и распространение знаний делает его чрезвычайно влиятельным в этой компании. Стив олицетворяет культуру этой компании, помогает сотрудникам делясь своей глубокой компетенцией, помогая автоматизировать задачи коллег при помощи Power Shell или Python, создавая прекрасное описание архитектуры (тут важно отметить прекрасный баланс между формой и содержанием), а так же помогая всем, кто нуждается в его помощи в реальном времени в Yammer и Twitter, сохраняя прозрачность перед инженерами, говоря о необходимости самокритики и самосовершенствования, о необходимости быть амбициозным.
</p>
<p>
    Когда Стив решил написать этот блог его мечтой было стать лидером, со всей прозрачностью, создать сторонников в отрасли, кто будет помогать приходить к компромиссным решениям в дизайне продукта. Открытие таких подробностей огромная редкость для компаний, никто не рассказывал столько сколько Стив в своем блоге. Даже Open Source компании, которые выглядят прозрачно на первый взгляд, никогда не раскрывают подробности о том, как их продукт работает. Мы же считаем, что, когда наши конкуренты знают о слабостях нашего продукта и архитектуры — это делает нас сильнее, не остается секретов. Публичная критика или обсуждение слабых мест архитектуры в кратчайшее время приводит в Yammer всю компанию, где мы пытаемся понять - является ли отмеченный недостаток недостатком, так наш продукт становится лучше. В этом сила честного общения с партнерами и клиентами.
</p>
</p>
<p>
    Этот постоянно совершенствующийся артефакт, помимо авторитета, имеет широчайшую читательскую аудиторию во всем мире. Архитекторы, менеджеры и ИТ-директора, останавливали меня в конференц-залах, чтобы поговорить о том, насколько прозрачен стиль письма, с подробными иллюстрациями, фигурами для visio и схемами. Стив потратил много времени, чтобы рассказать историю web-scale архитектуры, не навешивая ярлыков. Популяризировать нашу распределенную архитектуру было непросто, в мире, где ИТ-специалисты завалены решением "неотложных" задач и проблем. Библия позволяет сократить разрыв между ИТ и DevOps, она объясняет самые сложные вещи из глубин ИТ простыми словами. Мы надеемся, что в ближайшие 3-5 лет ИТ станет использовать простые термины, и приблизится к Web-Scale DevOps.
</p> 
<p>
    С первым изданием мы превращаем блог Стивена в книгу. День, когда мы перестанем обновлять этот материал, станет началом конца этой компании. Я ожидаю, что каждый из вас будет продолжать напоминать нам о том, что привело нас к сюда: правда, только правда и ничего кроме правды освободит вас (от самодовольства и гордыни).
</p>
<p>Будьте честными.</p>
</blockquote>

<p>&nbsp;</p>

<p class="sign">--Дирай Пандей, CEO, Nutanix</p>

<p>&nbsp;</p>

<figure id="id-aztlFk"><img alt="Stuart Miniman" class="iimagesv2stujpg" src="imagesv2/Stu.jpg" style="width:80%; max-width:218px; horizontal-align:middle">
<figcaption><span class="label">Рисунок 1-2. </span>
<p class="sign">Стюарт Минимэн, Главный Научный Сотрудник, Wikibon</p>
</figcaption>
</figure>

<blockquote>
<p>Сегодня пользователи каждый день сталкиваются с новыми технологиями. У сферы ИТ нет пределов для совершенствования, однако принятие обществом новых технологий, и тем более - изменение каких-либо привычных процессов и операций дело сложное и непростое. Быстрый рост технологий с открытым исходным кодом, например, сдерживается отсутствием полноценной документации. В качестве попытки борьбы с этой проблемой был реализован проект Wikibon. Библия Nutanix, начинавшаяся как блог Стивена Пойтраса со временем стала ценным источником информации для ИТ-специалистов, которые хотят узнать о принципах гиперконвергентной и web-scale архитектуры, поглубже погрузиться в особенности архитектуры гипервизора и платформы Nutanix. Концепции, о которых пишет Стив — это передовые технологии и решения, которые создаются одними из лучших инженеров отрасли. В этой книге сложные понятия описаны доступным языком, понятным широкому кругу читателей, и при этом без ущерба техническим подробностям.</p>
<p>Концепция распределенных систем и программно-определяемой инфраструктуры являются самой передовой на текущий момент, это очень важно донести до всех ИТ-специалистов. Я призываю прочитать эту книгу всех, кто хочет понять, что скрывается под этой концепцией. Технологии описанные здесь являются основой самых крупных центров обработки данных в Мире.</p></blockquote>

<p>&nbsp;</p>

<p class="sign">--Стюарт Минимэн, Главный Научный Сотрудник, Wikibon</p>

<h2>Введение</h2>

<figure id="id-ZptOIk"><img alt="Steven Poitras" src="imagesv2/poitras_pic.jpg" style="width:80%; max-width:500px; horizontal-align:middle">

<figcaption><span class="label">Рисунок 1-3. </span>
<p class="sign">Стивен Пойтрас, Главный архитектор решений, Nutanix</p>
</figcaption>
</figure>

<blockquote>
<p>Добро пожаловать в Библию Nutanix!  Я работаю с платформой Nutanix на постоянной основе – пытаюсь выявить проблемы, увеличить допустимые максимумы, а также администрирую платформу в своей лаборатории. Этот материал я задумывал, как документ, который будет содержать советы и приемы по работе с решениями компании от меня и инженеров, которые работают с платформой каждый день.</p>
<p>Примечание: Данный материал позволяет взглянуть на платформу изнутри и получить представление как все устроено. Для успешной работы с Nutanix Вам не потребуется глубоких знаний по всем упомянутым в данном материале темам.</p>
<p>Наслаждайтесь!</p>
</blockquote>

<p>&nbsp;</p>

<p class="sign">--Стивен Пойтрас, Главный архитектор решений, Nutanix</p>

</section>

<div data-type="part" id="a-brief-lesson-in-history-6qVi1">
<h1><span class="label">Часть I. </span>Краткий экскурс в историю</h1>

<p>Краткий обзор истории ИТ-инфраструктуры и предпосылок которые привели нас туда, где мы находимся сейчас.</p>

<section data-type="chapter" id="the-evolution-of-the-datacenter-R5INu4">
<h2>Эволюция центров обработки данных</h2>

<p>Центры обработки данных очень изменились за последние несколько десятков лет. В следующих разделах мы рассмотрим каждую эру.&nbsp;&nbsp;</p>

<section data-type="sect1" id="the-era-of-the-mainframe-NYI5u8uq">
<h3>Эра мейнфреймов</h3>

<p>Мейнфремы правили в ЦОД в течение многих лет и заложили основу большинства современных технологий. Мейнфремы обеспечивали:</p>

<ul>
	<li>Встроенный конвергентный процессор, оперативную память, и хранилище</li>
	<li>Резервирование на аппаратном уровне</li>
</ul>

<p>Однако, имели следующие недостатки:</p>

<ul>
	<li>Высокая стоимость инфраструктуры</li>
	<li>Чрезмерная сложность</li>
	<li>Недостаточная гибкость</li>
</ul>
</section>

<section data-type="sect1" id="the-move-to-stand-alone-servers-22IlTzuZ">
<h3>Переход на классические серверы</h3>

<p>Использование мейнфреймов было слишком дорогим и сложным для бизнеса, и это стало причиной появления классических серверов шириной 19". Их основные особенности:</p>

<ul>
	<li>Центральный процессор, оперативная память, локальные диски</li>
	<li>Большая гибкость, чем у мейнфреймов</li>
	<li>Сетевые интерфейсы и доступ по сети</li>
</ul>

<p>Такие серверные решения несли за собой следующие минусы:</p>

<ul>
	<li>Увеличение количества оборудования</li>
	<li>Низкий уровень утилизации оборудования</li>
	<li>Каждый сервер становился единой точкой отказа, как для хранилища так и для вычислений</li>
</ul>
</section>

<section data-type="sect1" id="centralized-storage-3jIvSMu5">
<h3>Централизованное хранилище</h3>

<p>Основная задача бизнеса - заработок денег, и данные являются одним из ключевых инструментов. Когда организации используют классические хранилища - им всегда требуется больше емкости и обеспечении высокой доступности данных, чтобы выход из строя сервера не приводил к их потере.</p>

<p>Так классические СХД заменили локальные емкости серверного оборудования и мейнфреймов, обеспечив сохранность данных. Их основные характеристики:</p>

<ul>
	<li>Объединенные ресурсы хранения и их эффективное использование</li>
	<li>Централизованная защита данных при помощи технологии RAID</li>
	<li>Доступ к данным по сети</li>
</ul>

<p>Вот некоторые недостатки, которые привнесли классические выделенные хранилища:</p>

<ul>
	<li>Классические СХД являются дорогостоящими, однако данные дороже</li>
	<li>Выросла сложность инфраструктуры (Фабрики SAN, адресация WWPN, RAID-группы, тома, количество шпинделей и так далее)</li>
	<li>Требуется использовать специальное ПО и команду</li>
</ul>
</section>

<section data-type="sect1" id="the-introduction-of-virtualization-PgIzHQum">
<h3>Внедрение виртуализации</h3>

<p>Виртуализация позволила увеличить плотность вычислений запуская несколько разнородных задач в рамках одного классического сервера. Виртуализация помогла Бизнесу утилизировать их оборудование, но увеличила последствия выхода оборудования из строя. <br>Основные характеристики виртуализации:</p>

<ul>
	<li>Позволяет перенести вычисления на уровень выше от оборудования - создать виртуальные машины</li>
	<li>Очень эффективное использование вычислительных ресурсов и консолидация нагрузок</li>
</ul>

<p>Виртуализация привнесла следующие вопросы:</p>

<ul>
	<li>Количество оборудования выросло, управление ими усложнилось</li>
	<li>Без технологий HA отказ сервера приводил к влиянию на большее количество сервисов</li>
	<li>Отсутствие общих ресурсов</li>
	<li>Снова требовалась отдельная команда и специальное ПО</li>
</ul>
</section>

<section data-type="sect1" id="virtualization-matures-lkInFEuj">
<h3>Развитие виртуализации</h3>

<p>Гипервизоры стали функциональным и эффективным решением. С появлением технологий VMware vMotion, HA, и DRS, пользователи получили возможность обеспечивать HA для ВМ и динамически распределять нагрузку на оборудование. Однако централизованное хранилище все еще являлось проблемой - конкуренция ВМ за ресурсы увеличивало нагрузку на хранилища. <br>Ключевые характеристики:</p>

<ul>
    <li>Кластеризация привела к объединению вычислительных ресурсов</li>
    <li>Появилась возможность динамического перераспределения нагрузки (DRS / vMotion)</li>
    <li>Высокая доступность ВМ позволяла сохранить работоспособность ПО при выходе из строя аппаратного сервера</li>
    <li>Требовалось использовать централизованное хранилище</li>  
</ul>

<p>Вопросы:</p>

<ul>
    <li>Увеличились требования к производительности централизованного хранилища</li>
    <li>Требования к хранилищу снова увеличивали количество оборудования</li>
    <li>Вместе с производительностью хранилища выросла и стоимость хранения</li>
    <li>Конкуренция за ресурсы хранения влияла на производительность</li>
    <li>Все это делало конфигурацию хранилища еще сложнее, требовалось следить за:
    <ul>
      <li>количеством ВМ использующих хранилище и томов созданных на СХД</li>
      <li>количеством шпинделей, чтобы обеспечивать приемлемую производительность</li>  
	</ul>
	</li>
</ul>
</section>

<section data-type="sect1" id="solid-state-disks-ssds-M2I9tquP">
<h3>Твердотельные накопители</h3>

<p>SSD помогли устранить узкое место в I/O, обеспечивая гораздо более высокую производительность без необходимости добавлять и добавлять шпиндели.&nbsp; Однако, контроллеры и сеть все еще не были готовы к такой производительности. <br>Ключевые характеристики:</p>

<ul>
	<li>Гораздо более высокие характеристики I/O, чем у HDD</li>
	<li>Время отклика сильно сократилось</li>
</ul>

<p>Вопросы:</p>

<ul>
    <li>Узкое место переместилось с уровня хранилища на уровень контроллеров и сетей</li>
    <li>Количество оборудования все еще было велико</li>
    <li>Сложность конфигурации СХД сохранялась</li>  
</ul>
</section>

<section data-type="sect1">
<h3>Пришествие облаков</h3>
<p>
  Термин Облако можно трактовать по-разному. Одним словом — это возможность предоставлять расположенный где-то сервис пользователям, которые расположены где-то еще.
</p>

<p>
  С появлением облаков изменились перспективы ИТ, Бизнеса и конечных пользователей.
</p>

<p>
  Бизнес требует от ИТ, чтобы ресурсы предоставлялись по облачной модели - как можно быстрее. Если это не происходит - Бизнес уходит непосредственно в облака и сталкивается там с проблемами безопасности.
</p>

<p>
  Основные столпы любого облачного сервиса:
</p>
<ul>
  <li>
    Самообслуживание / Получение ресурсов по требованию
    <ul>
      <li>
        Быстрая окупаемость / низкий порог входа
      </li>
    </ul>
  </li>
  <li>
   Фокус на сервисе и SLA
    <ul>
      <li>
        Гарантии времени работы, уровня доступности, производительности
      </li>
    </ul>
  </li>
  <li>
   Гранулярная модель потребления
    <ul>
      <li>
        Плати за то, что используешь. При этом некоторые сервисы могут быть бесплатными
      </li>
    </ul>
  </li>
</ul>

<h5>Классификация облаков</h5>
<p>
  Облака могут быть разделены на три основных типа:
</p>

<ul>
  <li>
    ПО как сервис (SaaS)
    <ul>
      <li>
        Любое ПО или услуга доступные через url
      </li>
      <li>
        Примеры: Workday, Salesforce.com, Google search, etc.
      </li>
    </ul>
  </li>
  <li>
    Платформа как сервис (PaaS)
    <ul>
      <li>
        Платформа для разработки и размещения приложений
      </li>
      <li>
        Примеры: Amazon Elastic Beanstalk / Relational Database Services (RDS), Google App Engine, и так далее
      </li>
    </ul>
  </li>
  <li>
    Инфраструктура как сервис (IaaS)
    <ul>
      <li>
        Виртуальные машины/Контейнеры/Вируализация сетевых функций
      </li>
      <li>
        Примеры: Amazon EC2/ECS, Microsoft Azure, Google Compute Engine (GCE), и так далее
      </li>
    </ul>
  </li>
</ul>

<h5>Сдвиг фокуса в области ИТ</h5>
<p>
    Облака ставят перед ИТ интересный вопрос - ответом на который может стать использование внешних облачных платформ/провайдеров или же обеспечение какой-то альтернативы им. 
    При этом, все хотят сохранить свои бизнес-данные внутри компании, предложить пользователям удобный портал самообслуживания и высокую скорость развертывания сервисов.
</p>
<p>
  То есть, сдвиг фокуса в сторону облачных сервисов толкает ИТ к тому, чтобы стать "облачным" сервис-провайдером для собственных конечных пользователей (сотрудников компании).
</p>

</section>
</section>

<section data-type="chapter" id="the-importance-of-latency-13I4Ta">
<h2>Важность задержек при работе с данными</h2>

<p>В таблице ниже приведены задержки для разных типов I/O:</p>

<table>
  <tr>
    <th>Операция</th>
    <th>Задержка</th>
    <th>Комментарии</th>
  </tr>
  <tr>
    <td>Доступ к кэшу первого уровня</td>
    <td>0.5 ns</td>
    <td></td>
  </tr>
  <tr>
    <td>Доступ к кэшу второго уровня</td>
    <td>7 ns</td>
    <td>14x L1 cache</td>
  </tr>
  <tr>
    <td>Доступ к DRAM</td>
    <td>100 ns</td>
    <td>20x L2 cache, 200x L1 cache</td>
  </tr>
  <tr>
    <td>3D XPoint на основе чтения NVMe SSD</td>
    <td>10,000 of ns (ожидаемо)</td>
    <td>10 us or 0.01 ms</td>
  </tr>
  <tr>
    <td>NAND NVMe SSD R/W</td>
    <td>20,000 ns</td>
    <td>20 us or 0.02 ms</td>
  </tr>
  <tr>
    <td>NAND SATA SSD R/W</td>
    <td>50,000-60,000 ns</td>
    <td>50-60 us or 0.05-0.06 ms</td>
  </tr>
  <tr>
    <td>Случайное чтение блоков 4K с SSD</td>
    <td>150,000 ns</td>
    <td>150 us or 0.15 ms</td>
  </tr>
  <tr>
    <td>Задержка P2P TCP/IP (физика на физику)</td>
    <td>150,000 ns</td>
    <td>150 us or 0.15 ms</td>
  </tr>
  <tr>
    <td>Задержка P2P TCP/IP (ВМ на ВМ)</td>
    <td>250,000 ns </td>
    <td>250 us or 0.25 ms</td>
  </tr>
  <tr>
    <td>Последовательное чтение 1MB из памяти</td>
    <td>250,000 ns</td>
    <td>250 us or 0.25 ms</td>
  </tr>
  <tr>
    <td>В среднем по ЦОД</td>
    <td>500,000 ns</td>
    <td>500 us or 0.5 ms</td>
  </tr>
  <tr>
    <td>Последовательное чтение 1MB с SSD</td>
    <td>1,000,000 ns</td>
    <td>1 ms, 4x memory</td>
  </tr>
  <tr>
    <td>Поиск диска</td>
    <td>10,000,000 ns или 10,000 us</td>
    <td>10 ms, 20x от среднего по ЦОД</td>
  </tr>
  <tr>
    <td>Последовательное чтение 1MB с диска</td>
    <td>20,000,000 ns or 20,000 us</td>
    <td>20 ms, 80x memory, 20x SSD</td>
  </tr>
  <tr>
    <td>Отправлка пакетов Калифорния -&gt; Нидерланды -&gt; Калифорния</td>
    <td>150,000,000 ns</td>
    <td>150 ms</td>
  </tr>
</table>

<p><em>(автор: Джефф Дин, https://gist.github.com/jboner/2841832)</em></p>

<p>Таблица показывает, что ЦПУ может получить доступ к КЭШ в среднем за ~0.5-7ns (L1 vs. L2). Для основной памяти ~100ns, когда локальное чтение 4K SSD ~150,000ns или же 0.15ms.</p>

<p>Если мы возьмем для примера стандартные SSD уровня предприятия (в нашем случае Intel S3700 - <a href="http://download.intel.com/newsroom/kits/ssd/pdfs/Intel_SSD_DC_S3700_Product_Specification.pdf">SPEC</a>), то сможем получить:</p>

<ul>
	<li>Производительность случайных операций I/O:
	<ul>
		<li>Случайное чтение, блоки по 4K: до 75,000 IOPS</li>
		<li>Случайная запись, блоки по 4K: до 36,000 IOPS</li>
	</ul>
	</li>
	<li>Пропускная способность при последовательных операциях I/O:
	<ul>
		<li>Последовательное чтение: до 500MB/s</li>
		<li>Последовательная запись: до to 460MB/s</li>
	</ul>
	</li>
	<li>Задержки:
	<ul>
		<li>Чтение: 50us</li>
		<li>Запись: 65us</li>
	</ul>
	</li>
</ul>

<section data-type="sect1" id="looking-at-the-bandwidth-QMIXtzTn">
<h3>Поговорим о пропускной способности</h3>

<p>Классические СХД используют две пары основных видов транспорта I/O:</p>

<ul>
	<li>SAN (FC)
	<ul>
		<li>4-, 8-, 16- и 32-Gb</li>
	</ul>
	</li>
	<li>LAN (включая FCoE)
	<ul>
		<li>1-, 10-Gb, (40-Gb IB), и т.д.</li>
	</ul>
	</li>
</ul>

<p>Для расчетов, приведенных ниже мы используем пропускную способность 500MB/s для чтения и 460MB/s для записи на базе Intel S3700.</p>

<p>Расчет производится следующим образом:</p>

<p>numSSD = ROUNDUP((numConnections * connBW (in GB/s))/ ssdBW (R or W))</p>

<p><i>Примечание:&nbsp;</i><em>Полученные значения были округлены. А также мы не берем в расчет ЦПУ, предполагая что у него нет ограничений</em></p>

<table>
	<tbody>
		<tr>
			<th colspan="2" rowspan="1">Пропускная способность сети</th>
			<th colspan="2" rowspan="1">Количество SSD необходимых для утилизации канала</th>
		</tr>
		<tr>
			<th>Тип подключения контроллера</th>
			<th>Доступная пропускная способность</th>
			<th>I/O чтения</th>
			<th>I/O записи</th>
		</tr>
		<tr>
			<td>Двойной 4Gb FC</td>
			<td>8Gb == 1GB</td>
			<td>2</td>
			<td>3</td>
		</tr>
		<tr>
			<td>Двойной 8Gb FC</td>
			<td>16Gb == 2GB</td>
			<td>4</td>
			<td>5</td>
		</tr>
		<tr>
			<td>Двойной 16Gb FC</td>
			<td>32Gb == 4GB</td>
			<td>8</td>
			<td>9</td>
		</tr>
    <tr>
      <td>Двойной 32Gb FC</td>
      <td>64Gb == 8GB</td>
      <td>16</td>
      <td>19</td>
    </tr>
		<tr>
			<td>Двойной 1Gb ETH</td>
			<td>2Gb == 0.25GB</td>
			<td>1</td>
			<td>1</td>
		</tr>
		<tr>
			<td>Двойной 10Gb ETH</td>
			<td>20Gb == 2.5GB</td>
			<td>5</td>
			<td>6</td>
		</tr>
	</tbody>
</table>

<p>Из таблицы видно, что если вы хотите использовать SSD на максимум, то сеть может быть узким местом при появлении 1-9 таких дисков, в зависимости от типа сети</p>
</section>

<section data-type="sect1" id="the-impact-to-memory-latency-J5IMf8Ty">
<h3>Задержки при работе с памятью</h3>

<p>Стандартная задержка при работе с памятью ~100ns, значит можно выполнить следующие расчеты:</p>

<ul>
	<li>Задержка при доступе к локальной памяти = 100ns + [накладные расходы на ОС / гипервизор] </li>
	<li>Задержка при чтении из памяти по сети = 100ns + задержки RTT + [2 x накладные расходы на ОС / гипервизор] </li>
</ul>

<p>Если принять, что RTT для стандартной сети равен ~0.5ms (может отличаться в зависимости от производителя оборудования) т.е. ~500,000ns, то:</p>

<ul>
	<li>Задержка при чтении из памяти по сети = 100ns + 500,000ns + [2 x накладные расходы на ОС / гипервизор]</li>
</ul>

<p>Если мы предположим, что имеем очень быструю сеть с RTT = 10,000ns, то: </p>

<ul>
	<li>Задержка при чтении из памяти по сети = 100ns + 10,000ns + [2 x накладные расходы на ОС / гипервизор]</li>
</ul>

<p>Т.е. даже при наличии сверхбыстрой сети мы имеем 10,000% накладных расходов по сравнению с локальной памятью. Если сеть медленная то, накладные расходы могут увеличится до 500,000%.</p>

<p>Чтобы эти накладные расходы как-то нивелировать, используется кэширование данных</p>
</section>
</section>

<section data-type="chapter" id="user-vs-kernel-NYIQSn">
<h2>Пространство пользователей и ядра</h2>
<p>Одной из самых жарких и часто обсуждаемых тем является обсуждение плюсов и минусов использования пользовательского пространства и пространства ядра операционной системы. Тут я остановлюсь на плюсах и минусах этих подходов.</p>

<p>Все операции ОС выполняются в двух основных пространствах:</p>

<ul>
  <li>
    Пространство ядра
    <ul>
        <li>Наиболее привилегированная часть ОС</li>
        <li>Управление задачами, памятью и так далее</li>
        <li>Работают драйверы и происходит управление оборудованием</li>  
    </ul>
  </li>
  <li>
    Пространство пользователя
    <ul>
      <li>Стандартное пространство для запуска пользовательских процессов</li>
      <li>Тут функционируют почти все процессы</li>
      <li>Память на этом уровне защищена, происходит выполнение процессов</li>
    </ul>
  </li>
</ul>

<p>Оба этих пространства позволяют ОС функционировать.Теперь давайте определим несколько ключевых элементов:</p>

<ul>
  <li>
    Системный вызов
    <ul>
      <li>Или же - вызов ядра, который осуществляется через прерывание из активного процесса</li>
    </ul>
    <li>
      Переключение контекста
      <ul>
        <li>Перенос выполнения между ядром и процессом</li>
      </ul>
    </li>
  </li>
</ul>

<p>Например рассмотрим, как осуществляеся запись данных приложением на диск:</p>

<ol>
  <li>Приложение хочет записать данные на диск</li>
  <li>Осуществляется вызов системного вызова</li>
  <li>Контекст переключается на ядро</li>
  <li>Ядро осуществляет копирование</li>
  <li>Осуществляется запись на диск с использованием драйвера</li>
</ol>

<p>Схема взаимодействия:</p>

<figure id="id-sf14"><img alt="User and Kernel Space Interaction" class="iimagesv2arch_prismpng" src="imagesv2/user_vs_kernel_1.png">
<figcaption><span class="label">Рисунок. </span>Взаимодействие пользовательского пространства и пространства ядра</figcaption>
</figure>

<p>Одно пространство лучше другого? В реальности есть следующие плюсы и минусы:</p>

<ul>
  <li>
    Пространство пользователя
    <ul>
      <li>Очень гибкое</li>
      <li>Изолированные домены (сервисы)</li>
      <li><i>Может быть</i> неэффективным
        <ul>
          <li>Переключение контекстов требует времени(~1,000ns)</li>
        </ul>
      </li>
    </ul></li>
    <li>
      Пространство ядра
      <ul>
        <li>Очень жесткое</li>
        <li>Один большой домен</li>
        <li><i>Может быть</i> эффективным
          <ul>
            <li>Меньше переключений контекста</li>
          </ul>
        </li>
      </ul>
    </li>
</ul>

<h3>Опрос против прерываний</h3>

<p>Еще одним ключевым моментом является тип взаимодействия между пространствами:</p>

<ul>
  <li>
    Опрос
    <ul>
      <li>Постоянный опрос - непрерывный запрос какой-то информации</li>
      <li>Пример: движение мышки, частота обновления монитора и так далее</li>
      <li>Постоянный расход процессорного времени</li>
      <li>Исключает использование обработчика прерываний ядра, низкие задержки
        <ul><li>Исключает контекстный переключатель</li></ul>
      </li>
    </ul>
  </li>
  <li>Прерывания
    <ul>
      <li>Периодические запросы ресурсов</li>
      <li>Пример: Студент поднимает руку, чтобы задать вопрос</li>
      <li>Может быть более эффективным, но не всегда</li>
      <li>Обычно задержка намного выше, чем при опросе</li>
    </ul>
  </li>
</ul>

<h3>Пространство пользователя</h3>

<p>Чем быстрее устройство (например: NVMe, Intel Optane, pMEM), тем более узким местом становится процесс взаимодействия ядра и устройства. Чтобы как-то решить эту проблему, многие производители переносят задачи <b>из пространства ядра</b> в пользовательское пространство и используют механизм опроса, для получения лучших результатов производительности.</p>

<p>Прекрасный пример - Intel <a href="https://software.intel.com/en-us/articles/accelerating-your-nvme-drives-with-spdk" target="_blank">Storage Performance Development Kit (SPDK)</a> и <a href="https://www.intel.com/content/www/us/en/communications/data-plane-development-kit.html" target="_blank">Data Plane Development Kit (DPDK)</a>.  Оба проекта направлены на получение максимальной производительности и сокращение задержек.</p>

<p>Такое перемещение ПО включает следующие изменения:</p>

<ol>
  <li>Перенос драйверов устройств в пользовательское пространство (вместо пространства ядра)</li>
  <li>Использование механизма опроса (вместо прерываний)</li>
</ol>

<p>Это обеспечивает большую производительность, потому что устраняет</p>

<ul>
  <li>Дорогие системные вызовы и управление прерываниями</li>
  <li>Копии данных</li>
  <li>Переключение контекстов</li>
</ul>

<p>Ниже показан пример взаимодействия устройств с помощью драйверов в пользовательском пространстве:</p>

<figure id="id-sf14"><img alt="User Space and Polling Interaction" class="iimagesv2arch_prismpng" src="imagesv2/user_vs_kernel_2.png">
<figcaption><span class="label">Рисунок. </span>Взаимодействие опроса и пользовательского пространства</figcaption>
</figure>

<p>Можно отметить, что часть решений компании Nutanix разработанных для AHV (vhost-user-scsi), используется компанией Intel для проекта SPDK.</p>

</section>

<section data-type="chapter" id="book-of-web-scale-NYIQSn">
<h2>Книга Web-Scale</h2>

<p class="definition"><strong>web·scale - /web ' skãl/ - noun - горизонтально масштабируемая архитектура</strong>
<br>
новый архитектурный подход к реализации инфраструктуры и вычислениям</p>

<p>В этой главе будут представлены основные понятия “Web-scale” инфраструктуры и почему мы используем их. Важно прояснить - использование такой архитектуры не обязывает вас быть размером с Google или Amazon, такая архитектура полезна при любом масштабе инфраструктуры (начиная с трех узлов).</p>
<p>Исторические вызовы:</p>

<ul>
	<li>Сложность, сложность, сложность</li>
	<li>Стремление к постепенному росту</li>
	<li>Необходимось быть более гибким</li>
</ul>

<p>Вот основные фишки Web-scale</p>

<ul>
	<li>Гиперконвергентность</li>
	<li>Программно-определяемые решения</li>
	<li>Распределенные автономные системы</li>
	<li>Постепенное, линейное масштабирование</li>
</ul>

<p>И еще:</p>

<ul>
	<li>Автоматизация на базе API, встроенная система аналитики</li>
  <li>Безопасность как основной элемент интегрированный в платформу</li>
	<li>Механизмы автоматического восстановления неисправностей</li>
</ul>

<p>Эти фишки будут раскрыты более детально чуть ниже по тексту</p>

<section data-type="sect1" id="hyper-convergence-ONIRcvSY">
<h3>Гиперконвергентность</h3>

<p>Существует несколько мнений, что же такое Гиперконвергентность&nbsp; Чаще всего это зависит от компонентов, о которых идет речь (например виртуализация, сеть и т.д.). Основная концепция: максимальная интеграция нескольких компонентов в единое решение. Чтобы достичь максимальной интеграции компоненты должны быть доработаны или разработаны именно так, чтобы идеально подходить друг другу. В случае Nutanix мы интегрируем хранилище и вычислительные мощности в узлы, которые являются основой нашего решения. &nbsp; Для других это могут быть, например, хранилище и сеть и так далее. </p>

<p>
  Итого, гиперконвергентность это:
</p>

<ul>
	<li>Максимальная интеграция двух или более компонент в единое решение, которое может легко масштабироваться</li>
</ul>

<p>Преимущества:</p>

<ul>
	<li>Стандартная единица масштабирования</li>
	<li>Локализация операций ввода/вывода</li>
	<li>Исключение классических вычислительных узлов и СХД</li>
</ul>
</section>

<section data-type="sect1" id="software-defined-intelligence-nrIRIWSn">
<h3>Программно-определяемые решения</h3>

<p>Программно-определяемые решения берут на себя задачи и основную логику, обычно присущую аппаратным компонентам и выполняются в рамках обычного серверного оборудования. В Nutanix мы взяли логику хранилища (RAID, дедупликацию, компрессию, и так далее) и перенесли ее в свое ПО, которое выполняется на каждой CVM - Nutanix Controller VMs</p>

<div data-type="note" class="note" id="pro-tip-05i5cRT9"><h6>Примечание</h6>
<h5>Поддерживаемые архитектуры</h5>

<p>Решение Nutanix поддерживает x86 и IBM POWER</p>
</div>

<p>
  Что все это означает:
</p>
<ul>
	<li>Мы перенесли ключевую логику с уровня аппаратного обеспечения на уровень ПО, которое может работать на любом серверном оборудовании</li>
</ul>

<p>Преимущества:</p>

<ul>
	<li>Быстрый релизный цикл</li>
	<li>Устранение зависимости от оборудования конкретного производителя</li>
	<li>Использование стандартного оборудования дешевле</li>
  <li>Длительный срок службы и защита инвестиций</li>
</ul>

<p>
  Уточним последний пункт: старое оборудование может позволять новейшему и самому передовому ПО работать. Т.е. даже на оборудовании, срок службы которого ограничен, будут выходить и прекрасно работать обновления.
</p>
</section>

<section data-type="sect1" id="distributed-autonomous-systems-b1IeU4Sb">
<h3>Распределенные автономные системы</h3>

<p>Распределенные автономные системы предполагают отказ от выделения каких-либо ролей и компонентов, вместо этого роль равномерно распределяется между всеми узлами и экземплярами ПО &nbsp; Вы можете представлять себе это, как простую распределенную систему. Классические производители предполагали, что оборудование будет достаточно надежным, и в большинстве случаев так оно и было. &nbsp; Однако, распределенная система по умолчанию предполагает, что оборудование рано или поздно придет в негодность, а ПО решит любую проблему без прерывания сервиса.</p>

<p>Распределённые системы были разработаны так, чтобы решать свои задачи локально на каждом узле, поддерживая при этом работоспособность остального кластера и сервисов функционирующих в его рамках &nbsp; Т.е. если ПО получит сообщение, что какой-то аппаратный или программный компонент сбоит то оно обработает ошибку и продолжит функционировать. Система оповещения сообщить пользователям о событии, и вместо остановки сервиса будет ожидать вмешательства администратора (например, ждать замены узла).&nbsp; При этом администратор, может осуществить переборку кластера без замены компонента. Если же речь идет о экземпляре ПО, которое являлось на данный момент "Мастером", то все еще проще - будет выбран новый мастер и все продолжит функционировать. Для распределения задач между экземплярами ПО используется концепция MapReduce.</p>

<p> Что все это означает:</p>

<ul>
	<li>Распределение ролей и обязанностей между всеми узлами кластера</li>
	<li>Использование концепции MapReduce для равномерного распределения задач</li>
	<li>Запуск процессов выбора нового Мастера, если это необходимо</li>
</ul>

<p>Преимущества:</p>

<ul>
	<li>Исключение любых точек отказа (SPOF)</li>
	<li>Распределение нагрузки, для устранения узких мест</li>
</ul>
</section>

<section data-type="sect1" id="incremental-and-linear-scale-out-rkIZhxSN">
<h3>Постепенное, линейное масштабирование</h3>

<p>Постепенное, линейное масштабирование предполагает возможность начать работу с минимально необходимым количеством ресурса, добавляя их по мере роста потребностей.&nbsp; Все конструкции, о которых говорилось раньше направлены на реализацию этой функциональности. Обычно существует три уровня компонент - вычислительная подсистема, хранилище и сеть. Все они масштабируются независимо друг от друга. &nbsp;Например, если вы добавляете новых серверов, вы не увеличиваете при этом производительность и емкость хранилища. А с платформой Nutanix вы одновременно масштабируете:</p>  

<ul>
    <li>Количество гипервизоров / вычислительных узлов</li>
    <li>Количество контроллеров хранилища</li>
    <li>Производительность вычислительных узлов / емкость хранилища</li>
    <li>Число узлов, участвующих в операциях в рамках кластера</li>  
</ul>

<p>Что все это означает:</p>

<ul>
	<li>Возможность постепенно масштабирования хранилища и вычислительных мощностей, с линейным увеличением производительности и доступности.</li>
</ul>

<p>Преимущества:</p>

<ul>
	<li>Возможность начать с малого количества ресурсов и масштабироваться до требуемого</li>
	<li>Равномерное и последовательное увеличение производительности</li>
</ul>
</section>
</section>

<section data-type="sect1" id="making-sense-of-it-all-22IWHQ">
<h3>Все это очень важно!</h3>

<p>Итого:</p>

<ol>
    <li>Неэффективное использование вычислительных ресурсов привело к переходу на виртуализацию</li>
    <li>Функции vMotion, HA и DRS привели к необходимости централизованного хранения</li>
    <li>Рост нагрузки на виртуальные машины привел к увеличению нагрузки и конкуренции на хранилище</li>
    <li>SSDs пришли, чтобы исправить ситуацию с производительностью, но сместили узкое место в сторону сети и контроллеров</li>
    <li>Доступ к кэшам и памяти по сети порождает большие накладные расходы, который сводят к минимуму плюсы</li>
    <li>Сложность конфигурации СХД остается прежней</li>
    <li>Кэширование на уровне серверов пришло, чтобы снять нагрузку на хранилище. Однако, это еще один компонент и точка отказа</li>
    <li>Локализация операций с данными позволяет избавится от узких мест и сгладить ситуацию с накладными расходами</li>
    <li>Сдвигается фокус от инфраструктуры к легкости управления, упрощается стек</li>
    <li>Рождается новый мир Web-Scale!</li>  
</ol>
</section>
</div>

<div data-type="part" id="book-of-prism-7gEiv">
<h1><span class="label">Part II. </span>Книга веб-интерфейса Prism</h1>

<p class="definition"><strong>prism - /'prizɘm/ - noun - панель управления</strong>
<br>
простой интерфейс для управления всей платформой Nutanix и центром обработки данных.</p>

<section data-type="chapter" id="design-methodology-and-iterations-13IRuV">
<h2>Методология проектирования</h2>

<p>
 Важнейшей нашей целью является создание красивого, отзывчивого и интуитивного интерфейса. Данный интерфейс является одним из самых важных компонентов платформы Nutanix. Мы относимся к нему очень серьезно. В этом разделе будет рассказано о нашей методологии проектирования, о том, к чему мы движемся. Уже скоро этот раздел будет дополнен!  
</p>

<p>
  А пока - почитайте прекрасный пост от создателя этого интерфейса и руководителя направления промышленного дизайна Джереми Салли  - <a href="http://salleedesign.com/stuff/sdwip/blog/nutanix-case-study/">http://salleedesign.com/stuff/sdwip/blog/nutanix-case-study/</a>
</p>

<p>
  Фигуры для редактора Visio доступны по ссылке: <a href="http://www.visiocafe.com/nutanix.htm">http://www.visiocafe.com/nutanix.htm</a>
</p>

</section>

<section data-type="chapter" id="architecture-NYIVT0">
<h2>Архитектура</h2>

<p>Prism - распределенная система управления, которая позволяет пользователям управлять объектами платформы Nutanix и следить за их состоянием.</p>

<p>Систему управления можно разделить на две основных части:</p>

<ul>
	<li>Интерфейсы
	<ul>
		<li>HTML5 UI, REST API, CLI, PowerShell CMDlets, и так далее.</li>
	</ul>
	</li>
	<li>Управление
	<ul>
		<li>Определение и соблюдение политик, разработка и статус услуг, аналитика и мониторинг</li>
	</ul>
	</li>
</ul>

<p>На рисунке изображена компонентная архитектура Prism как части платформы Nutanix:</p>

<figure id="id-XWtxHVTW"><img alt="High-Level Prism Architecture" class="iimagesv2arch_prismpng" src="imagesv2/arch_prism.png">
<figcaption><span class="label">Рисунок 5-1. </span>Высокоуровневая архитектура Prism</figcaption>
</figure>

<p>Prism делится на два продукта:</p>

<ul>
	<li>Prism Central (PC)
	<ul>
		<li>Система управления несколькими кластерами Acropolis посредством единого централизованного интерфейса. &nbsp; Это опциональное ПО поставляемое в виде ВМ</li>
		<li>Управление несколькими кластерами одновременно</li>
	</ul>
	</li>
	<li>Prism Element (PE)
	<ul>
		<li>Интерфейс для управления конкретным кластером. &nbsp;Данное ПО является частью любого кластера Nutanix.</li>
		<li>Управление одним, конкретным кластером</li>
	</ul>
	</li>
</ul>

<p>На рисунке изображена схема взаимодействия ПО Prism Central и Prism Element:</p>

<figure id="id-zmt2i4Tx"><img alt="Prism Architecture" class="iimagesv2prism_arch2png" src="imagesv2/prism_arch2.png">
<figcaption><span class="label">Рисунок 5-2. </span>Арихитектура ПО Prism</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-05i5cRT9"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Для больших, распределенных инсталляций рекомендуется использовать Prism Central для упрощения управления инфраструктурой через единую точку входа.</p>
</div>

<h3>Сервисы Prism</h3>

<p>Сервисы Prism выполняются на всех CVM. Среди копий сервиса определяется Мастер, который отвечает за обработку HTTP запросов.&nbsp; В случае каких-либо проблем с текущим Мастер-сервисом данная роль будет передана другой копии сервиса в кластере. Это относится ко всем компонентам ПО Nutanix, имеющим такую архитектуру.&nbsp; Если пользовательский запрос приходит не на Мастер-сервис - он будет перенаправлен Мастеру, с использованием HTTP ответа с кодом 301.</p>

<p>Ниже показано, как сервисы Prism обрабатывают HTTP запросы:</p>

<figure id="id-DktNCvTn"><img alt="Prism Services - Request Handling" class="iimagesv2prism_services3png" src="imagesv2/prism_services3.png">
<figcaption><span class="label">Рисунок 5-3. </span>Сервисы Prism - обработка HTTP запросов</figcaption>
</figure>

<div data-type="note" class="note" id="prism-ports-53iysDTA"><h6>Примечание</h6>
<h3>Порты Prism</h3>

<p>Prism слушает порты 80 и 9440, если трафик HTTP приходит на порт 80 он будет автоматически перенаправлен на порт HTTPS - 9440.</p>
</div>

<p>Если используется выделенный внешний адрес для доступа к Prism (рекомендуется),&nbsp;он всегда будет на Мастер-сервисе. &nbsp; В случае выхода из строя текущего Мастер-сервиса, адрес переместится на нового Мастера, а встроенные механизмы работы с ARP (gARP) будут использованы для удаления старых записей о физическом адресе устройства. &nbsp;Проще говоря - кластер всегда будет доступен по одному и тому же адресу.</p>

<div data-type="note" class="note" id="pro-tip-1RiATNTX"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Вы можете определить где сейчас находится Мастер-сервис путем выполнения команды 'curl localhost:2019/prism/leader' на любой из CVM кластера Nutanix.</p>
</div>
</section>

<section data-type="chapter" id="navigation-22IzS5">
<h2>Навигация</h2>

<p>Prism довольно удобен и прост в использовании, однако мы рассмотрим основные страницы и их назначение.</p>

<p>Если используется Prism Central, то доступ к нему можно получить посредством IP-адреса указанного при конфигурации и соответствующей записи DNS. &nbsp; Prism Element будет доступен или через Prism Central, или же просто по адресу любой CVM кластера или VIP-адресу кластера.</p>

<p>Когда страница загрузится вы увидите страницу логина, где будут представлены поля для ввода данных учетной записи. Это может быть локальная УЗ или же запись в Active Directory.</p>

<figure id="id-XWtpS0SW"><img alt="" class="iimagesv2prismprism_loginpng" src="imagesv2/Prism/prism_login.png">
<figcaption><span class="label">Рисунок 6-1. </span>Страница логина</figcaption>
</figure>

<p>После успешного входа вы будете перенаправлены на панель управления, где будет представлена общая информация по кластеру или кластерам, если вы подключились к Prism Central.</p>

<p>Prism Central и Prism Element будут более подробно описаны в следующих главах.</p>

<section data-type="sect1" id="prism-central-4aI3tqSe">
<h3>Prism Central</h3>

<p>Prism Central состоит из следующих страниц:</p>

<ul>
	<li>Home Page
	<ul>
		<li>Информация по всей инфраструктуре, включая детальную информацию о статусах сервисов, доступных ресурсах, производительности, задачах и так далее. &nbsp; Для получения более подробной информации вы можете просто нажать на тот элемент, который Вас интересует.</li>
	</ul>
	</li>
	<li>Explore Page
	<ul>
		<li>Управление и мониторинг сервисами, кластерами, виртуальными машинами и узлами</li>
	</ul>
	</li>
	<li>Analysis Page
	<ul>
		<li>Детальная информация о производительности кластеров и других объектов с корреляцией событий</li>
	</ul>
	</li>
	<li>Alerts
	<ul>
		<li>Оповещения от инфраструктуры</li>
	</ul>
	</li>
</ul>

<p>На рисунке показан пример панели управления Prism Central:</p>

<figure class="large" id="id-0OtpSxt2SW"><img alt="Prism Central - Dashboard" class="iimagesv2prismpc_dashboard2png" src="imagesv2/Prism/PC_dashboard2.png">
<figcaption><span class="label">Рисунок 6-2. </span>Prism Central - панель управления</figcaption>
</figure>

<p>Тут можно увидеть статус мониторинга всей инфраструктуры. А так же получить расширенную информацию, если есть какие-то оповещения или вам нужна какая-то информация.</p>

<div data-type="note" class="note" id="pro-tip-G9iVFptNS9"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Если все зеленое, можно закрыть окно и заняться чем-то еще :)</p>
</div>
</section>

<section data-type="sect1" id="prism-element-BNIWfdSz">
<h3>Prism Element</h3>

<p>Prism Element включает в себя следующие основные страницы:</p>

<ul>
	<li>Home Page
	<ul>
		<li>Панель инструментов с детальной информацией о событиях, ресурсах, производительности, состоянии компонент, запущенных задачах и т.д. &nbsp; Для получения дополнительной информации о любом объекте - просто нажмите на него. </li>
	</ul>
	</li>
	<li>Health Page
	<ul>
		<li>Детальная информация о состоянии инфраструктуры и оборудования. &nbsp; Включает в себя результаты выполнения NCC.</li>
	</ul>
	</li>
	<li>VM Page
	<ul>
		<li>Страница управления виртуальными машинами, операции CRUD (Acropolis)</li>
		<li>Мониторинг ВМ (не-Acropolis)</li>
	</ul>
	</li>
	<li>Storage Page
	<ul>
		<li>Управление контейнерами, мониторинг, операции CRUD</li>
	</ul>
	</li>
	<li>Hardware
	<ul>
		<li>Мониторинг и управление физическими узлами, дисками, сетью. &nbsp; Включает в себя функции расширения кластера и управления дисками.</li>
	</ul>
	</li>
	<li>Data Protection
	<ul>
		<li>Катастрофоустойчивость, Управление подключением к облакам и настройка Метро-кластера. &nbsp;Управление объектами хранилища,&nbsp;снимками ВМ, репликацией и восстановлением.</li>
	</ul>
	</li>
	<li>Analysis
	<ul>
		<li>Подробный анализ событий кластера с возможностью корреляции данных</li>
	</ul>
	</li>
	<li>Alerts
	<ul>
		<li>Оповещения и ошибки от локального кластера</li>
	</ul>
	</li>
</ul>

<p>Домашняя страница предоставляет детальную информацию по оповещениям, статусам сервисов, емкости, производительности, задачам, и так далее. &nbsp;Для получения детальной информации - нажмите на интересующем Вас объекте</p>
<p>Ниже показан пример отображения информации по кластеру в Prism Element:</p>

<figure class="large" id="id-DktkHEfaSr"><img alt="Prism Element - Dashboard" class="iimagesv2prismpe_dashboardpng" src="imagesv2/Prism/PE_dashboard.png">
<figcaption><span class="label">Рисунок 6-3. </span>Prism Element - панель управления</figcaption>
</figure>

<div data-type="note" class="note" id="keyboard-shortcuts-53i0FXfDS9"><h6>Примечание</h6>
<h3>Сочетания клавиш</h3>

<p>Удобство и простота интерфейса - это ключевая особенность Prism. &nbsp;Для упрощения навигации мы добавили несколько комбинаций горячих клавиш</p>

<p>Ниже описаны некоторые сочетания клавиш:</p>

<p>Изменение типа представления информации в рамках страницы:</p>

<ul>
	<li>O - Общая информация</li>
	<li>D - Диаграмма</li>
	<li>T - Таблица</li>
</ul>

<p>События и задачи:</p>

<ul>
	<li>A - Оповещения</li>
	<li>P - Задачи</li>
</ul>

<p>Навигация по меню (Перемещаться между пунктами меню можно с помощью стрелок):</p>

<ul>
	<li>M - Ниспадающее меню</li>
	<li>S - Настройки (иконка с шестеренкой)</li>
	<li>F - Панель поиска</li>
	<li>U - Пользовательское ниспадающее меню</li>
	<li>H - Помощь</li>
</ul>
</div>
</section>
</section>

<section data-type="chapter" id="usage-and-troubleshooting-3jIxHa">
<h2>Использование Prism</h2>

<p>В следующих разделах мы рассмотрим некоторые пути использования интерфейса Prism, а так же сценарии устранения неполадок.</p>

<section data-type="sect1" id="nutanix-software-upgrade-lkIBu8H5">
<h3>Обновление ПО Nutanix</h3>

<p>Обновление ПО Nutanix очень простой процесс.</p>

<p>Нужно зайти в Prism и нажать на иконку с шестеренкой в правом верхнем углу экрана, или же нажав клавишу 'S'. Затем выбрать пункт 'Upgrade Software':</p>

<figure id="id-d0t0TNunHd"><img alt="Prism - Settings - Upgrade Software" class="iimagesv2prismupgradeupgrade_1png" src="imagesv2/Prism/upgrade/upgrade_1.png">
<figcaption><span class="label">Рисунок 7-1. </span>Prism - Настройки - Обновление ПО</figcaption>
</figure>

<p>Эти действия запустят процесс обновления ПО, и Вы увидите окно 'Upgrade Software' где будет показана текущая версия ПО и новая версия, если она доступна. &nbsp;Также можно осуществить загрузку обновлений NOS вручную.</p>
<p>Затем можно получить обновление автоматически с ресурсов Nutanix или загрузить обновление вручную:</p>

<figure id="id-89tyFxuAHl"><img alt="Upgrade Software - Main" class="iimagesv2prismupgradeupgrade_2png" src="imagesv2/Prism/upgrade/upgrade_2.png">
<figcaption><span class="label">Рисунок 7-2. </span>Обновление ПО - Основное меню</figcaption>
</figure>

<p>Обновление ПО будет загружено на служебные ВМ Nutanix (CVMs):</p>

<figure id="id-0OtVfvu9HW"><img alt="Upgrade Software - Upload" class="iimagesv2prismupgradeupgrade_3png" src="imagesv2/Prism/upgrade/upgrade_3.png">
<figcaption><span class="label">Рисунок 7-3. </span>Обновление ПО - Загрузка</figcaption>
</figure>

<p>Когда ПО будет загружено - можно нажать на кнопку 'Upgrade' для начала процесса обновления:</p>

<figure id="id-DktQcOunHr"><img alt="Upgrade Software - Upgrade Validation" class="iimagesv2prismupgradeupgrade_4png" src="imagesv2/Prism/upgrade/upgrade_4.png">
<figcaption><span class="label">Рисунок 7-4. </span>Обновление ПО - Проверка обновления</figcaption>
</figure>

<p>Система покажет окно подтверждения действия:</p>

<figure id="id-GRtGUyubH9"><img alt="Upgrade Software - Confirm Upgrade" class="iimagesv2prismupgradeupgrade_5png" src="imagesv2/Prism/upgrade/upgrade_5.png">
<figcaption><span class="label">Рисунок 7-5. </span>Обновление ПО - Подтверждение обновления</figcaption>
</figure>

<p>Процесс установки новой версии начнется с проверки готовности платформы к обновлению:</p>

<figure id="id-RBtOCvuWHR"><img alt="Upgrade Software - Execution" class="iimagesv2prismupgradeupgrade_6png" src="imagesv2/Prism/upgrade/upgrade_6.png">
<figcaption><span class="label">Рисунок 7-6. </span>Обновление ПО - Выполнение</figcaption>
</figure>

<p>Как только обновление буде завершено - статус процесса изменится соответствующим образом, Платформа получит доступ ко всем новым функциям свежей версии ПО:</p>

<figure id="id-NMtXu8unHl"><img alt="Upgrade Software - Complete" class="iimagesv2prismupgradeupgrade_7png" src="imagesv2/Prism/upgrade/upgrade_7.png">
<figcaption><span class="label">Рисунок 7-7. </span>Обновление ПО - Завершение процесса</figcaption>
</figure>

<div data-type="note" class="note" id="note-P0i1TQuRHz"><h6>Примечание</h6>
<h5>Примечание</h5>

<p>Текущая пользовательская сессия Prism может быть прервана на короткое время, в момент обновления текущего Мастер-сервиса. &nbsp;Все пользовательские ВМ продолжат работать без прерывания.</p>
</div>
</section>

<section data-type="sect1" id="hypervisor-upgrade-M2I5TWHq">
<h3>Обновление гипервизора</h3>

<p>Обновление гипервизора полностью автоматическое и выполняется через Prism.</p>

<p>Для начала обновления Вы должны выполнить такие же действия - перейти в меню 'Upgrade Software' и выбрать пункт 'Hypervisor'.</p>

<p>Обновление может быть загружено автоматически с ресурсов Nutanix или вручную:</p>

<figure id="id-zmtoS4TxHM"><img alt="Upgrade Hypervisor - Main" class="iimagesv2prismupgradehyp_upgrade_1png" src="imagesv2/Prism/upgrade/hyp_upgrade_1.png">
<figcaption><span class="label">Рисунок 7-8. </span>Обновление гипервизора - Основное меню</figcaption>
</figure>

<p>Начнется загрузка обновления ПО на Гипервизоры. &nbsp;Как только ПО будет загружено - нажмите 'Upgrade' для начала процесса обновления:</p>

<figure id="id-xbtnFYTqHQ"><img alt="Upgrade Hypervisor - Upgrade Validation" class="iimagesv2prismupgradehyp_upgrade_2png" src="imagesv2/Prism/upgrade/hyp_upgrade_2.png">
<figcaption><span class="label">Рисунок 7-9. </span>Обновление гипервизора - Проверка обновления</figcaption>
</figure>

<p>Затем будет показано следующее окно подтверждения:</p>

<figure id="id-A0t1fYT9Hg"><img alt="Upgrade Hypervisor - Confirm Upgrade" class="iimagesv2prismupgradehyp_upgrade_3png" src="imagesv2/Prism/upgrade/hyp_upgrade_3.png">
<figcaption><span class="label">Рисунок 7-10. </span>Обновление гипервизора - Подтверждение обновления</figcaption>
</figure>

<p>Платформа начнет проверять гипервизоры в составе кластера и размещать на узлах дистрибутив новой версии ПО:</p>

<figure id="id-k1tzcqT5HX"><img alt="Upgrade Hypervisor - Pre-upgrade Checks" class="iimagesv2prismupgradehyp_upgrade_4png" src="imagesv2/Prism/upgrade/hyp_upgrade_4.png">
<figcaption><span class="label">Рисунок 7-11. </span>Обновление гипервизора - Проверки перед обновлением</figcaption>
</figure>

<p>Когда проверки будут закончены и все служебные действия завершены начнется сам процесс обновления ПО:</p>

<figure id="id-5WtYUDTAH9"><img alt="Upgrade Hypervisor - Execution" class="iimagesv2prismupgradehyp_upgrade_5png" src="imagesv2/Prism/upgrade/hyp_upgrade_5.png">
<figcaption><span class="label">Рисунок 7-12. </span>Обновление гипервизора - Выполнение обновления</figcaption>
</figure>

<p>Гипервизоры обновляются по очереди, ВМ работают без прерываний. &nbsp;ВМ будут перемещаться между узлами, с использованием функции живой миграции. После обновления каждый узел будет перезагружен. &nbsp;Этот процесс будет выполнен для каждого узла в кластере.</p>

<div data-type="note" class="note" id="pro-tip-21iyCmTqHR"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Вы можете проверить статус обновления ПО через командную строку CVM выполнив 'host_upgrade --status'. &nbsp;Детальная информация о обновлении конкретного узла расположена здесь ~/data/logs/host_upgrade.out</p>
</div>

<p>Как только обновление закончится - вы получите доступ ко всем новым возможностям новой версии гипервизора:</p>

<figure id="id-2btAumTqHR"><img alt="Upgrade Hypervisor - Complete" class="iimagesv2prismupgradehyp_upgrade_6png" src="imagesv2/Prism/upgrade/hyp_upgrade_6.png">
<figcaption><span class="label">Рисунок 7-13. </span>Обновление гипервизора - Complete</figcaption>
</figure>

</section>

<section data-type="sect1" id="cluster-expansion-add-node-QMI5SwHj">
<h3>Масштабирование кластера</h3>

<p>
    Возможность динамического масштабирования - ключевая функция кластеров Acropolis. Для масштабирования кластера Acropolis просто смонтируйте оборудование, подключите его к сети и нажмите на кнопку питания. Как только узел будет запущен - он будет найден посредством mDNS.
</p>
  

<p>
	На рисунке изображен пример найденного узла, готового для добавления в кластер:
</p>

<figure id="id-zmt9TOSxHM"><img alt="Add Node - Discovery" src="imagesv2/Prism/addnode/expand_1.png">
<figcaption><span class="label">Рисунок 7-14. </span>Добавление узла - Поиск</figcaption>
</figure>

<p>
	Множество узлов могут быть найдены и добавлены в кластер единовременно.
</p>

<p>
  Как только узлы найдены вы можете начинать процесс масштабирования, нажав на кнопку 'Expand Cluster' доступную в правом верхнем углу на странице 'Hardware':
</p>

<figure id="id-0Ot0FaS9HW"><img alt="Hardware Page - Expand Cluster" src="imagesv2/Prism/addnode/expand_2a.png">
<figcaption><span class="label">Рисунок 7-15. </span>Страница Hardware - Масштабирование кластера</figcaption>
</figure>

<p>
  Также, можно запустить процесс масштабирования из меню настройки:
</p>

<figure id="id-DktrfgSnHr"><img alt="Gear Menu - Expand Cluster" src="imagesv2/Prism/addnode/expand_2b.png">
<figcaption><span class="label">Рисунок 7-16. </span>Меню настроек - Масштабирование кластера</figcaption>
</figure>

<p>
  В меню масштабирования кластера можно отметить узлы для добавления и указать для них IP-адреса:
</p>

<figure id="id-GRtMcVSbH9"><img alt="Expand Cluster - Host Selection" src="imagesv2/Prism/addnode/expand_3.png">
<figcaption><span class="label">Рисунок 7-17. </span>Масштабирование кластера - Выбор узлов</figcaption>
</figure>

<p>
  После того, как узел будет выбран, можно загрузить образ требуемого гипервизора на добавляемые узлы. Для узлов с гипервизором AHV, или при наличии образа в хранилище дополнительных действий не требуется.
</p>

<figure id="id-RBtnU4SWHR"><img alt="Expand Cluster - Host Configuration" src="imagesv2/Prism/addnode/expand_4.png">
<figcaption><span class="label">Рисунок 7-18. </span>Масштабирование кластера - Подготовка узла</figcaption>
</figure>

<p>
	После завершения загрузки образа нажмите 'Expand Cluster' для начала процесса обновления узла и добавления его в кластер:
</p>

<figure id="id-NMtbCnSnHl"><img alt="Expand Cluster - Execution" src="imagesv2/Prism/addnode/expand_5.png">
<figcaption><span class="label">Рисунок 7-19. </span>Масштабирование кластера - Выполнение процесса</figcaption>
</figure>

<p>
	Задача будет сформирована и отправлена на исполнение, появится строка состояния для данной задачи:
</p>

<figure id="id-3rtEuXSeHB"><img alt="Expand Cluster - Execution" src="imagesv2/Prism/addnode/expand_6.png">
<figcaption><span class="label">Рисунок 7-20. </span>Масштабирование кластера - Выполнение процесса</figcaption>
</figure>

<p>
  Подробная информация о выполнении задачи доступна при нажатии на строке состояния:
</p>

<figure id="id-lNtDSzS5HV"><img alt="Expand Cluster - Execution" src="imagesv2/Prism/addnode/expand_7.png">
<figcaption><span class="label">Рисунок 7-21. </span>Масштабирование кластера - Выполнение процесса</figcaption>
</figure>

<p>
	После завершения процесса масштабирования, информация о ресурсах кластера и количестве узлов будет обновлена:
</p>

<figure id="id-Q4t9FESjHm"><img alt="Expand Cluster - Execution" src="imagesv2/Prism/addnode/expand_9.png">
<figcaption><span class="label">Рисунок 7-22. </span>Масштабирование кластера - Выполнение процесса</figcaption>
</figure>

</section>

<section data-type="sect1" id="io-analysis">
<h3>Метрики I/O</h3>

<!-- TODO:
<p>
  Performance characterization is a very big domain and one which people typically misjudge. Most frequently people characterize performance by looking bottom up looking at micro metrics (e.g. latency, cpu queues, etc).
</p>

<p>
  To truly characterize performance the best approach is to take a top down approach.  Start by looking at the application / system performance then evaluate down the stack.
</p>
-->

<p>
  Поиск узких мест - одна из самых важных задач в процессе анализа работы платформы и устранения неисправностей. Для упрощения этой задачи в интерфейсе Nutanix есть специальный раздел 'I/O Metrics' на странице с ВМ. 
</p>

<p>
  Задержки зависят от множества переменных (глубины очереди, размера I/O, состояния системы, производительности сети и так далее).  Эта страница предоставляет подробную информацию о размере I/O, задержках, источниках, и типах нагрузки.
</p>

<p>
  Чтобы получить доступ к этим данным - перейдите на страницу 'VM' и выберите нужную ВМ из списка:
</p>

<figure><img alt="VM Page - Details" src="imagesv2/Prism/iometrics/io_1.png">
<figcaption><span class="label">Рисунок. </span>Страница "VM" - Подробности о ВМ</figcaption>
</figure>

<p>
  Вкладка с информацией по I/O находится чуть ниже:
</p>

<figure><img alt="VM Page - I/O Metrics Tab" src="imagesv2/Prism/iometrics/io_1b.png">
<figcaption><span class="label">Рисунок. </span>Страница "VM" - Данные по метрикам I/O</figcaption>
</figure>

<p>
  На этой вкладке подробно расписаны все метрики I/O. Давайте разберем, что за данные тут доступны.
</p>

<p>
  Первое представление 'Avg I/O Latency' демонстрирует среднюю задержку при операциях чтения/записи за последние три часа. По умолчанию вы видите самые свежие данные по данной метрике.
</p>

<p>
  Тут можно выбрать интересующее время и получить информацию за конкретный период.
</p>

<figure><img alt="I/O Metrics - Latency Plot" src="imagesv2/Prism/iometrics/io_2.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - График задержек</figcaption>
</figure>

<p>
    Это может быть полезно, если наблюдается внезапный всплеск. Можно найти этот всплеск на графике, и нажать на него для получения детальной информации.  
</p>

<figure><img alt="I/O Metrics - Latency Plot" src="imagesv2/Prism/iometrics/io_2a.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - График задержек</figcaption>
</figure>

<p>
  Если с задержками все в порядке, то нет смысла копать дальше.
</p>

<p>
  Следующая секция демонстрирует график размера блока I/O при операциях чтения и записи:
</p>

<figure><img alt="I/O Metrics - I/O Size histogram" src="imagesv2/Prism/iometrics/io_3.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - Размер I/O</figcaption>
</figure>

<p>
  Тут мы видим, что наши операции чтения находятся в диапазоне между 4K и 32K:
</p>

<figure><img alt="I/O Metrics - Read I/O Size histogram" src="imagesv2/Prism/iometrics/io_3a.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - Размер I/O при чтении</figcaption>
</figure>

<p>
  Далее - мы видим размер операций записи - они в диапазоне от 16K до 64K, а некоторые - 512K:
</p>

<figure><img alt="I/O Metrics - Write I/O Size histogram" src="imagesv2/Prism/iometrics/io_3b.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - Размер I/O при записи</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Если видны пики по задержкам - в первую очередь стоит проверить размер операций I/O.  Большие операции I/Os (64K up to 1MB) обычно генерируют более существенные задержки, чем небольшие операции (4K to 32K).</p>
</div>

<p>
  Следующая секция демонстрирует гистограмму для задержек при операциях чтения и записи:
</p>

<figure><img alt="I/O Metrics - Latency histogram" src="imagesv2/Prism/iometrics/io_4.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - Данные по задержкам</figcaption>
</figure>

<p>
  На этом графике видно, что большая часть операций чтения имела задержку около 1ms, а некоторые из операций - 2-5ms.
</p>

<figure><img alt="I/O Metrics - Read Latency histogram" src="imagesv2/Prism/iometrics/io_4ab.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - Данные по задержкам для операций чтения</figcaption>
</figure>

<p>
  Если посмотреть в секцию 'Read Source', то будет видно, что большая часть операций чтения/записи выполняется с SSD:
</p>

<figure><img alt="I/O Metrics - Read Source SSD" src="imagesv2/Prism/iometrics/io_5a2.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - SSD</figcaption>
</figure>

<p>
  Как только данные считаны - они передаются в общий кэш (DRAM+SSD) в реальном времени (В секции 'Операции чтения/записи и кэширование' это раскрыто более подробно). Тут мы видим, что данные были загружены в кэш и теперь обслуживаются при помощи DRAM:  
</p>

<figure><img alt="I/O Metrics - Read Source DRAM" src="imagesv2/Prism/iometrics/io_5a.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - чтение из DRAM</figcaption>
</figure>

<p>
  Тут видно, что по большей части операции чтения имеют задержки менее 1ms:
</p>

<figure><img alt="I/O Metrics - Read Latency histogram" src="imagesv2/Prism/iometrics/io_4a.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - задержки при операциях чтения</figcaption>
</figure>

<p>
  А здесь видно, что большая часть операций записи выполняется с задержками в районе &lt;1-2ms:
</p>

<figure><img alt="I/O Metrics - Write Latency histogram" src="imagesv2/Prism/iometrics/io_4b.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - задержки при операциях записи</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Если наблюдается всплеск задержек при чтении, и размер операции - небольшой, то стоит проверить - откуда выполняется чтение. Любая операция чтения с HDD будет иметь значительно большую задержку, чем из кэша DRAM; однако, как только требуемые данные попадут в DRAM вы увидите улучшение ситуации с задержками. </p>
</div>

<p>
  Последняя секция отображает типичные профили операций чтения и записи - случайные эти операции или последовательные:
</p>

<figure><img alt="I/O Metrics - RW Random vs. Sequential" src="imagesv2/Prism/iometrics/io_5b.png">
<figcaption><span class="label">Рисунок. </span>Метрики I/O - Случайные и последовательные операции чтения/записи</figcaption>
</figure>

<p>
  Обычно профиль операций чтения/записи сильно зависят от типа приложения (например VDI генерирует случайные IO, а Hadoop - последовательные). Какие-то приложения будут смешивать оба типа. Например, базы данных будут генерировать случайные IO при запросах или добавлении данных и последовательные при операциях ETL.  
</p>



<!-- End I/O metrics section -->

</section>

<section data-type="sect1" id="capacity-planning-J5IrHVHl">
<h3>Планирование ресурсов</h3>

<p>Для получения информации по планированию ресурсов нужно нажать на интересующий кластер в секции 'cluster runway' интерфейса Prism Central:</p>

<figure class="large" id="id-zmt8uyHxHM"><img alt="Prism Central - Capacity Planning" class="iimagesv2prismpc_capplannerpng" src="imagesv2/Prism/pc_capplanner.png">
<figcaption><span class="label">Рисунок 7-23. </span>Prism Central - Планирование ресурсов</figcaption>
</figure>

<p>Тут содержится подробная информация о тренде использования ресурсов. &nbsp;Здесь можно получить информацию о главных потребителях ресурсов, путях высвобождения ресурсов и рекомендации по типу узлов, для дальнейшего масштабирования кластера.</p>

<figure id="id-xbtOSXHqHQ"><img alt="Prism Central - Capacity Planning - Recommendations" class="iimagesv2prismpc_recommendationpng" src="imagesv2/Prism/pc_recommendation.png">
<figcaption><span class="label">Рисунок 7-24. </span>Prism Central - Планирование ресурсов - Рекомендации</figcaption>
</figure>
</section>

<p>HTML5 интерфейс Prism - основной инструмент для простого управления кластером. &nbsp;Но не менее важен API который предоставляет прекрасную возможность для автоматизации. &nbsp;Все функции доступные через интерфейс Prism так же могут быть вызваны через REST APIs, что обеспечивает возможность интеграции с различном сторонним ПО. &nbsp;Это позволяет клиентам и партнерам компании интегрировать ПО от сторонних разработчиков или даже создавать собственный пользовательский интерфейс. &nbsp;</p>  

<p>В следующем разделе описаны все интерфейсы Платформы и приведены примеры их использования.</p>
</section>

<section data-type="chapter" id="apis-and-interfaces-PgIAF1">
<h2>API и интерфейсы</h2>

<p>Такие интерфейсы являются основой любой программно-определяемой среды. Nutanix предоставляет широкий набор таких интерфейсов для простого взаимодействия с платформой:</p>

<ul>
	<li>REST API</li>
	<li>CLI - ACLI &amp; NCLI</li>
	<li>Интерфейсы автоматизации</li>
</ul>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>deleveloper.nutanix.com</h5>

<p>Обязательно посетите раздел нашего сайта для получения подробной информации по этой теме: <a href=http://developer.nutanix.com>developer.nutanix.com</a>!</p>
</div>


<p>REST API позволяет получить доступ ко всем функциям Prism UI, так любое средство автоматизации и оркестрации может с легкостью управлять Платформой.&nbsp; Вы можете использовать любое удобное ПО - Saltstack, Puppet, vRealize Operations, System Center Orchestrator, Ansible для автоматизации и создания собственных сценариев работы с Nutaix&nbsp; Кроме того, любые сторонние разработчики могут создавать свои пользовательские интерфейсы для Платформы и отправлять запросы через REST.</p>

<p>Ниже показан инструмент удобной работы с API - Nutanix REST API Explorer, который позволяет отправлять запросы и получать примеры ответов:</p>

<figure class="large" id="id-Vrt4HjF5"><img alt="Prism REST API Explorer" class="iimagesv2restapipng" src="imagesv2/RestAPI.png">
<figcaption><span class="label">Рисунок 8-1. </span>Nutanix REST API Explorer</figcaption>
</figure>

<p>По каждой операции можно получить подробнейшую информацию и примеры вызовов REST:</p>

<figure class="large" id="id-89tJtRFA"><img alt="Prism REST API Sample Call" class="iimagesv2restapi2png" src="imagesv2/RestAPI2.png">
<figcaption><span class="label">Рисунок 8-2. </span>Пример вызова REST API</figcaption>
</figure>

<div data-type="note" class="note" id="api-authentication-schemes-Ami1fVF9"><h6>Примечание</h6>
<h5>Схема аутентификации API</h5>

<p>Начиная с версии 4.5.x используется стандартная аутентификация через HTTPS для проверки подлинности запросов по HTTP.</p>
</div>

<section data-type="sect1" id="acli-b1IPigFB">
<h3>ACLI</h3>

<p>The Acropolis CLI (ACLI) это командный интерфейс к платформе, являющийся неотъемлемой его частью. &nbsp;Этот компонент стал доступным, начиная с версии 4.1.2</p>

<p>Примечание: Все эти действия могут быть выполнены так же через веб-интерфейс и REST API.&nbsp; Я использую эти команды для автоматизации рутинных задач с помощью скриптов.</p>

<h5>Вход в ACLI</h5>

<p class="codedescription">Описание: Вход в оболочку ACLI (с любой CVM)</p>

<p class="codetext">Acli</p>

<p>OR</p>

 <p class="codedescription">Описание: Вход в оболочку ACLI через консоль Linux</p>

<p class="codetext">ACLI &lt;Command&gt;</p>

<h5>Вывод ACLI в формате json</h5>

 <p class="codedescription">Описание: Просмотр узлов в кластере</p>

<p class="codetext">Acli –o json</p>

<h5>Список узлов</h5>

 <p class="codedescription">Описание: Просмотр узлов в кластере</p>

<p class="codetext">host.list</p>

<h5>Создание сети</h5>

 <p class="codedescription">Описание: Создание сети на базе VLAN</p>

<p class="codetext">net.create &lt;TYPE&gt;.&lt;ID&gt;[.&lt;VSWITCH&gt;] ip_config=&lt;A.B.C.D&gt;/&lt;NN&gt;</p>

<p class="codetext">Пример: net.create vlan.133 ip_config=10.1.1.1/24</p>

<h5>Список сетей</h5>

 <p class="codedescription">Описание: Список сетей</p>

<p class="codetext">net.list</p>

<h5>Создание пула DHCP</h5>

 <p class="codedescription">Описание: Создание пула DHCP</p>

<p class="codetext">net.add_dhcp_pool &lt;NET NAME&gt; start=&lt;START IP A.B.C.D&gt; end=&lt;END IP W.X.Y.Z&gt;</p>

<p class="note">Примечание: Адрес .254 всегда резервируется под встроенный сервер DHCP, если он не был указан пользователем при создании сети</p>

<p class="codetext">Пример: net.add_dhcp_pool vlan.100 start=10.1.1.100 end=10.1.1.200</p>

<h5>Детальное описание сети</h5>

 <p class="codedescription">Описание: Подробный список ВМ в сети, их имен / UUID, MAC адресов и IP</p>

<p class="codetext">net.list_vms &lt;NET NAME&gt;</p>

<p class="codetext">Пример: net.list_vms vlan.133</p>

<h5>Настройка сервиса DHCP DNS для сети</h5>

<p class="codedescription">Описание: Настройка DHCP DNS</p>

<p class="codetext">net.update_dhcp_dns &lt;NET NAME&gt; servers=&lt;COMMA SEPARATED DNS IPs&gt; domains=&lt;COMMA SEPARATED DOMAINS&gt;</p>

<p class="codetext">Пример: net.set_dhcp_dns vlan.100 servers=10.1.1.1,10.1.1.2 domains=splab.com</p>

<h5>Создание виртуальной машины</h5>

 <p class="codedescription">Описание: Создание ВМ</p>

<p class="codetext">vm.create &lt;COMMA SEPARATED VM NAMES&gt; memory=&lt;NUM MEM MB&gt; num_vcpus=&lt;NUM VCPU&gt; num_cores_per_vcpu=&lt;NUM CORES&gt; ha_priority=&lt;PRIORITY INT&gt;</p>

<p class="codetext">Пример: vm.create testVM memory=2G num_vcpus=2</p>

<h5>Массовое создание ВМ</h5>

 <p class="codedescription">Описание: Массовое создание ВМ</p>

<p class="codetext">vm.create &nbsp;&lt;CLONE PREFIX&gt;[&lt;STARTING INT&gt;..&lt;END INT&gt;]&nbsp;memory=&lt;NUM MEM MB&gt; num_vcpus=&lt;NUM VCPU&gt; num_cores_per_vcpu=&lt;NUM CORES&gt; ha_priority=&lt;PRIORITY INT&gt;</p>

<p class="codetext">Пример: vm.create testVM[000..999]&nbsp;memory=2G num_vcpus=2</p>

<h5>Клонирование существующей ВМ</h5>

<p class="codedescription">Описание: Create clone of existing VM</p>

<p class="codetext">vm.clone &lt;CLONE NAME(S)&gt; clone_from_vm=&lt;SOURCE VM NAME&gt;</p>

<p class="codetext">Пример: vm.clone testClone clone_from_vm=MYBASEVM</p>

<h5>Создание нескольких клонов из существующей ВМ</h5>

 <p class="codedescription">Описание: Массовое создание клонов из существующей ВМ</p>

<p class="codetext">vm.clone &lt;CLONE PREFIX&gt;[&lt;STARTING INT&gt;..&lt;END INT&gt;] clone_from_vm=&lt;SOURCE VM NAME&gt;</p>

<p class="codetext">Пример: vm.clone testClone[001..999]&nbsp;clone_from_vm=MYBASEVM</p>

<h5>Создание диска и подключение его к ВМ</h5>

<p class="codetext"># Описание: Создание диска ВМ</p>

<p class="codetext">vm.disk_create &lt;VM NAME&gt; create_size=&lt;Size and qualifier, e.g. 500G&gt; container=&lt;CONTAINER NAME&gt;</p>

<p class="codetext">Пример: vm.disk_create testVM create_size=500G container=default</p>

<h5>Подключение сетевого интерфейса к ВМ</h5>

 <p class="codedescription">Описание: Создание и подключение сетевого интерфейса к ВМ</p>

<p class="codetext">vm.nic_create &lt;VM NAME&gt; network=&lt;NETWORK NAME&gt; model=&lt;MODEL&gt;</p>

<p class="codetext">Пример: vm.nic_create testVM network=vlan.100</p>

<h5>Настройка загрузочного диска ВМ</h5>

 <p class="codedescription">Описание: Настройка загрузочного диска ВМ</p>

<p>Настройка загрузочного диска, через указание специфического id</p>

<p class="codetext">vm.update_boot_device &lt;VM NAME&gt; disk_addr=&lt;DISK BUS&gt;</p>

<p class="codetext">Пример: vm.update_boot_device testVM disk_addr=scsi.0</p>

<h5>Настройка CDROM в качестве загрузочного диска</h5>

<p>Установка загрузки с CDrom</p>

<p class="codetext">vm.update_boot_device &lt;VM NAME&gt; disk_addr=&lt;CDROM BUS&gt;</p>

<p class="codetext">Пример: vm.update_boot_device testVM disk_addr=ide.0</p>

<h5>Монтирование ISO</h5>

 <p class="codedescription">Описание: Монтирование ISO</p>

<p>Порядок действий:</p>

<p>1. Загрузить ISOs в контейнер</p>

<p>2. Создать белый список IP-адресов клиента</p>

<p>3. Загрузить ISOs в общую папку</p>

<p>Создание CDrom с ISO</p>

<p class="codetext">vm.disk_create &lt;VM NAME&gt; clone_nfs_file=&lt;PATH TO ISO&gt; cdrom=true</p>

<p class="codetext">Пример: vm.disk_create testVM clone_nfs_file=/default/ISOs/myfile.iso cdrom=true</p>

<p>Если CDrom уже создан - просто смонтируйте его</p>

<p class="codetext">vm.disk_update &lt;VM NAME&gt; &lt;CDROM BUS&gt; clone_nfs_file&lt;PATH TO ISO&gt;</p>

<p class="codetext">Пример: vm.disk_update atestVM1 ide.0 clone_nfs_file=/default/ISOs/myfile.iso</p>

<h5>Отключение ISO от CDrom</h5>

 <p class="codedescription">Описание: Отключение образа ISO от CDrom</p>

<p class="codetext">vm.disk_update &lt;VM NAME&gt; &lt;CDROM BUS&gt; empty=true</p>

<h5>Включение ВМ</h5>

 <p class="codedescription">Описание:Включение ВМ</p>

<p class="codetext">vm.on &lt;VM NAME(S)&gt;</p>

<p class="codetext">Пример: vm.on testVM</p>

<p>Включение всех ВМ</p>

<p class="codetext">Пример: vm.on *</p>

<p>Включение всех ВМ по префиксу</p>

<p class="codetext">Пример: vm.on testVM*</p>

<p>Включение списка ВМ</p>

<p class="codetext">Пример: vm.on testVM[0-9][0-9]</p>
</section>

<section data-type="sect1" id="ncli-rkIAcbFJ">
<h3>NCLI</h3>

<p>Примечание: Все эти действия могут быть выполнены через веб-интерфейс и REST API. &nbsp;Я использую эти команды для написания автоматизации рутинных задач с помощью скриптов. </p>

<h5>Добавление подсети в белый список NFS</h5>

<p class="codedescription">Описание: Добавление подсети в белый список NFS</p>

<p class="codetext">ncli cluster add-to-nfs-whitelist ip-subnet-masks=10.2.0.0/255.255.0.0</p>

<h5>Получение информации о версии ПО Nutanix</h5>

 <p class="codedescription">Описание: Получение информации о версии ПО Nutanix</p>

<p class="codetext">ncli cluster version</p>

<h5>Отображение скрытых опций NCLI</h5>

 <p class="codedescription">Описание: Отображение скрытых опций и команд NCLI</p>

<p class="codetext">ncli helpsys listall hidden=true [detailed=false|true]</p>

<h5>Список пулов хранения</h5>

 <p class="codedescription">Описание: Список всех существующих пулов хранения</p>

<p class="codetext">ncli sp ls</p>

<h5>Список контейнеров</h5>

 <p class="codedescription">Описание: Список всех существующих контейнеров</p>

<p class="codetext">ncli ctr ls</p>

<h5>Создание контейнеров</h5>

<p class="codedescription">Описание: Создание нового контейнера</p>

<p class="codetext">ncli ctr create name=&lt;NAME&gt; sp-name=&lt;SP NAME&gt;</p>

<h5>Список ВМ</h5>

 <p class="codedescription">Описание: Список всех существующих ВМ</p>

<p class="codetext">ncli vm ls</p>

<h5>Список публичных ключей</h5>

 <p class="codedescription">Описание: Список всех существующих публичных ключей</p>

<p class="codetext">ncli cluster list-public-keys</p>

<h5>Добавление публичных ключей</h5>

 <p class="codedescription">Описание: Добавление публичных ключей для доступа к кластеру</p>

<p>Копирование публичных ключей на CVM</p>

<p>Добавление публичных ключей</p>

<p class="codetext">ncli cluster add-public-key name=myPK file-path=~/mykey.pub</p>

<h5>Удаление публичных ключей</h5>

 <p class="codedescription">Описание: Удаление публичных ключей</p>

<p class="codetext">ncli cluster remove-public-keys name=myPK</p>

<h5>Создание домена защиты</h5>

 <p class="codedescription">Описание: Создание домена защиты</p>

<p class="codetext">ncli pd create name=&lt;NAME&gt;</p>

<h5>Создание кластера назначения</h5>

 <p class="codedescription">Описание: Создание кластера назначения для выполнения репликации</p>

<p class="codetext">ncli remote-site create name=&lt;NAME&gt; address-list=&lt;Remote Cluster IP&gt;</p>

<h5>Создание домена защиты для всех ВМ в рамках контейнера</h5>

 <p class="codedescription">Описание: Защита всех ВМ в контейнере</p>

<p class="codetext">ncli pd protect name=&lt;PD NAME&gt; ctr-id=&lt;Container ID&gt; cg-name=&lt;NAME&gt;</p>

<h5>Создание домена защиты для конкретных ВМ</h5>

 <p class="codedescription">Описание: Защита конкретных ВМ</p>

<p class="codetext">ncli pd protect name=&lt;PD NAME&gt; vm-names=&lt;VM Name(s)&gt; cg-name=&lt;NAME&gt;</p>

<h5>Создание домена защиты для объектов хранилища (они же vDisk)</h5>

 <p class="codedescription">Описание: Защита конкретных объектов хранилища</p>

<p class="codetext">ncli pd protect name=&lt;PD NAME&gt; files=&lt;File Name(s)&gt; cg-name=&lt;NAME&gt;</p>

<h5>Создание снимка домена защиты</h5>

 <p class="codedescription">Описание: Разовое создание снимка домена защиты</p>

<p class="codetext">ncli pd add-one-time-snapshot name=&lt;PD NAME&gt; retention-time=&lt;seconds&gt;</p>

<h5>Создание расписания для создания снимков и репликации на кластер назначения</h5>

 <p class="codedescription">Описание: Создание расписания для создания снимков и репликации на кластер(ы) назначения</p>

<p class="codetext">ncli pd set-schedule name=&lt;PD NAME&gt; interval=&lt;seconds&gt; retention-policy=&lt;POLICY&gt; remote-sites=&lt;REMOTE SITE NAME&gt;</p>

<h5>Статус задач репликации</h5>

 <p class="codedescription">Описание: Статус задач репликации</p>

<p class="codetext">ncli pd list-replication-status</p>

<h5>Перенос домена защиты на другой кластер</h5>

 <p class="codedescription">Описание: Перенос домена защиты на другой кластер</p>

<p class="codetext">ncli pd migrate name=&lt;PD NAME&gt; remote-site=&lt;REMOTE SITE NAME&gt;</p>

<h5>Активация домена защиты</h5>

 <p class="codedescription">Описание: Активация домена защиты на кластере назначения</p>

<p class="codetext">ncli pd activate name=&lt;PD NAME&gt;</p>

<h5>Включение теневых клонов для DSF</h5>

 <p class="codedescription">Описание: Включение функции создания теневых клонов DSF</p>

<p class="codetext">ncli cluster edit-params enable-shadow-clones=true</p>

<h5>Активация функции дедупликации для vDisk</h5>

 <p class="codedescription">Описание: Включение функции создания снимков и дедупдликации для конкретного vDisk</p>

<p class="codetext">ncli vdisk edit name=&lt;VDISK NAME&gt; fingerprint-on-write=&lt;true/false&gt; on-disk-dedup=&lt;true/false&gt;</p>

<h5>Проверка состояния отказоуствойчивости кластера</h5>
<p class="codetext">
  # Статус узлов<br />
  ncli cluster get-domain-fault-tolerance-status type=node
</p>
<p class="codetext">
  # Статус блоков <br />
  ncli cluster get-domain-fault-tolerance-status type=rackable_unit
</p>
</section>

<section data-type="sect1" id="PowerShell-cmdlets-mkIgIjFr">
<h3>Командлеты для PowerShell</h3>

<p>Ниже мы рассмотрим командлеты для PowerShell от Nutanix и то как их использовать, а так же остановимся на некоторых основных моментах касаемо Windows PowerShell.</p>

<h5>Основы</h5>

<p>Windows PowerShell это мощная командная оболочка (отсюда и название ;P) а так же встроенный язык для автоматизации на основе фреймворка .NET&nbsp; Это очень простой в использовании язык, созданный быть интерактивным и интуитивным.&nbsp; Ключевые элементы конструкций PowerShell:</p>

<h5>CMDlets (командлеты)</h5>

<p>CMDlets это команды или классы .NET которые выполняют те или иные операции.&nbsp; Они следуют методологии получения и отправки запросов и использую структуру &lt;Глагол&gt;-&lt;Существительное&gt;.&nbsp; Например: Get-Process, Set-Partition, и так далее.</p>

<h5>Конвейеры</h5>

<p>Конвейеры важная конструкция в PowerShell (так же как и в Linux) и может очень упростить некоторые конструкции.&nbsp; С конвейером вы можете передавать результат одной секции конвейера в следующую, в качестве входных параметров.&nbsp; Конвейер может иметь такую длину, как вам необходимо. Простым примером конвейера может быть получение списка процессов, поиск конкретных по фильтру и сортировка результата:</p>  

<p class="codetext">Get-Service | where {$_.Status -eq "Running"} | Sort-Object Name</p>

<p>Кроме того, конвейер может быть использован для каждого значения в массиве, например:</p>

<p class="codetext"># Для каждого значения в массиве
<br>
$myArray | %{
<br>
&nbsp; # Выполнить какие-то действия
<br>
}</p>

<h5>Ключевые типы объектов</h5>

<p>Ниже рассмотрим несколько ключевых объектов PowerShell.&nbsp; 
  Тип объекта можно получить через метод .getType(), например: $someVariable.getType() вернет тип для объекта.</p>

<h5>Переменные</h5>

<p class="codetext">$myVariable = "foo"</p>

<p class="note">Примечание: Можно поместить в переменную конвейер:</p>

<p class="codetext">$myVar2 = (Get-Process | where {$_.Status -eq "Running})</p>

<p>В этом примере переменная получит результат выполнения команды в качестве значения.</p>

<h5>Массив</h5>

<p class="codetext">$myArray = @("Value","Value")</p>

<p class="note">Примечание: Можно создать массив из массивов, хэш-таблиц или пользовательских типов</p>

<h5>Хэш-таблица</h5>

<p class="codetext">$myHash = @{"Key" = "Value";"Key" = "Value"}</p>

<h5>Полезные команды</h5>

<p>Получение справки по конкретному командлету (так же как man в Linux)</p>

<p class="codetext">Get-Help &lt;имя командлета&gt;</p>

<p class="codetext">Пример: Get-Help Get-Process</p>

<p>Список свойств и методов команды или объекта</p>

<p class="codetext">&lt;Вырожение или объект&gt; | Get-Member</p>

<p class="codetext">Пример: $someObject | Get-Member</p>

<h5>Основные командлеты Nutanix и их использование</h5>

<p>Загрузить установщик командлетов Nutanix можно из Prism UI (после версии 4.0.1):</p>

<figure class="small" id="id-pltZS0I8FO"><img alt="Prism CMDlets Installer Link" class="iimagesv2cmdlets_dlpng" src="imagesv2/cmdlets_dl.png">
<figcaption><span class="label">Рисунок 8-3. </span>Ссылка на скачивание командлетов</figcaption>
</figure>

<h5>Загрузка оснастки Nutanix</h5>

<p>Проверьте загружена ли оснастка Nutanix, если нет - загрузите</p>

<p class="codetext">if ( (Get-PSSnapin -Name NutanixCmdletsPSSnapin -ErrorAction SilentlyContinue) -eq $null )
<br>
{
<br>
&nbsp;&nbsp;&nbsp; Add-PsSnapin NutanixCmdletsPSSnapin
<br>
}</p>

<h5>Список командлетов Nutanix</h5>

<p class="codetext">Get-Command | Where-Object{$_.PSSnapin.Name -eq "NutanixCmdletsPSSnapin"}</p>

<h5>Подключение к кластеру Acropolis</h5>

<p class="codetext">Connect-NutanixCluster -Server $server -UserName "myuser" -Password (Read-Host "Password: " -AsSecureString) -AcceptInvalidSSLCerts</p>

<h5>Получение списка ВМ по маске</h5>

<p>Объявление переменной</p>

<p class="codetext">$searchString = "myVM"
<br>
$vms = Get-NTNXVM | where {$_.vmName -match $searchString}</p>

<p>Выполнение</p>

<p class="codetext">Get-NTNXVM | where {$_.vmName -match "myString"}</p>

<p>Выполнение и форматирование</p>

<p class="codetext">Get-NTNXVM | where {$_.vmName -match "myString"} | ft</p>

<h5>Получение объектов хранилища</h5>

<p>Объявление переменной</p>

<p class="codetext">$vdisks = Get-NTNXVDisk</p>

<p>Выполнение</p>

<p class="codetext">Get-NTNXVDisk</p>

<p>Выполнение и форматирование</p>

<p class="codetext">Get-NTNXVDisk | ft</p>

<h5>Получение списка контейнеров Nutanix</h5>

<p>Объявление переменной</p>

<p class="codetext">$containers = Get-NTNXContainer</p>

<p>Выполнение</p>

<p class="codetext">Get-NTNXContainer</p>

<p>Выполнение и форматирование</p>

<p class="codetext">Get-NTNXContainer | ft</p>

<h5>Получение списка доменов защиты</h5>

<p>Объявление переменной</p>

<p class="codetext">$pds = Get-NTNXProtectionDomain</p>

<p>Выполнение</p>

<p class="codetext">Get-NTNXProtectionDomain</p>

<p>Выполнение и форматирование</p>

<p class="codetext">Get-NTNXProtectionDomain | ft</p>

<h5>Получение списка групп консистентности</h5>

<p>Объявление переменной</p>

<p class="codetext">$cgs = Get-NTNXProtectionDomainConsistencyGroup</p>

<p>Выполнение</p>

<p class="codetext">Get-NTNXProtectionDomainConsistencyGroup</p>

<p>Выполнение и форматирование</p>

<p class="codetext">Get-NTNXProtectionDomainConsistencyGroup | ft</p>

<h5>Ресурсы и скрипты:</h5>

<ul>
	<li>Nutanix Github - <a href="https://github.com/nutanix/Automation" target="_blank">https://github.com/nutanix/Automation</a></li>
	<li>Manually Fingerprint vDisks - <a href="http://bit.ly/1syOqch" target="_blank">http://bit.ly/1syOqch</a></li>
	<li>vDisk Report - <a href="http://bit.ly/1r34MIT" target="_blank">http://bit.ly/1r34MIT</a></li>
	<li>Отчет о доменах защиты - <a href="http://bit.ly/1r34MIT" target="_blank">http://bit.ly/1r34MIT</a></li>
	<li>Восстановление доменов защиты по очереди - <a href="http://bit.ly/1pyolrb" target="_blank">http://bit.ly/1pyolrb</a></li>
</ul>

<p>
Примечание: некоторые скрипты не поддерживаются и могут быть использованы только в качестве примера.
</p>

<p>Больше скриптов доступно на официальном гите Nutanix <a href="https://github.com/nutanix" target="_blank">https://github.com/nutanix</a></p>
</section>
</section>

<section data-type="chapter" id="integrations-lkIJt8">
<h2>Интеграция</h2>

<section data-type="sect1" id="openstack-M2IosAtq">
<h3>OpenStack</h3>

<p><a href="https://www.openstack.org/">OpenStack </a>это открытая платформа для реализации и управления облаками.&nbsp; Он состоит из пользовательского интерфейса, API и инфраструктурных сервисов (сервис вычисления, хранения, и так далее).</p>

<p>Решение Nutanix OpenStack состоит из нескольких основных компонент:</p>

<ul>
	<li>OpenStack Controller (OSC)
		<ul>
			<li>Существующая или вновь созданная ВМ, на которой размещаются сервисы OpenStack - UI, API и служебные сервисы. Тут обрабатываются все вызовы к API OpenStack. Если в качестве платформы используется Acropolis тут же может размещаться и драйвер Acropolis OpenStack</li>
		</ul>
	</li>
  <li>Acropolis OpenStack Driver
    <ul>
        <li>Отвечает за обработку вызовов OpenStack RPCs от контроллера OpenStack и трансляции их в стандартный API Acropolis. Может быть развернут в рамках OCS, OVM или на отдельной ВМ.</li>
    </ul>  
  </li>
  <li>Служебная ВМ - Acropolis OpenStack Services (OVM)
		<ul>
			<li>ВМ на которой размещаются драйверы Acropolis. Отвечает за получение вызовов OpenStack RPCs от контроллера OpenStack и трансляцией их в стандартный API Acropolis.</li>
		</ul>
	</li>
</ul>

<p>OSC может располагаться на существующей ВМ или сервере, или может быть развернут как часть решения Nutanix OpenStack.  OVM - является вспомогательной ВМ, и  разворачивается как часть решения Nutanix OpenStack.</p>

<p>Клиенты взаимодействуют с OSC используя стандартные методы и способы (Web UI / HTTP, SDK, CLI or API), OSC взаимодействует с OVM которая преобразует запросы OpenStack API в стандартные вызовы REST API Acropolis, через Acropolis OpenStack Driver.</p>

<p>Верхнеуровневая схема взаимодействия компонент:</p>

<figure id="id-0OtMtbsQtW"><img alt="OpenStack + Acropolis OpenStack Driver" src="imagesv2/openstack_overview2.png">
<figcaption><span class="label">Рисунок 9-1. </span>OpenStack + Acropolis OpenStack Driver</figcaption>
</figure>

<p>
  Такое решение позволяет использовать все преимущества OpenStack - пользовательский портал, API и так далее, без сложностей присущих классическим инсталляциям OpenStack. Все низкоуровневые сервисы (вычисление, хранение, сети) реализуются стандартными средствами Nutanix. А значит, нет необходимости разворачивать компоненты вроде Nova Compute, и так далее. Платформа будет принимать все стандартные запросы, и перенаправлять их к компонентам Nutanix, транслируя в вызовы Acropolis API. Кроме того, такое решение можно развернуть менее чем за 30 минут.
</p>

<div data-type="note" class="note" id="supported-openstack-controllers-kpi1fysgtX">
<h5>Поддерживаемые контроллеры OpenStack</h5>

<p>На текущий момент (начиная с 4.5.1) поддерживаются версии OpenStack начиная с Kilo и более поздние.</p>
</div>

<p>Верхнеуровневый маппинг ролей:</p>

<table>
  <tr>
    <th>Объект</th>
    <th>Роль</th>
    <th>Контроллер OpenStack</th>
    <th>Acropolis OVM</th>
    <th>Кластер Acropolis</th>
    <th>Prism</th>
  </tr>
  <tr>
    <td>Пользовательский интерфейс</td>
    <td>Пользовательский интерфейс и API</td>
    <td>X</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Административный интерфейс</td>
    <td>Мониторинг и управление инфраструктурой</td>
    <td>X</td>
    <td></td>
    <td></td>
    <td>X</td>
  </tr>
  <tr>
    <td>Оркестрация</td>
    <td>Управление объектами (CRUD) и жизненным циклом</td>
    <td>X</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Квоты</td>
    <td>Управоение ресурсами и лимитами на их использование</td>
    <td>X</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Пользователи, Группы и Роли</td>
    <td>Управление на базе ролей пользователей (RBAC)</td>
    <td>X</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Единая точка входа</td>
    <td>Сквозная авторизация</td>
    <td>X</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Интеграция с платформой</td>
    <td>Интеграция OpenStack и Nutanix</td>
    <td></td>
    <td>X</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>Инфраструктурные сервисы</td>
    <td>Целевая инфраструктура (вычисление, хранение, сеть)</td>
    <td></td>
    <td></td>
    <td>X</td>
    <td></td>
  </tr>
</table>

<section data-type="sect2" id="openstack-components-vlIPIGs4tE">
<h4>Компоненты OpenStack</h4>

<p>OpenStack состоит из набора компонент, каждый компонент отвечает за какую-то инфраструктурную функцию. Некоторые из этих функций будут развернуты на базе контроллера OpenStack Controller, некоторые на базе Acropolis OVM.</p>

<p>Таблица показывает соотношение компонентов OpenStack и их инфраструктурных ролей:</p>

<table>
  <tr>
    <th>Компонент</th>
    <th>Роль</th>
    <th>Контролер OpenStack</th>
    <th>Acropolis OVM</th>
  </tr>
  <tr>
    <td>Keystone</td>
    <td>Сервис идентификации</td>
    <td>X</td>
    <td></td>
  </tr>
  <tr>
    <td>Horizon</td>
    <td>Веб-интерфейс</td>
    <td>X</td>
    <td></td>
  </tr>
  <tr>
    <td>Nova</td>
    <td>Вычисление</td>
    <td></td>
    <td>X</td>
  </tr>
  <tr>
    <td>Swift</td>
    <td>Объектное хранилище</td>
    <td>X</td>
    <td>X</td>
  </tr>
  <tr>
    <td>Cinder</td>
    <td>Блочное хранилище</td>
    <td></td>
    <td>X</td>
  </tr>
  <tr>
    <td>Glance</td>
    <td>Хранилище образов</td>
    <td>X</td>
    <td>X</td>
  </tr>
  <tr>
    <td>Neutron</td>
    <td>Сеть</td>
    <td></td>
    <td>X</td>
  </tr>
	<tr>
		<td>Heat</td>
		<td>Оркестрация</td>
		<td>X</td>
		<td></td>
	</tr>
	<tr>
    <td>Другие</td>
    <td>Другие компоненты</td>
    <td>X</td>
    <td></td>
  </tr>
</table>

<p>На рисунке отображено взаимодействие между компонентами OpenStack:</p>

<figure id="id-2btWHXIosBtD"><img alt="OpenStack + Nutanix API Communication" class="iimagesv2openstack_commarch2png" src="imagesv2/openstack_commarch4.png">
<figcaption><span class="label">Рисунок 9-2. </span>Взаимодействие OpenStack + Nutanix API</figcaption>
</figure>

<p>В следующем разделе мы остановимся на ключевых компонентах OpenStack и расскажем как они интегрированы с Nutanix.</p>

<h5>Nova</h5>
<p>Nova - подсистема управления вычислительными узлами и задачами. В решении Nutanix OpenStack каждая OVM выступает в роли вычислительного узла, так каждый Acropolis Cluster и узел внутри него могут быть использованы для создания ВМ OpenStack. Acropolis OVM управляет сервисом Nova.</p>

<p>Все сервисы Nova доступны в веб-интерфейсе OpenStack на странице 'Admin'-&gt;'System'-&gt;'System Information'-&gt;'Compute Services'.</p>

 <p>На рисунке показаны сервисы Nova, узлы и их статус:</p>

<figure class="large" id="id-J2tNIjI8sAty"><img alt="OpenStack Nova Services" src="imagesv2/openstack_nova_services.png">
<figcaption><span class="label">Рисунок 9-3. </span>OpenStack - Сервисы Nova</figcaption>
</figure>

<p>Планировщик ресурсов Nova определяет на каком вычислительном узле (он же OVM) разместить ВМ на базе выбранных зон доступности. Такой запрос будет отправлен к выбранной OVM, а затем перенаправлен конкретному планировщику Acropolis (отвечающему за конкретный кластер). Планировщик Acropolis определит оптимальный узел для размещения ВМ внутри кластера. Конкретные узлы кластера Acropolis не отображаются в OpenStack. </p>

<p>Вы можете увидеть доступные "вычислительные узлы" на портале OpenStack во вкладке 'Admin'-&gt;'System'-&gt;'Hypervisors'.</p>

<p>На рисунке ниже отображен перечень Acropolis OVM как список вычислительных узлов OpenStack:</p>

<figure id="id-n3tOsPIos3te"><img alt="OpenStack Compute Host" src="imagesv2/openstack_nova_computehost.png">
<figcaption><span class="label">Рисунок 9-4. </span>OpenStack - Вычислительные узлы</figcaption>
</figure>

<p>На рисунке ниже показан кластер Acropolis, OpenStack распознает его как гипервизор:</p>

<figure class="large" id="id-rQtPT2I9sqtD"><img alt="OpenStack Hypervisor Host" src="imagesv2/openstack_nova_hypervisorhost.png">
<figcaption><span class="label">Рисунок 9-5. </span>OpenStack - Гипервизоры</figcaption>
</figure>

<p>
	Как видно на рисунке выше - целый кластер Nutanix представлен как единственный гипервизор.
</p>

<h5>Swift</h5>
<p>
    Swift - объектное хранилище, используемое для хранения файлов. Сейчас оно используется исключительно для создания и восстановления снимков и образов.
</p>

<h5>Cinder</h5>
<p>Cinder - компонент управления хранилищем для взаимодействия с iSCSI. Cinder использует API по управлению томами в Acropolis. Эти тома будут подключены к ВМ напрямую, в качестве блочных устройств.
</p>

<p>
	Вы можете увидеть перечень сервисов Cinder в интерфейсе OpenStack, в меню 'Admin'-&gt;'System'-&gt;'System Information'-&gt;'Block Storage Services'.
</p>

<p>На рисунке показаны сервисы Cinder, узел и статус:</p>

<figure class="large" id="id-YatRIzIDs3tG"><img alt="OpenStack Cinder Services" src="imagesv2/openstack_cinder_services.png">
<figcaption><span class="label">Рисунок 9-6. </span>OpenStack - Сервисы Cinder</figcaption>
</figure>

<h5>Glance - репозиторий образов</h5>
<p>
	Glance это хранилище образов в инфраструктуре OpenStack. Через данный модуль происходит выдача информации о доступных для использования образов ВМ, снимков и ISO.
</p>
<p>
  Репозиторий образов - хранилище образов под управлением Glance. Репозиторий может располагаться как в рамках платформы Nutanix, так и вне ее. Если образы хранятся на платформе Nutanix, они автоматически будут опубликован в OpenStack посредством Glance на OVM. Если хранилище образов настроено как внешний источник, Glance будет располагаться на контроллере OpenStack, а кэш образов будет использоваться кластером Acropolis.
  
</p>
<p>
  Модуль Glance разворачивается для каждого кластера Acropolis и всегда содержит кэш образов. Когда Glance активирован на нескольких кластерах Nutanix - репозиторий образов будет охватывать все эти кластеры, и образы, создаваемые через портал OpenStack будут распространены по всем кластерам с Glance. Кластеры без сервиса Glance будут держать кэши образов при помощи кэша образов. 
</p>

<div data-type="note" class="note" id="pro-tip-oZibumIWs2tJ"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>В рамках больших инсталляций сервис Glance должен функционировать как минимум на двух кластерах Acropolis на площадку. Такая инсталляция обеспечит HA для репозитория образов даже для случая, и образы всегда будут доступны, даже если их нет в кэше.</p>
</div>

<p>
  Когда в качестве репозитория образов используется внешний ресурс, задачу доставки образов на кластер Acropolis берет на себя сервис Nova. Все необходимые образы будут перенесены в кэш образов, для быстрого доступа к ним, в случае необходимости.
</p>

<h5>Neutron</h5>
<p>
  Сервис Neutron - сетевой компонент OpenStack который отвечает за настройку сети ВМ. OVM позволяет выполнять все CRUD операции, выполняемые из интерфейса OpenStack и осуществлять все необходимые задачи в Acropolis.
</p>

<p>
	Вы можете посмотреть список сервисов Neutron через пользовательский интерфейс OpenStack, в меню 'Admin'-&gt;'System'-&gt;'System Information'-&gt;'Network Agents'.
</p>

<p>На изображении показаны сервисы Neutron, узлы и статусы:</p>

<figure class="large" id="id-VrtXf2INsZtD"><img alt="OpenStack Neutron Services" src="imagesv2/openstack_neutron_services.png">		
  +	На текущий момент поддерживаются только два типа сети Local и VLAN.
<figcaption><span class="label">Рисунок 9-7. </span>OpenStack - сервисы Neutron</figcaption>
</figure>
<p>
    Сервис Neutron будет присваивать IP-адрес ВМ при каждой перезагрузке. В этом случае Acropolis резервирует требуемый адрес за ВМ которой он выдается. Когда ВМ выполняет DHCP запрос - Acropolis ответит на него в рамках VXLAN, как обычно в AHV.
</p>

<div data-type="note" class="note" id="supported-network-types-05iAcpIrsEtB"><h6>Примечание</h6>
<h5>Поддерживаемые типы сетей</h5>

<p>На текущий момент поддерживаются только два типа сети Local и VLAN.</p>
</div>

<p>Сервис Keystone и веб-интерфейс Horizon размещаются в OpenStack Controller в рамках Acropolis OVM. OVM содержит драйвер OpenStack отвечающий за трансляцию вызовов OpenStack API в стандартный Acropolis API.</p>
</section>
<section data-type="sect2" id="design-and-deployment-jrIJU1swtX">
<h4>Проектирование и инсталляция</h4>

<p>
    Для масштабных инсталляций облачных инфраструктур важно использовать максимально распределенную инфраструктуру, которая будет отвечать запросам пользователя. И обеспечивать гибкость и локализацию вычислений и данных.
</p>

<p>
  OpenStack имеет следующие высокоуровневые объекты:
</p>

<ul>
	<li>
		Регионы (Region)
		<ul>
			<li>
          Географический объект или область, включающие в себя по несколько зон доступности. Например, регионы - US-Northwest или US-West
			</li>
		</ul>
	</li>
	<li>
		Зоны доступности (Availability Zone)
		<ul>
			<li>
          Специфический ЦОД или инсталляция, где размещаются облачные сервисы. Может включать площадки, например - US-Northwest-1 или US-West-1
			</li>
		</ul>
	</li>
	<li>
		Группы узлов (Host Aggregate)
		<ul>
			<li>
				Группа вычислительных узлов - узел, стойка или же может соответствовать по размеру зоне доступности.
			</li>
		</ul>
	</li>
	<li>
		Вычислительные узлы (Compute Host)
		<ul>
			<li>
        Acropolis OVM, где запущены сервисы Nova.
			</li>
		</ul>
	</li>
	<li>
		Гипервизоры (Hypervisor Host)
		<ul>
			<li>
				Кластер Acropolis (выглядит как один узел с гипервизором).
			</li>
		</ul>
	</li>
</ul>

<p>
	Ниже показаны связи между этими компонентами:
</p>

<figure id="id-3rtxHwUOsdtk"><img alt="OpenStack - Deployment Layout" src="imagesv2/openstack_regions.png">
<figcaption><span class="label">Рисунок 9-8. </span>OpenStack - Инсталляция</figcaption>
</figure>
<p>
	Пример размещения приложений:
</p>

<figure id="id-lNtJtJUrsjtv"><img alt="OpenStack - Deployment Layout - Example" src="imagesv2/openstack_regions_example.png">
<figcaption><span class="label">Рисунок 9-9. </span>OpenStack - Пример инсталляции</figcaption>
</figure>
<p>
	Перечень вычислительных узлов, групп и зон доступности можно посмотреть в меню интерфейса OpenStack: 'Admin'-&gt;'System'-&gt;'Host Aggregates'.
</p>

<p>На рисунке изображены группы узлов, зоны доступности и вычислительные узлы непосредственно:</p>

<figure class="large" id="id-J2t5cvU8sAty"><img alt="OpenStack Host Aggregates and Availability Zones" src="imagesv2/openstack_hostagg_az.png">
<figcaption><span class="label">Рисунок 9-10. </span>OpenStack - группы узлов и зоны доступности</figcaption>
</figure>

<section data-type="sect3" id="services-design-and-scaling-okI9IMUWs2tJ">
<h4>Проектирование и масштабирование</h4>

<p>Для масштабных инсталляций рекомендуется использовать несколько Acropolis OVMs подключенных к OpenStack Controller и балансировку нагрузки между ними. Такой подход обеспечивает HA и распределяет нагрузку между несколькими OVM. OVM не содержат данных, по этому их количество может легко увеличиваться.</p>

<p>Ниже представлен пример схемы с использованием нескольких OVM в рамках одного ЦОД:</p>

<figure id="id-n3tkTPIzUBsNt1"><img alt="OpenStack - OVM Load Balancing" src="imagesv2/openstack_svmha2.png">
<figcaption><span class="label">Рисунок 9-11. </span>OpenStack - Балансировка нагрузки OVM</figcaption>
</figure>

<p>
	В качестве балансировщика можно использовать такие решения, как Keepalived и HAproxy.
</p>

<p>Для инсталляций в рамках нескольких ЦОД контроллер OpenStack будет взаимодействовать с несколькими OVM на разных площадках.</p>

<p>На рисунке пример схемы для нескольких ЦОД:</p>
<figure id="id-WmtatqIzUesDtk"><img alt="OpenStack - Multi-Site" src="imagesv2/openstack_siteha2.png">
<figcaption><span class="label">Рисунок 9-12. </span>OpenStack - Несолько ЦОД</figcaption>
</figure>
</section>
<h4>Инсталляция</h4>
<p>
  OVM может быть развернут как ПО в виде RPM, в рамках CentOS / Redhat или же как ВМ целиком. Acropolis OVM может инсталлироваться на любую платформу - Nutanix или не-Nutanix, но должен иметь доступ к контроллеру OpenStack и кластеру Nutanix.
</p>
<p>
  ВМ для Acropolis OVM может быть развернута на кластере Nutanix AHV следуя шагам, описанным ниже. Если OVM уже существует, этот пункт можно пропустить. Можно использовать готовый образ OVM или же использовать другую ВМ с CentOS / Redhat.
</p>
<p>
  Для начала требуется импортировать все диски Acropolis OVM на кластер Acropolis. Это можно сделать, скопировав образы дисков с помощью SCP или же загрузить их по URL. Мы рассмотрим вариант с импортом образов с использованием API.  Примечание: Как и говорилось ранее - можно развернуть эти ВМ где угодно, не обязательно на кластере Acropolis.
</p>

<p>
	Для импорта образов требуется выполнить следующие команды:
</p>
<p class="codetext">image.create &lt;IMAGE_NAME&gt; source_url=&lt;SOURCE_URL&gt; container=&lt;CONTAINER_NAME&gt;</p>
<p>Теперь нужно создать ВМ для ПО OVM выполнив следующие команды ACLI на любой из CVM, в рамках кластера:</p>
<p class="codetext">vm.create &lt;VM_NAME&gt; num_vcpus=2 memory=16G<br>
vm.disk_create &lt;VM_NAME&gt; clone_from_image=&lt;IMAGE_NAME&gt;<br>
vm.nic_create &lt;VM_NAME&gt; network=&lt;NETWORK_NAME&gt;<br>
vm.on &lt;VM_NAME&gt;
</p>

<p>После создания OVM - включите ее, подключиться по SSH и ввести данные учетной записи.</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Справка OVMCTL</h5>

<p>Справочаня информация доступна при выполнении команды на OVM:</p>
  <p class="codetext">ovmctl --help </p>

</div>

<p>
  OVM поддерживает следующие типы инсталляции:
</p>

<ul>
  <li>
    OVM-все-в-одном
    <ul>
      <li>
        OVM включает все драйверы Acropolis и контроллер OpenStack
      </li>
    </ul>
  </li>
  <li>
    OVM-как-сервис
    <ul>
      <li>
        OVM включает в себя все драйверы Acropolis и взаимодействует с внешним контроллером OpenStack
      </li>
    </ul>
  </li>
</ul>

<p>
  Оба типа инсталляции расписаны в разделе ниже. Вы можете использовать любой тип инсталляции, а так же переключаться между ними.
</p>

<h5>OVM-все-в-одном</h5>
<p>
  Следующие шаги описывают инсталляцию OVM-все-в-одном. Для настройки требуется подключиться по SSH к OVM(s) и выполнить перечисленные команды.
</p>

<p class="codetext">
  # Регистрация сервиса драйвера OpenStack <br>
	ovmctl --add ovm --name &lt;OVM_NAME&gt; --ip &lt;OVM_IP&gt; --netmask &lt;NET_MASK&gt; --gateway &lt;DEFAULT_GW&gt; --domain &lt;DOMAIN&gt; --nameserver &lt;DNS&gt;
</p>

<p class="codetext">
# Регистрация контроллера OpenStack<br>
ovmctl --add controller --name &lt;OVM_NAME&gt; --ip &lt;OVM_IP&gt;
</p>

<p class="codetext">
# Регистрация кластера Acropolis <br>
ovmctl --add cluster --name &lt;CLUSTER_NAME&gt; --ip &lt;CLUSTER_IP&gt; --username &lt;PRISM_USER&gt; --password &lt;PRISM_PASSWORD&gt;
		<br>
		<br>
Следующие значения являются стандартными: <br>
    <span style="margin-left:2em">Количество VCPUs на ядро = 4</span><br>
    <span style="margin-left:2em">Стандартное имя контейнера = default</span><br>
    <span style="margin-left:2em">Кэш образов = disabled, URL адрес кэша образов = None</span><br>
</p>

<p>
  Проверяем правильность конфигурации:
</p>

<p class="codetext">
  ovmctl --show
</p>

<p>
  Если команда вернет информацию - значит все хорошо.
</p>

<h5>OVM-как-сервис</h5>
<p>
    Следующие шаги описывают инсталляцию OVM в виде сервиса. Для настройки требуется подключиться по SSH к OVM(s) и выполнить перечисленные команды.
</p>

<p class="codetext">
  # Регистрация сервиса драйвера OpenStack <br>
	ovmctl --add ovm --name &lt;OVM_NAME&gt; --ip &lt;OVM_IP&gt;
</p>

<p class="codetext">
# Регистрация контроллера OpenStack <br>
ovmctl --add controller --name &lt;OS_CONTROLLER_NAME&gt; --ip &lt;OS_CONTROLLER_IP&gt; --username &lt;OS_CONTROLLER_USERNAME&gt; --password &lt;OS_CONTROLLER_PASSWORD&gt;
			<br>
			<br>
  Следующие значения являются стандартными:<br>
      <span style="margin-left:2em">Аутентификация: auth_strategy = keystone, auth_region = RegionOne</span><br>
      <span style="margin-left:4em">auth_tenant = services, auth_password = admin</span><br>
      <span style="margin-left:2em">База данных: db_{nova,cinder,glance,neutron} = mysql, db_{nova,cinder,glance,neutron}_password = admin</span><br>
      <span style="margin-left:2em">RPC: rpc_backend = rabbit, rpc_username = guest, rpc_password = guest</span>
</p>

<p class="codetext">
# Регистрация кластеров Acropolis<br>
ovmctl --add cluster --name &lt;CLUSTER_NAME&gt; --ip &lt;CLUSTER_IP&gt; --username &lt;PRISM_USER&gt; --password &lt;PRISM_PASSWORD&gt;
		<br>
		<br>
    Следующие значения являются стандартными: <br>
    <span style="margin-left:2em">Количество VCPUs на ядро = 4</span><br>
    <span style="margin-left:2em">Стандартное имя контейнера = default</span><br>
    <span style="margin-left:2em">Кэш образов = disabled, URL адрес кэша образов = None</span><br>
</p>

<p>
  Если при инсталляции контроллера OpenStack используются нестандартные пароли необходимо обновить их:
</p>

<p class="codetext">
# Обновление паролей контроллера<br>
ovmctl --update controller --name &lt;OS_CONTROLLER_NAME&gt; --auth_nova_password &lt;&gt; --auth_glance_password &lt;&gt; --auth_neutron_password &lt;&gt; --auth_cinder_password &lt;&gt; --db_nova_password &lt;&gt; --db_glance_password &lt;&gt; --db_neutron_password &lt;&gt; --db_cinder_password &lt;&gt;
</p>

<p>
    Проверяем правильность конфигурации:
</p>

<p class="codetext">
  ovmctl --show
</p>

<p>
	Теперь, когда OVM настроена, требуется настроить контроллер OpenStack на использование Glance и Neutron.
</p>

<p>
  Подключитесь к контроллеру OpenStack и введите следующие команды:
</p>

<p class="codetext">
  # enter keystonerc_admin <br />
  source ./keystonerc_admin
</p>

<p>
  Для начала требуется удалить существующие записи:
</p>

<p class="codetext">
 # Определите старый id сервиса Glance (порт 9292) <br>
 keystone endpoint-list
 # Удалите старую запись <br>
 keystone endpoint-delete &lt;GLANCE_ENDPOINT_ID&gt;
</p>

<p>
  Теперь нужно создать новую запись Glance которая будет указывать на OVM:
</p>

<p class="codetext">
  # Определите id сервиса Glance<br>
  keystone service-list | grep glance<br>
  # Вывод будет похож на:<br>
| 9e539e8dee264dd9a086677427434982 |   glance   |      image      |<br>
<br>
  # Добавьте запись <br>
  keystone endpoint-create \ <br>
  --service-id &lt;GLANCE_SERVICE_ID&gt; \ <br>
  --publicurl http://&lt;OVM_IP&gt;:9292 \ <br>
  --internalurl http://&lt;OVM_IP&gt;:9292 \ <br>
  --region &lt;REGION_NAME&gt; \ <br>
  --adminurl http://&lt;OVM_IP&gt;:9292
</p>

<p>
  Теперь нужно удалить запись Neutron, указывающую на контроллер:
</p>

<p class="codetext">
# Определите id старого сервиса Neutron (порт 9696) <br />
keystone endpoint-list
# Удалите старую запись Neutron <br>
keystone endpoint-delete &lt;NEUTRON_ENDPOINT_ID&gt;
</p>

<p>
  Далее - создать новую запись для Neutron, которая будет указывать на OVM:
</p>

<p class="codetext">
  # Определите id сервиса Neutron<br>
  keystone service-list | grep neutron<br>
  # Вывод будет похож на:<br>
| f4c4266142c742a78b330f8bafe5e49e |  neutron   |     network     |<br>
<br>
# Добавьте новую запись Neutron <br>
keystone endpoint-create \ <br>
--service-id &lt;NEUTRON_SERVICE_ID&gt; \ <br>
--publicurl http://&lt;OVM_IP&gt;:9696 \ <br>
--internalurl http://&lt;OVM_IP&gt;:9696 \ <br>
--region &lt;REGION_NAME&gt; \ <br>
--adminurl http://&lt;OVM_IP&gt;:9696
</p>

<p>
  После обновления записей нужно обновить конфигурацию сервисов Nova и Cinder, указать адрес OVM в качестве адреса Glance сервиса.
<p>
	Сначала нужно внести изменения в файл Nova.conf - /etc/nova/nova.conf:
</p>
<p class="codetext">
	[glance] <br>
	... <br>
  # Default glance hostname or IP address (string value) <br>
  host=&lt;OVM_IP&gt; <br>
  <br>
  # Default glance port (integer value) <br>
  port=9292 <br>
	... <br>
  # A list of the glance api servers available to nova. Prefix <br>
  # with https:// for ssl-based glance api servers. <br>
  # ([hostname|ip]:port) (list value) <br>
  api_servers=&lt;OVM_IP&gt;:9292
</p>

<p>
  Далее - отключить сервис nova-compute на контроллере OpenStack (если еще не отключен):
</p>
<p class="codetext">
systemctl disable openstack-nova-compute.service<br>
systemctl stop openstack-nova-compute.service<br>
service openstack-nova-compute stop<br>
</p>
<p>
	Затем внести изменения в файл /etc/cinder/cinder.conf :
</p>
<p class="codetext">
	# Default glance host name or IP (string value) <br>
  glance_host=&lt;OVM_IP&gt; <br>
  # Default glance port (integer value) <br>
  glance_port=9292 <br>
  # A list of the glance API servers available to cinder <br>
  # ([hostname|ip]:port) (list value) <br>
  glance_api_servers=$glance_host:$glance_port <br>
</p>
<p>
  Так же требуется закомментировать строки:
</p>
<p class="codetext">
  # Comment out the following lines in cinder.conf<br>
#enabled_backends=lvm<br>
#[lvm]<br>
#iscsi_helper=lioadm<br>
#volume_group=cinder-volumes<br>
#iscsi_ip_address=<br>
#volume_driver=cinder.volume.drivers.lvm.LVMVolumeDriver<br>
#volumes_dir=/var/lib/cinder/volumes<br>
#iscsi_protocol=iscsi<br>
#volume_backend_name=lvm<br>
</p>
<p>
  Затем отключить сервис cinder на контроллере OpenStack:
</p>
<p class="codetext">
  systemctl disable openstack-cinder-volume.service<br>
  systemctl stop openstack-cinder-volume.service<br>
  service openstack-cinder-volume stop<br>
</p>
<p>
  Затем отключить сервис glance-image на контроллере OpenStack:
</p>
<p class="codetext">
  systemctl disable openstack-glance-api.service <br>
systemctl disable openstack-glance-registry.service<br>
systemctl stop openstack-glance-api.service <br>
systemctl stop openstack-glance-registry.service<br>
service openstack-glance-api stop<br>
service openstack-glance-registry stop<br>
</p>

<p>
  После изменения конфигурационных файлов требуется перезагрузить сервисы Nova и Cinder, чтобы изменения вступили в силу.
  Это можно сделать или используя команды ниже или запустив скрипты, которые доступны для загрузки.
</p>

<p class="codetext">
	# Перезапуск сервисов Nova<br>
	service openstack-nova-api restart<br>
	service openstack-nova-consoleauth restart<br>
	service openstack-nova-scheduler restart<br>
	service openstack-nova-conductor restart<br>
	service openstack-nova-cert restart<br>
	service openstack-nova-novncproxy restart<br>
	<br>
	# или запустите скрипт:<br>
	~/openstack/commands/nova-restart <br>
	<br>
	# Перезапуск сервисов Cinder <br>
	service openstack-cinder-api restart<br>
	service openstack-cinder-scheduler restart<br>
	service openstack-cinder-backup restart <br>
	<br>
	# или запустите скрипт:<br>
	~/openstack/commands/cinder-restart
</p>

<!-- TODO - add iptables rules -->

<!-- End of openstack section -->
</section>

<section data-type="sect1" id="puppet-M2IosAtq">
<h3>Puppet</h3>

<p>
  Puppet - решение для управления жизненным циклом конфигураций (LCM) которое широко используется в DevOps.
</p>

<!-- Eng of puppet section -->
</section>

<section data-type="sect2" id="troubleshooting-andamp-advanced-administration-yAIkhjswtx">
<h4>Поиск и устранение неисправностей</h4>

<section data-type="sect3" id="key-log-locations-eqIlsmhJsJtN">
<h5>Расположение основных журнальных файлов</h5>
<table>
  <tr>
    <th>Компонент</th>
    <th>Путь к журналу</th>
  </tr>
  <tr>
    <td>Keystone</td>
    <td>/var/log/keystone/keystone.log</td>
  </tr>
  <tr>
    <td>Horizon</td>
    <td>/var/log/horizon/horizon.log</td>
  </tr>
  <tr>
    <td>Nova</td>
    <td>/var/log/nova/nova-api.log<br>/var/log/nova/nova-scheduler.log<br>/var/log/nova/nove-compute.log*</td>
  </tr>
  <tr>
    <td>Swift</td>
    <td>/var/log/swift/swift.log</td>
  </tr>
  <tr>
    <td>Cinder</td>
    <td>/var/log/cinder/api.log<br>/var/log/cinder/scheduler.log<br>/var/log/cinder/volume.log</td>
  </tr>
  <tr>
    <td>Glance</td>
    <td>/var/log/glance/api.log<br>/var/log/glance/registry.log</td>
  </tr>
  <tr>
    <td>Neutron</td>
    <td>/var/log/neutron/server.log<br>/var/log/neutron/dhcp-agent.log*<br>/var/log/neutron/l3-agent.log*<br>/var/log/neutron/metadata-agent.log*<br>/var/log/neutron/openvswitch-agent.log*</td>
  </tr>
</table>
<p>
	Журналы помеченные звездочкой доступны только на Acropolis OVM.
</p>

<div data-type="note" class="note" id="pro-tip-l9ioTesBhEsBtZ"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Если сервер OpenStack Manager помечен как 'down', даже если сервис в OVM функционирует - проверьте NTP. Большинство сервисов требует синхронизации времени между контроллером OpenStack и Acropolis OVM.</p>
</div>

</section>

<section data-type="sect3" id="command-reference-wrIWughosjtE">
<h5>Справочник по командам</h5>

<p>Загрузите источник для Keystone source (выполняется перед выполнением других команд)</p>

<p class="codetext">source keystonerc_admin</p>

<p>Список сервисов Keystone</p>

<p class="codetext">keystone service-list</p>

<p>Список записей Keystone</p>

<p class="codetext">keystone endpoint-list</p>

<p>Создание записи Keystone</p>

<p class="codetext">
	keystone endpoint-create \ <br>
  --service-id=&lt;SERVICE_ID&gt; \ <br>
  --publicurl=http://&lt;IP:PORT&gt; \ <br>
  --internalurl=http://&lt;IP:PORT&gt; \ <br>
  --region=&lt;REGION_NAME&gt; \ <br>
  --adminurl=http://&lt;IP:PORT&gt; <br>
</p>

<p>Список сервисов Nova</p>

<p class="codetext">nova list</p>

<p>Просмотр информации о сервисе</p>

<p class="codetext">nova show &lt;INSTANCE_NAME&gt;</p>

<p>Список гипервизоров Nova</p>

<p class="codetext">nova hypervisor-list</p>

<p>Просмотр информации о гипервизорах</p>

<p class="codetext">nova hypervisor-show &lt;HOST_ID&gt;</p>

<p>Список образов Glance</p>

<p class="codetext">glance image-list</p>

<p>Просмотр информации о образах Glance</p>

<p class="codetext">glance image-show &lt;IMAGE_ID&gt;</p>

</section>
</section>
</section>
</section>
</div>

<div data-type="part" id="book-of-acropolis-KEaio">
<h1><span class="label">Part III. </span>Книга Acropolis</h1>

<p class="definition"><strong>a·crop·o·lis  -  /ɘ ' kräpɘlis/  -  noun  -  data plane</strong>
<br>
Платформа хранения, вычисления и виртуализации.
</p>

<section data-type="chapter" id="architecture-NYI5ul">
<h2>Архитектура</h2>
<p>Acropolis - это распределенная программная платформа управления и оркестрации.</p>

<p>Рассмотрим три основных компонента:</p>

<ul>
	<li>Распределенная система управления хранилищем (Distributes Storage Fabric - DSF)
	<ul>
		<li>
        Это то, с чего началась платформа Nutanix, это ПО тесно связано с распределённой файловой системой Nutanix (NDFS).
        Теперь NDFS не просто система объединяющая все диски в единый ресурсный пул, а нечто большее - платформа хранения с широчайшей функциональностью.&nbsp;</li>  
	</ul>
	</li>
	<li>Распределенная система управления приложениями (App Mobility Fabric - AMF)
	<ul>
		<li>Гипервизор абстрагирует гостевые ОС от аппаратного уровня, а AMF абстрагирует выполнение задач от гипервизора (ВМ, Хранилище, контейнеры и т.д.).&nbsp; Это ПО позволяет динамически перемещать нагрузку между гипервизорами и облаками, именно этот компонент предоставляет возможность менять гипервизор на узлах Nutanix. 
    </li>
	</ul>
	</li>
	<li>Гипервизор
	<ul>
		<li>Многофункциональный гипервизор на основе CentOS и KVM.</li>
	</ul>
	</li>
</ul>

<p>Все ПО Nutanix являются распределенными, такие же решения используются на уровнях виртуализации и управления ресурсами.&nbsp;Служебные сервисы Acropolis управляют нагрузками, ресурсами, созданием и управлением ВМ.&nbsp; Acropolis абстрагирует служебные подсистемы и задачи от пользовательских нагрузок.</p>  

<p>Такая структура позволяет переносить нагрузки между гипервизорами, поставщиками облачных решений и платформами.</p>

<p>Ниже показана концептуальная верхнеуровневая архитектура Acropolis:</p>
<figure id="id-d0tvtNuD"><img alt="High-level Acropolis Architecture" class="iimagesv2arch_acropolispng" src="imagesv2/arch_acropolis.png">
<figcaption><span class="label">Рисунок 10-1. </span>Acropolis - верхнеуровневая архитектура</figcaption>
</figure>

<div data-type="note" class="note" id="supported-hypervisors-for-vm-management-8Mirfxuk"><h6>Примечание</h6>
<h5>Поддерживаемые гипервизоры</h5>

<p>Начиная с версии ПО 4.7, управление ВМ и другими объектами поддерживаться для AHV и ESXi, однако в будущем данный список может быть увеличен. &nbsp;API по управлению хранилищем и операций чтения данных о объектах доступно для всех гипервизоров.</p>  
</div>

<section data-type="sect1" id="converged-platform-ONIRcmuA">
<h3>Гиперконвергентная платформа</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/OPYA5-V0yRo">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//OPYA5-V0yRo"></iframe></div>

<p>Nutanix это хранилище и вычислительные мощности объединенные в единую распределенную платформу виртуализации. Платформа поставляется как программно-аппаратное решение, в виде серверных блоков по два (серия 6000/7000) или четыре вычислительных узла (серия 1000/2000/3000/3050) высотой 2U.</p>

<p>Каждый вычислительный узел размещает на себе гипервизор (ESXi, KVM, Hyper-V) и служебную виртуальную машину Nutanix (CVM).&nbsp; В рамках CVM выполняется все ПО Nutanix, обслуживаются операции I/O для гипервизора и всех ВМ запущенных на узле. &nbsp; Для узлов на базе VMware vSphere SCSI контроллер обслуживающий SSD и HDD диски - напрямую подключен к CVM с использованием VM-Direct Path (Intel VT-d).&nbsp; В случае с Hyper-V, все устройства хранения напрямую пробрасываются в CVM.</p>

<p>Ниже показана концептуальная верхнеуровневая архитектура компонент:</p>

<figure id="id-GRtVF5czu1"><img alt="Converged Platform" class="iimagesv2converged_platformpng" src="imagesv2/converged_platform.png">
<figcaption><span class="label">Рисунок 10-3. </span>Гиперконвергентная платформа</figcaption>
</figure>

<!-- end of converged platform section -->
</section>

<section data-type="sect1" id="distributedsystem">
<h3>Распределенная система</h3>

<p>
  Основные требования к распределенной системе:
</p>

<ol>
  <li>
    отсутствие любых единых точек отказа
  </li>
  <li>
    Система должна линейно масштабироваться
  </li>
  <li>
    Все операции должны быть распределены по кластеру (MapReduce)
  </li>
</ol>

<p>Узлы Nutanix образуют распределенную систему - кластер Nutanix. Он обеспечивает функционирование ПО Prism и Acropolis. Все сервисы и компоненты распределены между всеми CVM кластера, чтобы обеспечить высокую доступность и линейный, предсказуемый рост производительности при масштабировании.</p>

<p>Ниже приведена структурная схема кластера Nutanix:</p>

<figure class="large" id="id-oPtkTBTr"><img alt="Distributed Storage Fabric Overview" class="iimagesv2dsf_overviewpng" src="imagesv2/dsf_overview.png">
<figcaption><span class="label">Рисунок 11-1. </span>Кластер Nutanix - Распределенная система</figcaption>
</figure>

<p>
  Все данные и метаданные так же распределяются по всему кластеру. Распределяя данные по всем узлам и устройствам хранения мы можем обеспечить действительно высокую производительность и уровень отказоуствойчивости.
</p>

<p>
  Такой подход к хранению, в свою очередь, позволяет сервису Curator задействовать всю мощь кластера для обеспечения параллелизма всех операций. Например, параллелизм операций по защите данных, компрессии, обеспечения избыточного кодирования и дедупликации данных.  
</p>

<p>
  Ниже показано, как снижается процент нагрузки на узлы при масштабировании кластера:
</p>

<figure id="id-EGt4so1243"><img alt="Work Distribution - Cluster Scale" src="imagesv2/workbyscale.png">
<figcaption><span class="label">Рисунок. </span>Распределение нагрузки - масштабирование кластера</figcaption>
</figure>

<p>
  Важно: По мере роста кластера многие операции становятся более эффективными, так как нагрузка равномерно распределяется между всеми узлами.
</p>

</section>

<section data-type="sect1" id="software-defined-nrIRIyux">
<h3>Программно-определяемая платформа</h3>

<p>
    Основные требования к программно-определяемой платформе:
</p>
<ul>
  <li>
    Должна обеспечиваться мобильность (между разным оборудованием и гипервизорами)
  </li>
  <li>
    Должна поддерживаться работа на оборудовании разных производителей
  </li>
  <li>
    Должна обеспечиваться высокая скорость разработки (новые возможности, исправление ошибок, обновления безопасности)
  </li>
  <li>
    Должна обеспечиваться параллелизация вычислений
  </li>
</ul>

<p>
  Как говорилось ранее Nutanix представляет собой программное решение, поставляемое как программно-аппаратный комплекс.&nbsp; CVM один из самых важный компонентов платформы, где размещаются все логические компоненты ПО Nutanix. &nbsp;Данный компонент был разработан, чтобы стать частью архитектуры пригодной для неограниченного масштабирования. &nbsp;Одним из ключевых преимуществ программно-определяемого решения - независимость от аппаратной платформы. &nbsp; При таком подходе новые возможности могут быть предоставлены пользователю, независимо от оборудования, которое он использует. &nbsp;
</p>
  

<p> А значит, не имея привязки к каким-то специфическим ASIC/FPGA решение Nutanix может разрабатывать и предоставлять новые возможности через простое обновление ПО. &nbsp; Т.е. какая-то функция, например дедупликация, становится возможной просто при установке актуальной версии ПО Nutanix. &nbsp; Такие сложные функции могут быть развернуты как на новое серверное оборудование, так и на устаревшее. &nbsp; Например, у вас есть инсталляция, где работает старая версия ПО Nutanix, на устаревающем оборудовании - серия 2400. &nbsp; Эта серия поставлялась без функции дедупликации данных, однако эта функция полезна и было бы хорошо ее заполучить. &nbsp;Тогда вы просто ставите на серию 2400 актуальное ПО и получаете ее. Вот и все - очень удобно и просто. &nbsp;   
</p>

<p>Так же легко вводятся и новые адаптеры для доступа к данным на DFS.&nbsp; В самом начале продукт поддерживал доступ только по iSCSI, но со временем были добавлены NFS и SMB.&nbsp; В будущем будут добавлены адаптеры под конкретные типы нагрузок или гипервизоры - например HDFS и так далее.&nbsp; И все эти возможности так же будут доступны через простое обновление ПО. Это ключевое отличие решения Nutanix от классических инфраструктурных решений, когда для получения новых возможностей нужно обновлять не только ПО, но и оборудование. &nbsp; С Nutanix все иначе. Как только функция попала в релиз - вы получаете к ней доступ на любом оборудовании, гипервизоре и так далее. </p>

<p>Ниже представлена схема размещения компонент в программно-определяемом решении:</p>

<figure id="id-GRtnH4Izu1"><img alt="Software-Defined Controller Framework" class="iimagesv2software_defined_controllerpng" src="imagesv2/software_defined_controller.png">
<figcaption><span class="label">Рисунок 10-4. </span>Программно-определяемая архитектура</figcaption>
</figure>
</section>

<section data-type="sect1" id="cluster-components-b1IeUMu5">
<h3>Компоненты кластера</h3>

<p>Краткий рассказ на английском языке доступен по &nbsp;<a href="https://youtu.be/3v5RI_IbfV4">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//3v5RI_IbfV4"></iframe></div>

<p>
  Nutanix очень простой для развертывания и использования продукт. Таким он становится благодаря высокому уровню абстракции и огромному количеству интерфейсов для автоматизации и интеграции.
</p>
<p>Ниже приведена схема со всеми программными компонентами ПО в рамках кластера Nutanix:</p>

<figure id="id-GRtkSgUzu1"><img alt="Nutanix Cluster Components" class="iimagesv2cluster_componentspng" src="imagesv2/cluster_components3.png">
<figcaption><span class="label">Рисунок 10-5. </span>Кластер Nutanix - Компоненты ПО</figcaption>
</figure>

<h5>Cassandra</h5>

<ul>
	<li>Роль: Распределенное хранилище метаданных</li>
	<li>Описание: Данный компонент основан на открытом ПО Apache Cassandra, компонент отвечает за хранение и управление метаданными кластера. Данные распределяются на манер кольца между всеми узлами кластера Nutanix.&nbsp; Алгоритм Paxos используется для обеспечения консистентности данных&nbsp; Сервис запущен на всех узлах кластера, и доступен через интерфейс - Medusa.&nbsp;   
  </li>
</ul>

<h5>Zookeeper</h5>

<ul>
	<li>Роль: Менеджер конфигурации кластера</li>
  <li>Описание: Zookeeper хранит все настройки кластера, включая адреса узлов, кластера, статусы сервисов и узлов. Сервис реализован на базе открытого ПО Apache Zookeeper.&nbsp; Это сервис запускается на трех узлах кластера, один из них получает роль лидера. &nbsp; Лидер обрабатывает все запросы и перенаправляет их к остальным двум узлам. &nbsp; Если Лидер не ответил - новый лидер будет выбран автоматически. &nbsp;Zookeeper доступен через интерфейс - Zeus.</li>  
</ul>

<h5>Stargate</h5>

<ul>
	<li>Роль: Менеджер операций I/O</li>
	<li>Описание: Stargate отвечает за управление процессами работы с данными и операции I/O и является основным интерфейсом от гипервизора (через NFS, iSCSI, или SMB).&nbsp; Этот сервис запущен на всех узлах кластера, для локализации операций I/O.</li>
</ul>

<h5>Curator</h5>

<ul>
	<li>Роль: Менеджер распределения нагрузки MapReduce</li>
	<li>Описание: Curator отвечает за распределение нагрузки по узлам кластера, включая балансировку данных между дисками, про-активный сбор данных и так далее. &nbsp;Curator запускается на всех узлах и контролируется экземпляром с ролью Мастер который отвечает за перенаправление задач между экземплярами ПО.&nbsp; Сервис выполняет два типа сканирования - полное, выполняется каждые 6 часов и частичное - каждый час.
  </li>
</ul>

<h5>Prism</h5>

<ul>
	<li>Роль: Веб-интерфейс и API</li>
  <li>Описание: Prism компонент обеспечивающий настройку, мониторинг и управление кластером Nutanix.&nbsp; Данный компонент включает в себя Ncli, пользовательский веб-интерфейс HTML5, и REST API. &nbsp; Prism запускается на всех узлах кластера с выбором лидера, как и в случае с большинством других компонент платформы. </li>  
</ul>

<h5>Genesis</h5>

<ul>
	<li>Роль: Менеджер кластерных сервисов</li>
	<li>Описание:&nbsp; Genesis - компонент работающий на всех узлах кластера и отвечающий за работу с остальными сервисами и их начальную конфигурацию.&nbsp; Genesis запускается независимо от кластера и может работать если кластер остановлен или вовсе не сконфигурирован.&nbsp; Единственное, что требуется для нормальной работы Genesis - запущенный сервис Zookeeper.&nbsp; Команды cluster_init и cluster_status полностью обрабатываются компонентом Genesis.</li>
</ul>

<h5>Chronos</h5>

<ul>
	<li>Роль: Планировщик заданий и задач</li>
	<li>Описание: Chronos отвечает за планирование заданий и задач приходящих от компонента Curator, а так же следит за их выполнением на узлах кластера.&nbsp; Chronos работает на всех узлах кластера и контролируется мастер-процессом Chronos Master который отвечает за распределение задач между экземплярами ПО.</li>
</ul>

<h5>Cerebro</h5>

<ul>
    <li>Роль: Менеджер репликации и катастрофоустойчивости</li>
    <li>Описание: Cerebro отвечает за выполнение репликации и функции DR распределенного хранилища данных. &nbsp; Сюда входит контроль за снимками, репликацией на удаленные кластеры, миграцию и восстановление. &nbsp; Cerebro работает на всех узлах кластера, т.е. все узлы отвечают за передачу данных между кластерами Nutanix.</li>  
</ul>

<h5>Pithos</h5>

<ul>
	<li>Роль: Менеджер конфигурации объектов хранилища (vDisk)</li>
	<li>Описание: Pithos отвечает за объекты хранилища - они же vDisk.&nbsp; Запускается на всех узлах кластера и работает поверх сервиса Cassandra.</li>
</ul>
</section>

<section data-type="sect1" id="acropolis-services-BNIDiAuZ">
<h3>Сервисы Acropolis</h3>

<p>Сервис Acropolis в режиме Slave запускается на каждой CVM, при этом осуществляется выбор роли Master которой отвечает за планирование задач, выполнение, функциональность IPAM и так далее. &nbsp;В случае проблем с текущим Master данная роль будет автоматически передана одному из текущих Slave.</p>

<p>Распределение ролей между режимами:</p>

<ul>
	<li>Acropolis Master
	<ul>
		<li>Планирование и выполнение задач</li>
		<li>Сбор и публикация статистики</li>
		<li>Управление сетью (для гипервизора)</li>
		<li>VNC прокси (для гипервизора)</li>
		<li>HA (для гипервизора)</li>
	</ul>
	</li>
	<li>&nbsp;Acropolis Slave
	<ul>
		<li>Сбор и публикация статистики</li>
		<li>VNC прокси (для гипервизора)</li>
	</ul>
	</li>
</ul>

<p>Ниже представлена схема взаимодействия ролей Master и Slave:</p>

<figure id="id-DktkHGijuw"><img alt="Acropolis Services" class="iimagesv2acrop_componentspng image" src="imagesv2/acrop_components.png">
<figcaption><span class="label">Рисунок 10-2. </span>Сервисы Acropolis</figcaption>
</figure>
</section>
<!-- end of acrop services section -->

<section data-type="sect1">
<h3>Динамический планировщик</h3>

<p>
  Эффективное планирование ресурсов является критически важным для их эффективного использования. Динамический планировщик Acropolis расширяет традиционные функции планировщика по планированию вычислительных ресурсов для принятия решения о размещении ВМ. Он следит не только за вычислительными мощностями кластера, но и за нагрузкой на хранилище и диски, для более равномерного распределения нагрузки. Это гарантирует эффективное использование ресурсов и оптимальную производительность ВМ для конечных пользователей.  
</p>

<p>
  Планирование ресурсов делится на две области:
</p>

<ul>
  <li>
    Начальное размещение
    <ul>
      <li>
        Определение узлов, где ВМ будут размещены первоначальном запуске
      </li>
    </ul>
  </li>
  <li>
    Последующая оптимизация
    <ul>
      <li>
        Перераспределение нагрузки между узлами на базе метрик поступающих от компонент
      </li>
    </ul>
  </li>
</ul>

<p>
  С момента своего появления планировщик Acropolis осуществлял планирование использования ресурсов при первоначальном запуске ВМ. В новой версии планировщика Asterix - планирование и оптимизация ресурсов осуществляется в реальном времени на базе метрик.
</p>

<p>
    Ниже представлена верхнеуровневая архитектура планировщика:
</p>
<figure><img alt="Acropolis Dynamic Scheduler" src="imagesv2/scheduler_1.png">
<figcaption><span class="label">Рисунок. </span>Acropolis - динамический планировщик</figcaption>
</figure>

<p>
  Динамический планировщик запускается каждые 15 минут, для оптимизации нагрузки и перераспределения ее по кластеру (Gflag: lazan_anomaly_detection_period_secs). Предполагаемый рост потребления ресурсов рассчитывается на основе исторических данных. На основе анализа данных осуществляется распределение нагрузки.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Иной подход к оптимизации ресурсов</h5>
<p>
  Если рассматривать существующие механизмы распределения нагрузки (VMware DRS, Microsoft PRO) становится понятно, что все они делают упор на перераспределении нагрузки по ресурсам кластера. 
  Примечание: насколько агрессивно ПО выполняет балансировку определяет пользователь, посредством конфигурации ПО.  
</p>

<p>
  Например, если у вас три узла в кластере. Они нагружены неравномерно - 50%, 5%, %5. Типичное решение по балансировке будет пытаться добиться равномерного распределения нагрузки ~20% на узел. Но почему?  
</p>

<p>
  В действительности в первую очередь необходимо исключить конкуренцию за ресурсы, а не устранять перекос по нагрузке на узлы. Если конкуренции за ресурсы нет, то смысла в выполнении балансировки нет так же. Ведь выполнение бессмысленных операций по перемещению ВМ по кластеру так же требует определенного объема ОЗУ и процессорного времени.  
</p>

<p>
  Динамический балансировщик Acropolis будет выполнять перемещение ВМ именно в случае, если ожидается конкуренция за ресурсы. А не просто перемещая ВМ по кластеру.  
  Примечание: Acropolis DSF работает не так, как классические механизмы распределения нагрузки. Его основная цель исключить конкуренцию за ресурсы, исключить горячие места и обеспечить высокую производительность всей платформе. Более подробно механизм работы DSF описан в секции 'Балансировка данных'. 
</p>

<p>
  При первом включении платформы динамический балансировщик сразу же приступает к первоначальной балансировке нагрузки.
</p>

</div>

<h5>Решения о размещении ВМ</h5>
<p>
  Решение о размещении ВМ основывается на трех показателях:
</p>

<ul>
  <li>
    Использование вычислительных мощностей
    <ul>
      <li>
        Наше ПО постоянно отслеживает уровень использования вычислительных мощностей на каждом узле кластера.
        В случае, если ожидаемая нагрузка на CPU достигает порогового значения ПО выполняет перемещение ВМ с узла, для перераспределения нагрузки.  
        (На текущий момент - 85% CPU | Gflag: lazan_host_cpu_usage_threshold_fraction). 
        Важно отметить, что миграция будет выполняться только при конкуренции за ресурсы.
        Если же наблюдается перекос утилизации ресурсов между узлами, то ПО не производит никаких действий, для экономии ресурсов кластера.
      </li>
    </ul>
  </li>
  <li>
    Производительность хранилища
    <ul>
      <li>
        В рамках платформы Nutanix мы управляем и вычислительными ресурсами и хранилищем.
        По этому балансировщик следит и за сервисом Stargate на каждом узле кластера.
        Если вдруг процент потребляемых сервисом ресурсов возрастает до порогового значения - ПО начинает перенос ВМ, сохраняя равномерность нагрузки на хранилище.
        (На текущий момент -  85% выделенного для сервиса Stargate | Gflag: lazan_stargate_cpu_usage_threshold_pct).
      </li>
    </ul>
  </li>
  <li>
    Правила размещения ВМ ([Anti-]Affinity)
    <ul>
      <li>
        Правила размещения ВМ определяют где должны или не должны располагаться виртуальные машины. 
        Например в некоторых случаях вы должны иметь возможность запускать какие-то ВМ на конкретных узлах, следуя политике лицензирования прикладного ПО.
        Или же вам нужно запустить какие-то ВМ на разных узлах, чтобы исключить возможность их одновременной потере при выходе из строя узла кластера.
        Планировщик учитывает эти настройки при перемещении ВМ по кластеру.
      </li>
    </ul>
  </li>
  <!--
  <li>
    Networking
  </li>
  <li>
    Node capabilities / features
  </li>
  -->
</ul>

<p>
  Планировщик будет учитывать все перечисленные выше показатели.
  Будет стараться не осуществлять лишних перемещений, если в этом нет необходимости.
</p>

<p>
  Система будет оценивать эффективность каждого перемещения, анализировать полученный результат.
  Такая модель обучения ПО позволяет максимально оптимизировать все процессы и принимать решения о перераспределении нагрузки более взвешенно.
</p>

</section>

<section data-type="sect1" id="drive-breakdown-rkIZh9uO">
<h3>Диски</h3>

<p>В этом разделе будут рассмотрены накопители информации используемые в платформе и то как и для чего они используются.
  Примечание: В качестве единицы для расчета использовались Гибибайты (GiB) вместо Гигабайтов (GB).&nbsp;
  Так же учитывались накладные расходы на файловую систему.</p>

<h5>SSD диски</h5>

<p>На SSD размещаются следующие ключевые компоненты:</p>

<ul>
  <li>Служебные ВМ Nutanix (CVM)</li>
  <li>ПО Cassandra (хранилище метаданных)</li>
  <li>OpLog (постоянный буфер записи)</li>
  <li>Unified Cache (часть кэша, размещаемого на SSD)</li>
  <li>Хранилище данных</li>
</ul>

<p>На рисунке ниже представлена схема использования SSD дисков:</p>

<figure id="id-1ntnFGhwuQ"><img alt="SSD Drive Breakdown" class="iimagesv2drive_ssdpng" src="imagesv2/drive_ssd.png">
<figcaption><span class="label">Рисунок 10-6. </span>Использование SSD</figcaption>
</figure>

<p>Примечание: Начиная с версии 4.0.1 размер стал динамическим, это позволяет обеспечивать динамический рост для емкости для размещения данных.&nbsp; 
  Приведенные значения предполагают полностью использованный OpLog.&nbsp;
  Пропорции графических элементов не соответствует объемам емкости.&nbsp; 
  Оценку оставшейся емкости в GiB следует выполнять сверху вниз.&nbsp; 
  Например, оставшаяся емкость для OpLog рассчитывается путем вычитания размера Nutanix Home и Cassandra из общей емкости отформатированного SSD.</p>

<p>
  OpLog распределяется между всеми SSD узла, начиная с версии 5.5 - до 8 штук на узел (Gflag: max_ssds_for_oplog).
  Если доступны NVMe - OpLog будет расположен на них, вместо SATA SSD.
</p>

<p>
  Служебная директория с ПО Nutanix размещается на первых двух SSD, и зеркалируется для обеспечения отказоуствойчивости.
  Начиная с версии ПО 5.0 Cassandra распределяется по нескольким SSD, в рамках узла (на текущий момент - максимум 4).
  Под данные СУБД отводится по 15Гб на каждый SSD (может использовать и SSD Stargate, если объем метаданных растет). 
  В системах с двумя SSD метаданные будут зеркалироваться между SSDs. 
</p>

<p>
  До версии 5.0 Cassandra размещалась на первом SSD по умолчанию, 
  в случае проблемы с этим диском - CVM перезапускалась и хранилище Cassandra поднималось на втором SSD.
  В этом случае для метаданных резервировалось по 30GiB на первых двух устройствах.
</p>

<p>Большинство моделей Nutanix поставляется с одним или двумя SSD, однако некоторые поставляются и с большим количеством. 
  Например, если мы возьмем как пример 3060 или 6060 с двумя SSD по 400GB, то получим 100GiB для OpLog, 40GiB для Unified Cache, 
  и ~440GiB для хранилища экстентов на узел.</p>

<h5>HDD диски</h5>

<p>Диски HDD используются гораздо проще::</p>

<ul>
  <li>Хранилище Curator</li>
  <li>Хранилище данных (Постоянное хранилище)</li>
</ul>

<figure id="id-MltqUBhxu1"><img alt="HDD Drive Breakdown" class="iimagesv2drive_hddpng" src="imagesv2/drive_hdd.png">
<figcaption><span class="label">Рисунок 10-7. </span>Использование HDD</figcaption>
</figure>

<p> Если взять для примера модель 3060 с четырьмя HDD по 1TB каждый, то получим
   80GiB зарезервированных процессом Curator и ~3.4TiB для размещения пользовательских данных на каждый узел.</p>

<p>Примечание: эти данные актуальны для релиза 4.0.1 и выше, они могут отличаться в следующих версиях.</p>
</section>

<section data-type="sect1">
<h2>Безопасность и шифрование</h2>
<h3>Безопасность</h3>
<p>
  Безопасность важнейший компонент платформы Nutanix, который мы постоянно держим в голове с самого первого дня. 
   Жизненный цикл разработки подсистемы безопасности (SecDL) предполагает применение правил ИБ на каждом шагу разработки.
   Платформа имеет встроенную систему обеспечения информационной безопасности сразу с завода, и не требует пользовательской настройки или вмешательства.
</p>

<p>
  Платформа Nutanix имеет следующие сертификаты / квалификацию:
</p>

<ul>
  <li>
    Общие критерии (Common Criteria)
    <ul>
      <li>
        Разрабатывались для того, чтобы решение могло продаваться на рынке государственных компаний и структур (в основном для обороны и разведки) 
        по этому нужно лишь провести оценку по такому набору стандартов. Эти стандарты вырабатывались государственными органами Канады, Франции, Германии, Нидерландов, Англии и США. 
      </li>
    </ul>
  </li>
  <li>
    Руководства по технической реализации ИБ (STIGs)
    <ul>
      <li>
        Стандарты конфигурации для DOD IA и использующих IA систем и устройств.
        С 1998 года DISA FSO играет критическую роль повышения уровня ИБ для систем безопасности DoD (Департамента обороны).
        В свою очередь STIG содержит указание на блокировку ИС и ПО которые могут быть уязвимы для хакерских атак.
      </li>
    </ul>
  </li>
  <li>
    Стандарт FIPS 140-2
    <ul>
      <li>
        Стандарт FIPS 140-2 стандарт ИБ по аккредитации модулей криптографии производимых частными организациями
        для государственных учреждений и регулируемых отраслей (здоровье и финансы) которые используются при агрегации, хранении, 
        передаче или распространении конфиденциальной, но не секретной информации.
      </li>
    </ul>
  </li>
  <li>
    Соответствие TAA
    <ul>
      <li>
        Закон о заключении торговых соглашений в соответствии с законом о торговле от 1974 года. 
        Продукт для государственных органов и предприятий должен иметь страну происхождения США.
      </li>
    </ul>
  </li>
  <!-- UPDATE when notified from ERIC
  <li>
    NIAP-CCEVS Approved Product List
  </li>
  -->
</ul>

<h5>Автоматизация Управления Конфигурацией Безопасности (SCMA)</h5>

<p>
  Подсистема безопасности работает на постоянной основе, с момента запуска кластера.
  Проверка выполняется без вмешательства пользователя, проверяя соответствие уровня ИБ для CVM/AHV на протяжении всего жизненного цикла.
  Такой механизм проверок постоянно проверяет все ли настройки соответствуют STIG и если находит несоответствие - исправляет ситуацию автоматически.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Специальное выполнение SCMA</h5>

<p>По умолчанию задачи SCMA запускаются по расписанию, однако могут быть запущены и по требованию пользователя. 
  Для запуска SCMA необходимо выполнить следующие команды в рамках любой CVM:</p>

<p class="codetext">
  # Запуск на одной CVM </br>
  sudo salt-call state.highstate </br>
</br>
  # Запуск на всех CVM </br>
  allssh "sudo salt-call state.highstate"
</p>
</div>

<p>
  Командная строка Nutanix, позволяет пользователям управлять конфигурацией платформы, а так же включать дополнительные параметры безопасности для усиления ИБ.
</p>

<h5>Настройки безопасности CVM</h5>

<p>
  Для настройки политик SCMA были добавлены специальные консольные команды. Ниже перечислены основные команды:
</p>

<p>
  Просмотр текущих настроек ИБ CVM
</p>
<p class="codetext">
  ncli cluster get-cvm-security-config
</p>

<p>
  Упомянутая выше команда вернет текущие настройки кластера. Ниже представлен перечень настроек:
</p>

<p class="codetext">
Enable Aide : false </br>
Enable Core : false</br>
Enable High Strength P... : false </br>
Enable Banner : false</br>
Enable SNMPv3 Only : false</br>
Schedule : DAILY
</p>

<p>
  Установка сообщения отображаемого при входе на CVM
</p>
<p>
  Данная команда включает или выключает сообщение со стандартными условиями входа в CVM.
</p>
<p class="codetext">
  ncli cluster edit-cvm-security-params enable-banner=[yes|no] #Default:no
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Настройка сообщения</h5>

<p>Отображаемое при входе сообщение может быть изменено, путем выполнения следующих действий на любой из CVM:</p>
<ol>
  <li>
    Создаем резервную копию стандартного сообщения:
    <ul>
      <li>
        sudo cp -a /srv/salt/security/KVM/sshd/DODbanner /srv/salt/security/KVM/sshd/DODbannerbak
      </li>
    </ul>
  </li>
  <li>
    Используйте редактор vi для изменения сообщения:
  </li>
  <ul>
    <li>
      sudo vi /srv/salt/security/KVM/sshd/DODbanner
    </li>
  </ul>
  <li>
    Повторите шаги на всех CVM или скопируйте измененный файл посредством SCP
  </li>
  <li>
    Включите сообщения, используя команду указанную выше.
  </li>
</ol>
</div>

<p>
  Настройка пароля для CVM
</p>
<p>
  Данная команда принудительно увеличит требования к сложности пароля (minlen=15,difok=8,remember=24).
</p>
<p class="codetext">
  ncli cluster edit-cvm-security-params enable-high-strength-password=[yes|no] #Default:no
</p>

<p>
  Настройка системы обнаружения вторжений (AIDE)
</p>
<p>
  Данная команда включает или выключает подсистему обнаружения вторжений. Сервис будет запускаться еженедельно.
</p>
<p class="codetext">
  ncli cluster edit-cvm-security-params enable-aide=true=[yes|no] #Default:no
</p>

<p>
  Принудительное включение SNMPv3
</p>
<p>
  Данная команда включает принудительное использование SNMPv3 для отправки трапов.
</p>
<p class="codetext">
  ncli cluster edit-cvm-security-params enable-snmpv3-only=[true|false] #Default:false
</p>

<p>
  Установка расписания для SCMA 
</p>
<p>
  Данная команда позволяет настроить периодичность выполнения запуска SCMA.
</p>
<p class="codetext">
  ncli cluster edit-cvm-security-params schedule=[HOURLY|DAILY|WEEKLY|MONTHLY] #Default:HOURLY
</p>

<h5>Настройки безопасности для Гипервизора</h5>

<p>
  Для настройки политик SCMA были добавлены специальные консольные команды. Ниже перечислены основные команды:
</p>

<p>
   Просмотр настроек безопасности для гипервизора
</p>
<p class="codetext">
  ncli cluster get-hypervisor-security-config
</p>

<p>
 Данная команда вернет стандартный перечень настроек:
</p>

<p class="codetext">
Enable Aide : false </br>
Enable Core : false</br>
Enable High Strength P... : false </br>
Enable Banner : false</br>
Schedule : DAILY
</p>

<p>
  Настройка сообщения отображаемого при входе
</p>
<p>
  Данная команда позволяет включить или выключить сообщение для отображения при входе в консоль гипервизора:
</p>
<p class="codetext">
  ncli cluster edit-hypervisor-security-params enable-banner=[yes|no] #Default:no
</p>

<p>
  Настройка сложности пароля для гипервизора
</p>
<p>
  Данная команда принудительно увеличит требования к сложности пароля (minlen=15,difok=8,remember=24).
</p>
<p class="codetext">
  ncli cluster edit-hypervisor-security-params enable-high-strength-password=[yes|no] #Default:no
</p>

<p>
    Настройка системы обнаружения вторжений (AIDE)
</p>
<p>
    Данная команда включает или выключает подсистему обнаружения вторжений. Сервис будет запускаться еженедельно.
</p>
<p class="codetext">
  ncli cluster edit-hypervisor-security-params enable-aide=true=[yes|no] #Default:no
</p>

<p>
    Установка расписания для SCMA 
</p>
<p>
    Данная команда позволяет настроить периодичность выполнения запуска SCMA.
</p>
<p class="codetext">
  ncli cluster edit-hypervisor-security-params schedule=[HOURLY|DAILY|WEEKLY|MONTHLY] #Default:HOURLY
</p>

<h5>Блокировка Кластера</h5>

<p>
  Блокировка Кластера - операция блокировки доступа к CVM посредством пароля. При этом доступ на основе ключа может быть разрешен.
</p>

<p>
  Конфигурация данного режима доступна через меню Настройки интерфейса Prism:
</p>

<figure id="id-1ntnFGhwuQ"><img alt="Cluster Lockdown Menu" src="imagesv2/Prism/lockdown/lockdown1.png">
<figcaption><span class="label">Рисунок. </span>Блокировка Кластера - Меню</figcaption>
</figure>

<p>
  Данная станица отображает текущую конфигурацию блокировки и позволяет добавить/удалить SSH ключи:
</p>

<figure id="id-1ntnFGhwuQ"><img alt="Cluster Lockdown Page" src="imagesv2/Prism/lockdown/lockdown5.png">
<figcaption><span class="label">Рисунок. </span>Блокировка Кластера - Страница</figcaption>
</figure>

<p>
  Для добавления нового ключа - нажмите кнопку 'New Public Key':
</p>

<figure id="id-1ntnFGhwuQ"><img alt="Cluster Lockdown - Add Key" src="imagesv2/Prism/lockdown/lockdown2.png">
<figcaption><span class="label">Рисунок. </span>Блокировка Кластера - Добавление ключа</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Работа с ключами SSH</h5>

<p>Для генерации ключа SSH выполните команду:</p>
<p class="codetext">
ssh-keygen -t rsa -b 2048
</p>

<p>
  Эта команда сгенерирует пару ключей:
</p>
<ul>
  <li>
    id_rsa - приватный ключ
  </li>
  <li>
    id_rsa.pub - публичный ключ, который будет использоваться для доступа к кластеру
  </li>
</ul>
</div>

<p>
  После добавления ключей и проверки доступа с их использованием вы можете отключить доступ по паролям.
 Для этого нужно снять галочку - 'Enable Remote Login with Password.' Появится всплывающее окно, где требуется подтвердить действие, нажав ОК.
</p>

<h3>Шифрование данных и управление ключами</h3>

<p>Шифрование данных подразумевает, что возможность расшифровать данные есть только у авторизованных пользователей платформы.
  Это означает, что все другие пользователи доступ к данным получить не смогут.</p>

<p>Например, если у меня есть сообщение, которое я хочу отправить кому-то, и быть уверенным, что только они смогут его прочитать. Я могу зашифровать сообщение (открытый текст) с помощью шифра (ключа) и отправить им зашифрованное сообщение (зашифрованный текст). Если это сообщение украдено или перехвачено, злоумышленник может видеть только зашифрованный текст, который в основном бесполезен, не имея шифра для расшифровки сообщения. Как только доверенная сторона получила сообщение, они могут расшифровать сообщение, используя ключ, который мы им дали.</p>

<p>Вот несколько методов шифрования данных:</p>

<ul>
  <li>Симметричное шифрование (шифрование с закрытым ключом):
    <ul>
    <li>Один и тот же ключ используется для шифрования и расшифровки данных</li>
    <li>Пример: AES, PGP*, Blowfish, Twofish, etc.</li>    
    </ul>
  </li>
  <li>Асимметричное шифрование (шифрование с открытым ключом):
    <ul>
    <li>Один ключ используется для шифрования (открытый ключ), другой-для дешифрования (закрытый ключ)</li>
    <li>Пример: RSA, PGP*, etc.</li>    
    </ul>
  </li>
</ul>

<p>Примечание: PGP (или GPG) использует оба типа шифрования.</p>

<p>О шифровании данных говорят в двух основных контекстах: </p>
<ul>
  <li>Онлайн шифрование: данные, передаваемые между двумя сторонами (например, передача данных по сети)</li>
  <li>Оффлайн шифрование: статические данные (например, данные хранимые на устройстве)</li>
</ul>

<p>Начиная с версии 5.8 шифрование данных происходит в режиме оффлайн</p>

<p>В следующих разделах описывается, как Nutanix управляет шифрованием данных и параметрами управления ключами.</p>

<h4>Шифрование данных</h4>
<p>Nutanix предоставляет три варианта шифрования данных:</p>
<ul>
  <li>Встроенная в ПО функциональность шифрования (FIPS-140-2 Level-1) *начиная с версии 5.5</li>
  <li>Использование дисков с самошифрованием (SED) (FIPS-140-2 Level-2)</li>
  <li>Шифрование на базе ПО и оборудования</li>
</ul>

<p>Это шифрование настраивается на уровне кластера или контейнера и зависит от типа гипервизора:</p>
<ul>
  <li>Шифрование на уровне кластера:
  <ul><li>AHV, ESXi, Hyper-V</li></ul>
  </li>
  <li>Шифрование на уровне контейнеров:
  <ul><li>ESXi, Hyper-V</li></ul>
  </li>
</ul>

<p>Примечание: для инсталляций на базе шифрования SED фактически выполняется шифрование на уровне кластера, так как все устройства имеют встроенные механизмы шифрования.</p>

<p>Вы можете проверить текущий статус шифрования в разделе настроек 'Data-at-Rest Encryption'.  
  Здесь же можно включить или выключить шифрование, если это применимо к текущей инсталляции.</p>

<p>В данном примере мы видим, что шифрование включено на уровне кластера:</p>

<figure id="id-dataenc"><img alt="Data Encryption - Enabled (cluster level)" src="imagesv2/encr1.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - Включено на уровне кластера</figcaption>
</figure>

<p>В данном примере шифрование включено для определенных контейнеров:</p>

<figure id="id-dataenc"><img alt="Data Encryption - Enabled (container level)" src="imagesv2/encr4.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - Включено на уровне контейнера</figcaption>
</figure>

<p>Вы можете включить или изменить конфигурацию, нажав на кнопку "edit configuration".  
    Это вызовет меню для настройки KMS:
</p>

<figure id="id-dataenc"><img alt="Data Encryption - Configure" src="imagesv2/encr2.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - Настройка</figcaption>
</figure>

<p>Для внешних KMS - меню проведет вас через процесс запроса CSR, который затем можно предоставить в центр сертификации для подписания.</p>

<h5>Встроенные в ПО механизмы шифрования</h5>
<p>ПО Nutanix обеспечивает оффлайн шифрование на базе AES-256.
  ПО может взаимодействовать с внешним KMS сервером совместимым с KMIP или TCG (Vormetric, SafeNet, и пр.) или же со встроенным KMS для версии ПО 5.8 или выше.
  Для шифрования / дешифрования система использует Intel AES-NI, чтобы свести к минимуму любое потенциальное влияние на производительность программного обеспечения.</p>

<p>При записи в OpLog или на диск данные шифруются до снятия контрольной суммы.</p>

<p>Шифрование - последняя операция с данными, которая выполняется до их записи на диск:</p>

<figure id="id-dataenc"><img alt="Data Encryption - Transform Application" src="imagesv2/encr5.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - Преобразование данных</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Шифрование и эффективность данных</h5>

<p>Так как мы шифруем данные после применения дедупликации или сжатия, мы гарантируем, что эффективность методов сохранения свободного пространства сохраняется на прежнем уровне.  
    Проще говоря, коэффициенты дедупликации и сжатия будут одинаковыми для зашифрованных или незашифрованных данных.</p>
</div>

<p>Когда выполняется чтение данных - осуществляется чтение данных с диска, дешифрация и предоставление их пользователю или ПО.  
  При этом мы гарантируем, что объем данных не будет увеличен.  Учитывая, что мы используем Intel AES NI процедура дешифрации не влечет за собой роста нагрузки на платформу.</p>

<h5>Шифрование на основе SED</h5>
<p>
    На рисунке показана верхнеуровневая архитектура:
</p>

<figure id="id-dataenc"><img alt="Data Encryption - SED" src="imagesv2/data_encryption1.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - SED</figcaption>
</figure>

<p>
  Шифрование на основе SED предполагает разделение устройств хранения на наборы данных, которые могут быть защищены или не защищены.
  В случае с Nutanix - загрузочный раздел и домашние директории ПО зашифрованы.
  Все устройства хранения данных зашифрованы при помощи больших ключей, согласно стандартам второго уровня.
</p>

<p>
  Когда сервер запускается будет отправлен запрос ключей к серверу KMS для получения ключей, чтобы разблокировать доступ к дискам.
  Согласно политикам безопасности никаких ключи не кешируются на кластере. В случае холодной перезагрузки или спроса состояния сервера через IPMI 
  узел должен обратиться к серверу KMS для получения доступа к дискам. Теплая перезагрузка не приведет к такому поведению.
</p>

<h4>Управление ключами (KMS)</h4>

<p>Nutanix предоставляет встроенный сервис управления ключами (локальный менеджер ключей - LKM) и возможность их хранения (начиная с версии 5.8).
  Данный сервис является альтернативой выделенным, внешним системам управления ключами.
  Этот сервис был разработан, чтобы свести на нет необходимость в выделенном решении KMS и упростить среду, 
  однако внешние KMS по-прежнему поддерживаются.</p>

<p>Как упоминалось в предыдущем разделе, управление ключами является очень важной частью любого решения для шифрования данных.  
    Ключи используются на всех уровнях платформы, для предоставления максимально защищенное решение по управлению платформой.</p>

<p>В решении используются ключи трех типов:</p>

<ul>
  <li>Ключ шифрования данных (DEK)
  <ul>
    <li>Ключ, используемый для шифрования данных</li>
  </ul>
  </li>
  <li>Ключ шифрования ключа (KEK)
  <ul>
    <li>Ключ шифрования, используемый для шифрования DEK</li>
  </ul>
  </li>
  <li>Мастер ключ шифрования (MEK)
  <ul>
    <li>Ключ шифрования, используемый для шифрования KEK</li>
    <li>Применяется только при использовании локального менеджера ключей</li>
  </ul>
  </li>
</ul>

<p>На следующем рисунке показаны связи между различными ключами и параметрами KMS:</p>

<figure id="id-dataenc"><img alt="Data Encryption - Key Management" src="imagesv2/encr6.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - Управление ключами</figcaption>
</figure>

<p>Служба локального диспетчера ключей (LKM) распределена между каждым узлом Nutanix и работает на каждом CVM. 
  Сервис использует крипто-модуль FIPS 140-2, а управление ключами совершенно прозрачно для конечного пользователя.</p>

<p>При настройке шифрования данных, встроенный сервис управления ключами может быть выбран посредством выбора опции 'Cluster's local KMS':</p>

<figure id="id-dataenc"><img alt="Data Encryption - Configure" src="imagesv2/encr2.png">
<figcaption><span class="label">Рисунок. </span>Шифрование данных - Настройка</figcaption>
</figure>

<p>Мастер-ключ распределен между всеми узлами в кластере с использованием алгоритма Shamir's Secret Sharing для обеспечения отказоуствойчивости и безопасности.  
  Минимум ROUNDUP(N/2) узлов должно быть доступно для получения ключей, где N = количество узлов в кластере.</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Резервное копирование и ротация ключей</h5>

<p>После включения шифрования рекомендуется создать резервную копию ключа (ключей) шифрования данных (DEK).  
    Если резервная копия создана, она должна быть защищена надежным паролем и храниться в безопасном месте.</p>

<p>Система обеспечивает возможность ротации (rekey) как KEK, так и MAK. 
    Он автоматически ротирует мастер-ключ (MEK) каждый год. Однако, это может быть выполнено и по запросу пользователя.  
    В случае добавления/удаления узла мы также выполняем ротацию главного ключа.</p>
</div>

<!-- end of data encryption section -->
</section>
<!-- End of security section -->
</section>

<section data-type="chapter" id="distributed-storage-fabric-22IlTD">
<h2>Распределенное хранилище данных</h2>

<p>
  Для гипервизора распределенное хранилище данных выглядит как единая система хранения данных,
  при этом все операции ввода-вывода обрабатываются локально для обеспечения максимальной производительности.&nbsp; 
  Более подробную информацию о том, как узлы кластера образуют распределенную систему можно найти в следующем разделе.
</p>

<section data-type="sect1" id="data-structure-M2IySRTw">
<h3>Структура данных</h3>

<p>Распределенное хранилище данных включает в себя следующие объекты:</p>

<h5>Пул хранения (Storage Pool)</h5>

<ul>
	<li>Роль: Группа физических устройств</li>
	<li>Описание: Пул хранения - это группа физических устройств хранения данных, включая PCIe SSD, SSD и HDD.&nbsp; 
      Пул хранения может охватывать несколько узлов Nutanix и расширяется по мере масштабирования кластера.&nbsp; 
      В большинстве конфигураций используется только один пул хранения.</li>
</ul>

<h5>Контейнер (Container)</h5>

<ul>
	<li>Роль: Группа ВМ или файлов</li>
	<li>Описание: Контейнер - логический сегмент пула хранения, в рамках контейнера размещаются группы ВМ или файлов (vDisks).&nbsp; 
    Некоторые настройки (например RF) определяются на уровне контейнера, при этом некоторые могут применяться непосредственно к файлам или ВМ.&nbsp; 
    К емкости контейнера может быть предоставлен доступ посредством SMB/NFS.</li>
</ul>

<h5>Файл (vDisk)</h5>

<ul>
	<li>Роль: vDisk</li>
	<li>Описание: vDisk это файл, имеющий минимальный размер 512KB, например .vmdks или образ диска VM.&nbsp; 
    vDisks представляет собой набор экстентов, которые в свою очередь формируют собой группы экстентов.</li>
</ul>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Максимальный размер файла (vDisk)</h5>

<p>Со стороны распределенного хранилища данных и ПО к файлам не предъявляется никаких искусственных ограничений. 
  Начиная с версии 4.6 размер файла - это 64 битное целое число, которое определяет и хранит размер в байтах. 
  Это означает, что теоретический максимальный размер файла - 2^63-1 или 9E18 (9 Экзабайт). 
  Любые ограничения ниже этого значения будут связаны с ограничениями на стороне клиента, например максимальный размер vmdk определяемый ESXi.</p>
</div>

<p>На следующем рисунке показано, как они сопоставляются между хранилищем данных и гипервизором:</p>

<figure id="id-DktyigS1Tw"><img alt="High-level Filesystem Breakdown" class="iimagesv2data_structure_1png" src="imagesv2/data_structure_1.png">
<figcaption><span class="label">Рисунок 11-2. </span>Высокоуровневая схема распределенного хранилища данных</figcaption>
</figure>

<h5>Экстент</h5>

<ul>
	<li>Роль: Логически непрерывные хранимые данные</li>
	<li>Описание: Экстент - это логический сегмент данных, размером 1MB который состоит из n последовательных блоков данных (N - зависит от размера блока ФС в гостевой ОС).
    &nbsp; Все изменения экстентов осуществляются на базе суб-экстентов (слайсов) для большей гранулярности операций и эффективности.
    &nbsp; Каждый суб-экстент может быть перемещен в кеш, в зависимости от объема считываемых/кешируемых данных.</li>
</ul>

<h5>Группа экстентов</h5>

<ul>
	<li>Роль: Физически непрерывные хранимые данные</li>
	<li>Описание: Группа экстентов - непрерывный набор данных размером 1MB или 4MB.&nbsp; 
    Данные хранятся как файлы на физических дисках, которыми управляет CVM.&nbsp; 
    Экстенты динамически распределяются по группам для обеспечения равномерного распределения данных по узлам и дискам, для улучшения производительности.&nbsp; 
    Примечание: начиная с версии 4.0, группы могут иметь размер как 1MB, так и 4MB в зависимости от дедупликации.</li>
</ul>

<p>На следующем рисунке показано, как эти структуры связаны между различными файловыми системами:&nbsp;</p>

<figure id="id-NMtEsnSYT8"><img alt="Low-level Filesystem Breakdown" class="iimagesv2data_structure_2png" src="imagesv2/data_structure_2.png">
<figcaption><span class="label">Рисунок 11-3. </span>Низкоуровневая схема работы с ФС</figcaption>
</figure>

<p>Вот еще одно графическое представление того, как эти элементы связаны:</p>

<figure id="id-3rtoTXSlTb"><img alt="Graphical Filesystem Breakdown" class="iimagesv2data_structure_3png" src="imagesv2/data_structure_3.png">
<figcaption><span class="label">Рисунок 11-4. </span>Графическая схема работы с ФС</figcaption>
</figure>
</section>

<section data-type="sect1" id="io-path-overview-QMIBHzTQ">
<h3>Процесс I/O и кэширование</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/SULqVPVXefY">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//SULqVPVXefY"></iframe></div>

<p>Процесс I/O выглядит следующим образом:</p>

<figure id="id-89t0S2H9TZ"><img alt="DSF I/O Path" class="iimagesv2io_path_basepng" src="imagesv2/io_path_base.png">
<figcaption><span class="label">Рисунок 11-5. </span>Процесс I/O</figcaption>
</figure>

<p>
  В конфигурации на базе SSD все данные будут располагаться на этих дисках, дополнительные операции по переносу данных между HDD и SSD выполняться не будут.
</p>

<h5>OpLog</h5>

<ul>
	<li>Роль: Постоянный буфер записи</li>
	<li>Описание: Буфер OpLog похож по своей сути на журнал файловой системы, и разработан как промежуточная область для обработки всплесков операций случайной записи.
     Такие операции обрабатываются, объединяются и затем последовательно записываются на распределенное хранилище данных (Extent Store).&nbsp; 
     Данные OpLog постоянно синхронизируются между N служебных ВМ Nutanix (CVM), только после завершения записи и репликации для обеспечения высокой доступности данных..&nbsp; 
     Все экземпляры компонента OpLogs участвуют в процессе работы с данными, таким образом достигается равномерное распределение нагрузки между узлами кластера.&nbsp; 
     OpLog всегда размещается на SSD для обеспечения максимальной производительности для операций I/O, особенно если они случайные. &nbsp; 
     Таким образом, все устройства SSD участвуют в работе OpLog. В случае последовательный операций I/O - запись выполняется сразу в распределенное хранилище данных, минуя OpLog.&nbsp; 
     Пока данные находятся в OpLog и не сброшены на РХД, все операции с ними будут выполнятся прямо в OpLog.&nbsp; 
     Для контейнеров с активированными настройками дедупликации все операции записи будут дедуплицироваться на основе хешей, это позволит дедуплицировать их в универсальном кеше.</li>
</ul>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Распределение OpLog</h5>

<p>
  OpLog является общим ресурсом, он распределяется между всеми файлами, для обеспечения равных возможностей и 
  обеспечения высокого уровня производительности. Для этого существует лимит который определяет максимальный размер OpLog на файл РХД.
  соответственно ВМ с несколькими дисками получат емкость OpLog для каждого из них.
</p>

<p>Начиная с версии 4.6 данный лимит 6Gb, в более ранних версиях 2GB.</p>

<p>
  Данный Gflag отвечает за настройку лимита: vdisk_distributed_oplog_max_dirty_MB.
</p>

</div>

<h5>Хранилище экстентов</h5>

<ul>
	<li>Роль: Постоянное хранилище данных</li>
	<li>Описание: Хранилище экстентов - по своей сути является масштабируемым постоянным хранилищем данных, объединяет в себе уровни хранения SSD и HDD.&nbsp; 
    Данные попадают в данное хранилище двумя путями: A) сбросом из OpLog или B) напрямую, при последовательных операциях, минуя OpLog.&nbsp; 
    Nutanix динамически распределяет данные между SSD и HDD на основе типов шаблонов операций I/O.</li>
</ul>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Последовательная запись</h5>

<p>Операции записи определяются как последовательные когда более 1.5Мб данных находятся в очереди на запись в файл.  
  Такие операции будут переданы напрямую в хранилище экстентов, минуя OpLog. Так как данные уже объединены в непрерывные цепочки и не требуют дополнительной обработки.</p>

<p>
  Это настраиваемый параметр - Gflag: vdisk_distributed_oplog_skip_min_outstanding_write_bytes.
</p>

<p>
  Все остальные операции I/O, включая большие (например &gt;64K) будут предварительно обрабатываться OpLog.
</p>
</div>

<h5>Единый Кэш (Unified Cache)</h5>

<ul>
	<li>Роль: Динамический кэш на чтение</li>
	<li>Описание: Единый кэш представляет собой кэш на чтение, к которому применяется механизм дедупликации.
    Он располагается в оперативной памяти CVM и на SSD устройствах. Работа с данными в кэш происходит с использованием алгоритма LRU (least recently used).&nbsp; 
    Во время выполнения запроса на чтение, данные будут помещены в часть единого кэш, которая располагается в оперативной памяти.&nbsp; 
    Любой следующий запрос инициализирует кеширование запрашиваемых данных в разделе кэш размещенном на SSD и оперативной памяти.&nbsp;
    Таким образом работа с данными происходит в два этапа, с применением LRU - первичный запрос к данным и повторный запрос данных.&nbsp; 
    После выполнения первого и второго этапа - счетчик LRU будет сброшен и операция повторится снова, при повторном запросе данных.</li>
</ul>

<p>Работа механизма единого кеша выглядит следующим образом:</p>

<figure id="id-RBtnUjHQT1"><img alt="DSF Unified Cache" class="iimagesv2content_cachepng" src="imagesv2/content_cache.png">
<figcaption><span class="label">Рисунок 11-6. </span>Единый кэш</figcaption>
</figure>

<div data-type="note" class="note" id="cache-granularity-and-logic-21igh5H0T3"><h6>Примечание</h6>
<h5>Гранулярность кэширования и логика</h5>

<p>Данные попадают в кэш с гранулярностью 4K, кеширование выполняется в реальном времени.</p>

<p>
  Каждая CVM имеет свой собственный локальный кэш, который используется при работе с дисками ВМ и файлами (vDisk).  
  Каждый диск ВМ, при клонировании получает собственную карту блоков. Исходный диск помечается как неизменяемый.
  Так каждая ВМ имеет свою собственную закэшированную копию исходного образа.
</p>

<p>
  В случае перезаписи данных - операция будет перенаправлена в новый экстент, согласно собственной карте блоков ВМ.
  Такой подход гарантирует целостность кэша и сохраняет его неповрежденным.
</p>
</div>

<h5>Кэш экстентов</h5>

<ul>
	<li>Роль: Кэш на чтение расположенных в ОЗУ</li>
	<li>Описание: Кэш экстентов - кэш на чтение целиком расположенный в ОЗУ служебных ВМ - CVM.&nbsp; 
    Этот кэш хранит в себе данные для которых не применяется дедупликация и компрессия.&nbsp; 
    С версии ПО 3.5 он отделен от универсального кэша (Unified Cache), однако с версии 4.5 был снова объединен с ним.</li>
</ul>
</section>

<section data-type="sect1" id="scalable-metadata-4aI3tRTg">
<h3>Масштабируемые метаданные</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/MlQczJhQI3U">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//MlQczJhQI3U"></iframe></div>

<p>Метаданные являются ядром любой интеллектуальной платформы, особенно они важны для файловых систем и хранилищ данных.&nbsp; 
    С точки зрения РХД, есть несколько моментов имеющих ключевое значение: </p>
<ul>
  <li>
    Должен быть целостным на 100%, в каждый момент времени
  </li>
  <li>
    Должен соответствовать модели <a href="https://en.wikipedia.org/wiki/ACID_(computer_science)">ACID</a>
  </li>
  <li>
    Должен иметь неограниченную масштабируемость
  </li>
  <li>
    Должен не иметь узких мест при любом масштабировании (должен масштабироваться линейно)
  </li>
</ul>

<p>
  Как упоминалось ранее - для хранения метаданных РХД использует хранилище типа ключ-значение, работающую в структуре "кольцо".&nbsp; 
  Для обеспечения избыточности и высокой доступности данных используется фактор репликации, а метаданные размещаются по всем нечетным узлам кластера.&nbsp; 
  В случае записи или обновления метаданных операция происходит на одном узле, а затем реплицируется по N узлов (N-зависит от размера кластера).&nbsp; 
  Большинство узлов должны дать подтверждение, перед осуществлением записи, для реализации такого механизма используется алгоритм Paxos.&nbsp; 
  Таким образом обеспечивается строгая согласованность всех данных и метаданных, хранящихся в рамках платформы.
</p>

<p>На рисунке ниже отображена схема размещения метаданных в кластере из 4х узлов:</p>

<figure id="id-A0tOHyt8Tx"><img alt="Cassandra Ring Structure" class="iimagesv2metadata_1png" src="imagesv2/metadata_1.png">
<figcaption><span class="label">Рисунок 11-7. </span>Cassandra - структура "Кольцо"</figcaption>
</figure>

<p>Производительность при масштабировании - еще один важный момент.&nbsp; 
  В отличие от классической модели двух контроллеров (“master/slave”), каждый узел Nutanix отвечает за набор метаданных.&nbsp; 
  Это исключает стандартные узкие места, распределяя нагрузку между узлами кластера.&nbsp;
  Консистентная схема хэширования используется для партиционирования ключей, для минимизации перераспределения ключей в случае изменения размеров кластера.
  Новые узлы встраиваются в кольцо существующего кластера между существующими участниками кластера, хранящих на себе метаданные.
  Кроме того, применяется механизм "block awareness" для повешения надежности.</p>

<p>Ниже на рисунке показан пример масштабирования "кольца"</p>

<figure class="large" id="id-GRtofpt0T1"><img alt="Cassandra Scale Out" class="iimagesv2metadata_2png" src="imagesv2/metadata_2.png">
<figcaption><span class="label">Рисунок 11-8. </span>Cassandra - Масштабирование</figcaption>
</figure>

<!-- End of scalable metadata section -->
</section>

<section data-type="sect1" id="data-protection-J5I3F8Tw">
<h3>Защита данных</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/OWhdo81yTpk">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//OWhdo81yTpk"></iframe></div>

<p>Платформа Nutanix использует фактор репликации (RF), и систему контрольных сумм,для обеспечения резервирования и доступности в случае отказа узлов или дисков.&nbsp;
  Как объяснялось выше, OpLog выступает в роли промежуточной области для обработки входящих операций записи на уровне SSD с низкой задержкой.&nbsp; 
  После записи в локальный OpLog данные синхронно реплицируются на одну или две CVM в зависимости от фактора репликации,
  прежде чем вернуть подтверждение успешной записи (ack) на узел.&nbsp; 
  Это позволяет быть уверенным, в том что данные существуют как минимум на двух или трех независимых узлах и находятся в безопасности. 
  Примечание: Для фактора репликации RF3, минимальное количество узлов - 5, а значит метаданные будут иметь фактор репликации RF5.&nbsp;</p>

<p>
  Узлы OpLog выбираются каждый раз при записи следующего 1GB данных, в процессе участвуют все узлы кластера.
  Множество факторов влияет на выбор следующих узлов - время отклика, загруженность, утилизация ресурсов и так далее.  
  Это исключает любую фрагментацию и гарантирует, что каждый CVM/OpLog может использоваться одновременно.
</p>

<p>Обеспечение RF настраивается на уровень контейнера, через интерфейс Prism. 
  Все узлы участвуют в репликации OpLog для исключения горячих точек и обеспечения линейной масштабируемости.&nbsp; 
  
  Когда данные начинают записываться для них рассчитывается контрольная сумма и сохраняется как часть метаданных.
  Затем данные ассинхронно сбрасываются в хранилище экстентов где для них применяется политика резервирования.&nbsp;
  В случае выхода из строя узла или диска данные перераспределяются по оставшимся узлам/дискам, с учетом уровня RF.&nbsp; 
  Каждый раз, когда происходит запрос к данным контрольная сумма пересчитывается, чтобы убедиться, что данные валидны.&nbsp; 
  Если контрольные суммы на совпадают - будет осуществлено чтение из резервной копии, а невалидные данные перезаписаны.</p>

<p>
  Данные постоянно находятся под наблюдением системы, чтобы даже в случае отсутствия операций I/O следить за их целостностью. 
  Служба Stargate будет постоянно проверять контрольные суммы групп экстентов, когда нет сильной нагрузки на РХД.
  Таким образом данные защищены для случаев выхода из строя секторов на накопителях и так далее.
</p>

<p>Ниже представлена логическая схема этих процессов:&nbsp;</p>

<figure id="id-A0tbFVF8Tx"><img alt="DSF Data Protection" class="iimagesv2data_protectionpng fse fs" src="imagesv2/data_protection.png">
<figcaption><span class="label">Рисунок 11-8. </span>РХД - защита данных</figcaption>
</figure>
</section>

<section data-type="sect1" id="availability-domains-WnIbC8TM">
<h3>Домены доступности</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/LDaNY9AJDn8">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//LDaNY9AJDn8"></iframe></div>

<p>Домены доступности (node/block/rack awareness) один из ключевых компонентов в распределенной системе, позволяющих
  правильно размешать данные относительно компонент, для обеспечения надежности.&nbsp; 
  На текущий момент РХД может обеспечивать распределение между узлами и блоками, так чтобы выход из строя целого блока не приводил к потере данных.
  Для больших кластеров есть возможность обеспечить выход из строя одного серверного шкафа целиком.&nbsp; 
  Блок - это серверное шасси, в котором может быть установлено один, два или четыре узла. 
  Примечание: Кластер должен включать минимум 3 блока, для обеспечения выхода их строя одного блока.&nbsp;</p>

<p>Для обеспечения нормального функционирования домена доступности на базе блоков рекомендуется
  использовать однотипные блоки.&nbsp; 
  Стандартные сценарии использования доменов доступности можно найти в нижней части этого раздела.&nbsp; 
  Три блока требуется для того, чтобы собрать кворум. Это могут быть три блока 3450 по 4 узла каждый.&nbsp; 
  При настройке домена доступности выход из строя блока целиком не влияет на работу ВМ и кластера.&nbsp; 
  Примечание: Внутри блока только БП и вентиляторы являются общими компонентами.</p>

<p>Домены доступности сфокусированы на следующих компонентах:</p>

<ul>
	<li>Данные (Диски ВМ)</li>
	<li>Метаданные (Cassandra)</li>
	<li>Конфигурация кластера (Zookeeper)</li>
</ul>

<h5>Данные</h5>

<p>Данные на РХД постоянно дублируются и остаются доступными в случае выхода из строя или планового отключения узлов или блоков.&nbsp; 
  Это актуально как для RF2 и RF3, так и для случае выхода из строя блоков, при использовании доменов доступности. 
  Если сравнить домен доступности на уровне узлов и блоков, то мы увидим, что при отказе узла данные потерявшие резервную копию будут скопированы на оставшиеся узлы в кластере.
  Такая же модель работы распространяется и на уровень блоков - все данные будут копированы на оставшиеся блоки.</p>

<p>На схеме ниже изображен пример работы репликации для инсталляции из трех блоков:</p>

<figure id="id-lNtMiOCATe"><img alt="Block Aware Replica Placement" class="iimagesv2avail_dom_1png" src="imagesv2/avail_dom_1.png">
<figcaption><span class="label">Рисунок 11-25. </span>Домены доступности - размещение реплик данных</figcaption>
</figure>

<p>В случае выхода из строя блока, данные потерявшие свою резервную копию будут дублированы на оставшихся узлах:</p>

<figure id="id-Q4t4IaC5TX"><img alt="Block Failure Replica Placement" class="iimagesv2avail_dom_2png" src="imagesv2/avail_dom_2.png">
<figcaption><span class="label">Рисунок 11-26. </span>Домены доступности - выход из строя блока</figcaption>
</figure>

<h5>Домены доступности и отказоустойчивость</h5>
<p>Рассмотрим некоторые стандартные сценарии и уровни отказоустойчивости:</p>

<table>
  <tr>
    <th></th>
    <th></th>
    <th colspan="2">Обрабатываемые отказы</th>
  </tr>
  <tr>
    <th>Количество блоков</th>
    <th>Правила размещения</th>
    <th>Кластер FT1</th>
    <th>Кластер FT2</th>
  </tr>
  <tr>
    <td>&lt;3</td>
    <td>Узел</td>
    <td>Один узел</td>
    <td>Два узла</td>
  </tr>
  <tr>
    <td>3-5</td>
    <td>Узел+Блок</td>
    <td>Один блок <br/>(до 4х узлов)</td>
    <td>Один блок <br/>(до 4х узлов)</td>
  </tr>
  <tr>
    <td>5+</td>
    <td>Узел+Блок</td>
    <td>Один блок <br/>(до 4х узлов)</td>
    <td>Два блока <br/>(до 4х узлов)</td>
  </tr>
</table>

<p>
  Начиная с версии ПО 4.5 включение доменов доступности на уровне блоков не является обязательным требованием.
  Это было сделано, чтобы ассиметричные кластеры не отключали эту функцию. Однако лучше иметь одинаковые блоки, для использования этой функции и избежания ситуации с неравномерно распределенными данными. 
</p>

<p>
  До версии ПО 4.5 следующие рекомендации должны выполняться:
</p>
<ul>
  <li>Если количество SSD <strong>или</strong> HDD отличаются между блоками &gt; возможна организация домена доступности на базе: <strong>Узлов</strong></li>
  <li>Если количество SSD и HDD отличаются между блоками &lt; возможна организация домена доступности на базе: <strong>Блоков + Узлов</strong></li>
</ul>

<p>Максимальное расхождение рассчитывается по формуле: 100 / (RF+1)</p>

<ul>
	<li>Например: 33% для RF2 или 25% для RF3</li>
</ul>

<h5>Метаданные</h5>

<p>Как упомяналось в разделе Масштабируемые метаданные - Платформа использует серьезно модифицированную СУБД Cassandra для хранения метаданных и другой информации.&nbsp; 
   Cassandra использует структуру "кольцо" и репликацирует метаданные по N узлов для обеспечения отказоустойчивости и доступности данных.</p>

<p>На следующем рисунке показан пример кольца Cassandra для кластера с 12 узлами:</p>

<figure id="id-n3tDuwC1TN"><img alt="12 Node Cassandra Ring" class="iimagesv2avail_dom_3png fse fs image" src="imagesv2/avail_dom_3.png" style="width: 50%; height: 50%">
<figcaption><span class="label">Рисунок 11-27. </span>Cassandra - Кластер из 12 узлов</figcaption>
</figure>

<p>Репликация между узлами Cassandra выполняется по часовой стрелке, по всему кольцу.&nbsp; 
  Если настроены домены доступности на уровне блоков, то узлы кластера Cassandra размещаются так, чтобы находиться в разных блоках.</p>

<p>Ниже представлен пример размещения узлов кластера по блокам:</p>

<figure id="id-m2tmHdCoTN"><img alt="Cassandra Node Block Aware Placement" class="iimagesv2avail_dom_4png" src="imagesv2/avail_dom_4.png">
<figcaption><span class="label">Рисунок 11-28. </span>Cassandra - Размещение узлов по блокам</figcaption>
</figure>

<p>При отказе или выводе на обслуживание блока Nutanix как минимум две копии данных будут доступны.</p>

<p>Ниже отображена топология репликации между узлами, для объединения в кольцо:</p>

<figure id="id-jRtRfmCeTa"><img alt="Full Cassandra Node Block Aware Placement" class="iimagesv2avail_dom_5png" src="imagesv2/avail_dom_5.png">
<figcaption><span class="label">Рисунок 11-29. </span>Cassandra - Размещение данных для обеспечения отказа блока целиком</figcaption>
</figure>

<div data-type="note" class="note" id="metadata-awareness-conditions-wDioinCwTQ"><h6>Примечание</h6>
<h5>Уровни доступности метаданных</h5>
<p>Ниже рассмотрены сценарии и уровни доступности метаданных для них:</p>

<ul>
	<li>Отказоустойчивость. Уровень 1 - FT1 (Данные RF2 / Метаданные RF3) будет доступен выход из строя одного блока если:
		<ul>
			<li>
				&gt;= 3 блока
			</li>
			<li>Где X - количество узлов в блоке, с максимальной плотностью. Тогда, оставшиеся блоки должны включать в себя не менее 2х узлов.
				<ul>
					<li>
						Например: 4 блока с 2,3,4,2 узлов на блок соответственно.
						<ul>
							<li>
                В этой конфигурации максимальное количество узлов на блок - 4, это значит что в оставшихся трех блоках должно быть 2x4 (8) узлов.
                В этом случае реализовать отказ одного блока без влияния на кластер невозможно, так как в оставшихся узлах всего 7 узлов.
							</li>
						</ul>
					</li>
					<br>
					<li>
            Например: 4 блока с 3,3,4,3 узлов на блок соответственно.
						<ul>
							<li>
                В этой конфигурации максимальное количество узлов на блок - 4, это значит что в оставшихся трех блоках должно быть 2x4 (8) узлов.
                В этом случае реализовать конфигурацию позволяющую обеспечить отказ блока без влияния на кластер можно. Так как остается еще 9 узлов.
                Что выше нашего минимума.
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>Отказоустойчивость. Упровень 2 - FT2 (Data RF3 / Metadata RF5) будет доступен выход из строя одного блока если:
		<ul>
			<li>
				&gt;= 5 блоков
			</li>
			<li>Где X - количество узлов в блоке, с максимальной плотностью. Тогда, оставшиеся блоки должны включать в себя не менее 4х узлов.
				<ul>
					<li>
            Например: 6 блоков с 2,3,4,2,3,3 узлов на блок соответственно.
						<ul>
							<li>
                В этой конфигурации максимальное количество узлов на блок - 4, это значит что в оставшихся трех блоках должно быть 4x4 (16) узлов.
                В этом случае реализовать конфигурацию позволяющую обеспечить отказ блока без влияния на кластер невозможно.
                Так как остается только 13 узлов.
							</li>
						</ul>
						</li>
					<br>
					<li>
            Например: 6 блоков с 2,4,4,4,4,4 узлов на блок соответственно.
						<ul>
							<li>
                В этой конфигурации максимальное количество узлов на блок - 4, это значит что в оставшихся трех блоках должно быть 4x4 (16) узлов.
                В этом случае реализовать конфигурацию позволяющую обеспечить отказ блока без влияния на кластер можно. Так как остается еще 18 узлов.
                Что выше нашего минимума.
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
</div>

<h5>Данные о конфигурации кластера</h5>

<p>Nutanix использует Zookeeper для хранения основных настроек кластера.&nbsp; Эта роль так же является распределенной между узлами, и может функционировать при отказе блока целиком.</p>
<p>Ниже изображена пример схемы размещения 3 ролей Zookeeper для обеспечения отказа одного блока без влияния на кластер:</p>

<figure id="id-qMtEh2CyTQ"><img alt="Zookeeper Block Aware Placement" class="iimagesv2avail_dom_6png" src="imagesv2/avail_dom_6.png">
<figcaption><span class="label">Рисунок 11-30. </span>Zookeeper - отказоустойчивость</figcaption>
</figure>

<p>В случае отказа блока и потери одного из активных сервисов Zookeeper - сервис будет запущен на другом узле кластера, как показано ниже:</p>

<figure id="id-EGt4soCoTo"><img alt="Zookeeper Placement Block Failure" class="iimagesv2avail_dom_7png" src="imagesv2/avail_dom_7.png">
<figcaption><span class="label">Рисунок 11-31. </span>Zookeeper - выход из строя блока</figcaption>
</figure>

<p>Когда блок будет снова доступен - сервис Zookeeper будет перемещен обратно, где он находился до отказа блока.</p>
<p>Примечание: До версии 4.5 такой перенос не выполнялся автоматически и требовал ручного вмешательства.</p>

</section>

<section data-type="sect1" id="data-path-resiliency-BNIWfkTZ">
<h3>Отказоустойчивость путей данных</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/SJIb_mTdMPg">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//SJIb_mTdMPg"></iframe></div>

<p>Надежность и отказоустойчивость является ключевыми частями концепции любого классического и распределенного хранилища данных.&nbsp;</p>

<p>Но в отличие от классических архитектур которые строятся вокруг отказоустойчивого оборудования, Nutanix считает, что оборудование в какой-то момент может сломаться.&nbsp;Учитывая это, система спроектирована так, чтобы решать проблемы с оборудование без прерывания сервисов.</p>

<p>Примечание: Мы не считаем, что все оборудование имеет низкое качество. Это всего лишь условный концепт.&nbsp; 
  Команды компании, которые занимаются оборудованием и тестированием имеют высочайшую квалификацию.</p>

<p>
  Как упоминалась в предыдущих главах данные и метаданные защищаются путем дублирования, согласно уровню RF на базе глобального уровня FT.
  Начиная с версии ПО 5.0 поддерживаются FT1 и FT2, которые обеспечивают RF3 для метаданных и RF2 для данных, а так же RF5 для метаданных и RF3 для данных соответственно.
</p>

<p>
  Более подробно защита метаданных описана в разделе "Масштабируемые метаданные".
  Механизмы защиты данных подробно описаны в разделе "Защита данных".
</p>

<p>
  В нормальном состоянии данные по кластеру будут размещены следующим образом:
</p>

<figure id="id-EGt4so1243"><img alt="Data Path Resiliency - Normal State" src="imagesv2/resiliency_1.png">
<figcaption><span class="label">Рисунок. </span>Отказоустойчивость Путей Данных - Нормальное состояние</figcaption>
</figure>

<p>
  Как вы видите виртуальные машины и их диски имеют две или три копии на хранилище данных и распределены между узлами и физическими устройствами хранения.
</p>

<div data-type="note" class="note" id="pro-tip-05i5c2349"><h6>Примечание</h6>
<h5>Распределение данных</h5>

<p>Обеспечивая распределение метаданных и данных по всем узлам и дисковым устройствам, мы можем обеспечить максимально возможную производительность при обычном потоке данных их повторной защите.</p>

<p>
  По мере поступления данных в систему их первичные копии и копии реплик будут распределяться между локальными и всеми остальными удаленными узлами. Тем самым мы можем устранить любые потенциальные горячие точки (например, медленно узел или диск) и обеспечить согласованную производительность записи.
</p>

<p>
  В случае сбоя диска или узла, когда данные должны быть повторно защищены - вся мощность кластера может быть использована для перераспределения данных. В этом случае сканирование метаданных (чтобы определить местоположение данных на отказавших устройствах и найти их реплики) будет равномерно распределено по всем CVM.  Как только реплики данных найдены, все работоспособные CVM, их дисковые устройства (SSD+HDD) и сетевые интерфейсы узла будут одновременно использоваться, чтобы восстановить данные.
</p>

<p>
  Например, если в кластере из 4х узлов, где вышел из строя один диск - каждая CVM возьмет на себя 25% нагрузки по сканированию метаданных и восстановления данных. Для кластера из 10 узлов - каждая CVM возьмет на себя 10% нагрузки по сканированию метаданных и восстановления данных. Для кластера из 50 узлов - каждая CVM возьмет на себя 2% нагрузки по сканированию метаданных и восстановления данных.
</p>

<p>
  Ключевой момент: Используя встроенные механизмы распределения данных и нагрузки по кластеру мы можем быть уверенными, что мы предоставляем максимальную производительность и обеспечиваем минимальное время на восстановление данных. А так же все остальные процессы на кластере проистекают максимально оптимальными путями, за минимальные промежутки времени. Это касается EC, компрессии, дедупликации и так далее.
</p>

<p>
  Классические решения, при этом имеют важнейшее отличие - при наличии двух контроллеров, либо дисков с зеркалированием - так или иначе при росте нагрузки страдает вся система. Нагрузка на любой парный компонент приводит к увеличению нагрузки на его пару, увеличивается время отклика, появляются узкие места.
</p>

<p>
  Так же в классическом решении выход из строя одного из контроллеров, или любого парного устройства снижают отказоуствойчивость решения, а так часто влияют на его производительность. Так, в случае сбоя, система теряет в производительности и находится в крайне нестабильном состоянии до момента замены компонента, а иногда и после. Балансировка данных происходит медленнее, так как производительность ограничена оставшимся в паре компонентом. Кроме того есть риск потери данных, если возникнет какая-то еще неиспра вность.
</p>
</div>

<h5>Потенциальные уровни отказа</h5>

<p>Распределенная Платформа Nutanix может обеспечивать отказоуствойчивость на базе компонент, сервисов, и служебных ВМ (CVM). Итак Платформа обслуживает отказы на следующих уровнях:</p>

<ul>
	<li>Отказ диска</li>
	<li>"Отказ" CVM</li>
	<li>Отказ узла</li>
</ul>

<div data-type="note" class="note" id="pro-tip-05i5c2349"><h6>Примечание</h6>
<h5>Когда начинается перераспределение данных?</h5>

<p>В случае незапланированного сбоя восстановление данных начинается <b>незамедлительно</b>. В некоторых случаях мы будем пытаться выключать компоненты автоматически, если они работают некорректно.</p>

<p>В отличие от некоторых других поставщиков решений, которые ждут 60 минут до того как начать процесс репликации данных и держат всего одну копию в течении этого времени, мы не готовы идти на риск потери данных даже если мы увеличим использование объема хранилища.</p>

<p>Мы можем восстанавливать данные сразу потому, что: 1. гранулярность наших метаданных позволяет это делать, 2. платформа сразу же, динамически, определяет узлы для RF (в случае сбоя любого компонента мы продолжаем писать данные согласно уровню RF), 3. мы определяем восстановление сбойных компонент и после их восстановления проверяем актуальность данных, если они актуальны - мы не будем восстанавливать их повторно. В таких случаях данные могут бы "пере-реплицированы", и Curator найдет лишние копии и удалит их.</p>

</div>

<h5>Отказ диска</h5>

<p>Отказом диска считается ситуация, когда диск был удален, на нем растет количество ошибок, или же он перестал отвечать или генерирует ошибки I/O. Когда Stargate идентифицирует ошибки I/O или отсутствие ответов от дисков в определенный промежуток времени - диск помечается как отключенный. После того, как произошло одно из перечисленных событий система запустит S.M.A.R.T. и проверит статус устройства. Если все тесты будут выполнены, диск будет помечен как работоспособный. В обратном случае его статус не изменится. Если Stargate пометил диск как отключенный несколько раз (сейчас это три раза в час), Hades не будет восстанавливать статус диска, даже если проверка S.M.A.R.T. выполнена успешно.</p>

<p>Влияние на ВМ:</p>

<ul>
	<li>Событие HA: <strong>Нет</strong></li>
	<li>Прерывание I/O: <strong>Нет</strong></li>
	<li>Задержки: <strong>Без влияния</strong></li>
</ul>

<p>В случае сбоя диска немедленно будет выполнено сканирование сервисом Curator. Он проверит метаданные и постарается найти копии данных вышедшего из строя диска на других узлах и дисках кластера.</p>

<p>Как только они будут найдены - запуститься процесс репликации этих копий, для восстановления нужного количества согласно RF.&nbsp;</p>

<p>
  В это время будут запущены тесты Drive Self Test (DST) для наблюдения за ошибками в журналах SMART.
</p>

<p>
  Ниже изображена схема, на которой показан отказ диска и повторная репликация данных:
</p>

<figure id="id-EGt4so1243"><img alt="Data Path Resiliency - Disk Failure" src="imagesv2/resiliency_2.png">
<figcaption><span class="label">Рисунок. </span>Отказоустойчивость Путей Данных - Отказ диска</figcaption>
</figure>

<p>Важно отметить как Nutanix распределяет данные между всеми узлами / CVM / дисками; все узлы / CVM / диски будут участвовать в повторной репликации данных.&nbsp;</p>

<p>Это существенно снижает время требуемое для защиты данных, так как вся мощность кластера используется; чем больше кластер - тем быстрее будут защищены.</p>

<h5>Отказ узла</h5>

<p>Влияние на ВМ:</p>

<ul>
	<li>Событие HA: <strong>Да</strong></li>
	<li>Прерывание I/O: <strong>Нет</strong></li>
	<li>Задержки: <strong>Без влияния</strong></li>
</ul>

<p>Выход из строя узла является отправной точкой для запуска события HA, ВМ размещаемые на сбойном узле будут перезапущены на других узлах кластера.&nbsp; После перезапуска, ВМ продолжат выполнять операции I/Os, и как обычно они будут обслуживаться локальными CVM.</p>

<p>Почти как и при сбое диска, Curator выполнит сканирование данных и найдет их копии на работающих узлах/дисках. Как только данные будут найдены - будет запущен процесс повторного их клонирования.</p>

<figure id="id-EGt4so1243"><img alt="Data Path Resiliency - Node Failure" src="imagesv2/resiliency_3.png">
<figcaption><span class="label">Рисунок. </span>Отказоустойчивость Путей Данных - Отказ узла</figcaption>
</figure>

<p>В случае если узел продолжает быть недоступным длительный период (начиная с версии ПО 4.6 - 30 минут) CVM этого узла будет удалена из кольца метаданных.&nbsp; Она будет подключена назад, как только снова станет доступной и проработает некоторое время.</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Состояние высокой доступности данных будет представлено в интерфейсе Prism.</p>

<p>
  Вы так же можете проверить состояние высокой доступности данных через cli:
  <p class="codetext">
    # Статус на уровне узлов <br />
    ncli cluster get-domain-fault-tolerance-status type=node
  </p>
  <p class="codetext">
    # Статус на уровне блоков <br />
    ncli cluster get-domain-fault-tolerance-status type=rackable_unit
  </p>

  <p>
    Информация о состоянии высокой доступности данных должны быть всегда актуальны, однако вы можете обновить их принудительно запустив задачу сервиса Curator.
  </p>
</p>
</div>

<h5>"Отказ" CVM</h5>

<p>"Отказ" CVM может возникать при перезагрузке CVM, которая будет временно недоступна на время перезагрузки.&nbsp; Система спроектирована таким образом, чтобы обрабатывать такие перезагрузки без проблем.&nbsp; В случае недоступности CVM все локальные операции I/O будут временно перенаправлены на другие CVM в кластере.&nbsp; Этот механизм может отличаться между разными гипервизорами.&nbsp;</p>

<p>Штатное обновление ПО кластера задействует функцию обеспечения HA для CVM, т.е. на время обновления CVM все I/O обрабатывают остальные CVM в кластере.</p>

<p>Влияние на ВМ:</p>

<ul>
	<li>Событие HA: <strong>Нет</strong></li>
	<li>Прерывание I/O: <strong>Нет</strong></li>
	<li>Задержки: <strong>Потенциально могут вырасти, так как I/O осуществляется по сети</strong></li>
</ul>


<p>В случае "отказа" одной из CVM  все обслуживаемые ей операции I/O будут распределены между остальными служебными ВМ в кластере.&nbsp; Для гипервизоров ESXi и Hyper-V обработка событий HA для CVM выполняется через операцию "CVM Autopathing", которая вызывается через HA.py (как “happy”), которая изменит пути для трафика идущего к внутреннему адресу (192.168.5.2) на внешние адреса других CVM в кластере.&nbsp; Это позволяет обеспечить доступ хранилищу и данным, перенаправив, I/O к другим CVM кластера.</p>

<p>Как только CVM вернется в рабочее состояние, пути будут удалены и все локальные операции I/O снова будут обрабатываться на локальной CVM.</p>

<p>В случае с AHV используется iSCSI multi-pathing где приоритетный путь - путь к CVM и два дополнительных указывают на другие CVM в кластере.&nbsp; В случае ошибок на основном пути - трафик будет переключен на один из запасных.</p>

<p>Как и в случае с ESXi и Hyper-V, если CVM вернется в рабочее состояние - будет снова выбран путь к локальной CVM.</p>
</section>

<section data-type="sect1">
<h3>Оптимизация емкости</h3>

<p>
  Платформа Nutanix включает в себя широкий спектр технологий для оптимизации ресурсов хранения, которые эффективно используются при любом доступном пространстве хранения и типах нагрузки. Эти технологии являются интеллектуальными и адаптируются для различных типов нагрузок, исключая необходимость в ручной настройке и оптимизации.
</p>

<p>
  Следующие типы оптимизации емкости используются:
</p>

<ul>
  <li>
    Избыточное кодирование
  </li>
  <li>
    Компрессия
  </li>
  <li>
    Дедупликация
  </li>
</ul>

<p>

  Более подробно с каждой из этих функций можно ознакомиться в следующих разделах.
</p>

<p>
  В таблице расписано какие виды оптимизации применяются к различным типам нагрузки:
</p>

<table>
  <tr>
    <th>Преобразование Данных</th>
    <th>Наиболее подходящие типы приложений</th>
    <th>Комментарии</th>
  </tr>
  <tr>
    <td>Избыточное кодирование</td>
    <td>Большинство</td>
    <td>Обеспечивает высокую доступность и сокращает перерасход пространства, в отличие от классического RF.  
      Не влияет на производительность стандартных операций I/O.  
      Есть небольшой перерасход в случае выхода из строя диска/узла/блока, когда данные должны быть декодированы.</td>
  </tr>
  <tr>
    <td>Сжатие входящих данных</td>
    <td>Все</td>
    <td>Не влияет на случайные операции I/O, помогает увеличить утилизацию ресурсов хранения.  
      Данный уровень сжатия обеспечивает прирост производительности для больших или последовательных операций I/O уменьшая количество данных для репликации и сокращая чтение с диска.</td>
  </tr>
  <tr>
    <td>Фоновое сжатие данных</td>
    <td>Не применимо</td>
    <td>Данный уровень сжатия данных обеспечит эффективное сжатие и пост-процессинг для случайных и коротких операций I/O.</td>
  </tr>
  <tr>
    <td>Дедупликация кэшируемых данных</td>
    <td>P2V/V2V,Hyper-V (ODX),Клонирование между контейнерами</td>
    <td>Повышение эффективности кэша для данных, которые не были созданы или склонированны при помощи ПО Acropolis</td>
  </tr>
  <tr>
    <td>Дедупликация хранимых данных</td>
    <td>Все</td>
    <td>Преимущества перечисленных выше видов оптимизации и снижение накладных расходов на диск</td>
  </tr>
</table>

<section data-type="sect2" id="erasure-coding-nrIjcoTx">
<h4>Избыточное кодирование</h4>

<p>Платформа Nutanix использует фактор рекпликации для защиты и обеспечения доступности данных.&nbsp; 
  Такой подход позволяет высочайший уровень производительности, так как не требует чтения более чем из одной копии данных и не требует пересчета 
  контрольных сумм в случае отказов.&nbsp; 
  Однако, он увеличивает стоимость хранения, так как объем хранимых данных увеличивается за счет хранения дополнительных полных копий данных.&nbsp;</p>

<p>Для обеспечения баланса между доступностью и сохранением полезного объема хранения DSF предоставляет возможность использования технологии избыточного кодирования (EC).</p>

<p>Концепция EC похожа на RAID (например 4, 5, 6,  и пр.) где производится контроль четности, EC кодирует срез блоков данных на разных узлах и осуществляет расчет четности.&nbsp;
   В случае недоступности узла и/или диска четность используется для расчета потерянных блоков данных (происходит декодирование данных).&nbsp; 
   На РХД блок данных - это группа экстентов, и каждый такой блок находится и принадлежит разным файлам (vDisk).</p>

<p>Количество данных и блоков контроля четности в срезе - настраивается на базе уровня устойчивости к сбоям.&nbsp; 
  Конфигурация определяет количество &lt;блоков данных&gt; и количество &lt;блоков четности&gt;.</p>

<p>Например, доступность уровня “RF2 like” (т.е. N+1) может содержать 3 или 4 блока данных и  1 блок для контроля четности на срез данных (т.е. 3/1 или 4/1).&nbsp; 
  Уровень “RF3 like” (т.е. N+2) может содержать 3 или 4 блока данных и 2 блока для контроля четности (т.е. 3/2 или 4/2).</p>

<div data-type="note" class="note" id="pro-tip-1RinF8cdTQ"><h6>Примечание</h6>
<h5>EC + домены доступности</h5>

<p>Начиная с версии 5.8, EC может размещать блоки данных и блоки контроля четности так, чтобы обеспечивать отказ блоков согласно идее доменов доступности.
  (До версии 5.8 данные размещались так, чтобы отказ узлов не приводил к потере данных).</p>

<p>Уже созданные контейнеры с включенной функцией EC не будут незамедлительно перенастроены согласно доменам доступности на уровне блоков сразу после обновления до версии 5.8. 
  Только если блоков и узлов в кластере достаточно (strip size (k+n) + 1) режим обработки отказов на уровне узлов будет перенастроен в режим обработки отказов блоков. 
  Новые контейнеры с включенным EC будут сразу размещаться для обработки отказов на уровне блоков.  </p>
</div>

<p>Ожидаемый перерасход ресурсов может быть рассчитан как &lt;кол-во блоков четности&gt; / &lt;кол-во блоков данных&gt;.&nbsp; 
  Например, срез данных с соотношением 4/1 имеет 25% перерасхода пространства, или 1.25X в сравнении с 2X для RF2.&nbsp; 
  Срез данных 4/2 будет иметь 50% перерасхода, или 1.5X в сравнении с 3X для RF3.</p>

<p>В таблице ниже расписаны примеры перерасхода ресурсов при разных уровнях избыточности:</p>

<table>
  <tr>
    <th></th>
    <th colspan="2"><center>FT1 (RF2)</center></th>
    <th colspan="2"><center>FT2 (RF3)</center></th>
  </tr>
  <tr>
    <td>Размер кластера<br>(Узлы)</td>
    <td>Размер среза данных EC<br>(data/parity blocks)</td>
    <td>Перерасход ресурсов при EC<br>(vs. 2X of RF2)</td>
    <td>Размер среза данных EC<br>(data/parity)</td>
    <td>Перерасход ресурсов при <br>(vs. 3X of RF3)</td>
  </tr>
  <tr>
    <td>4</td>
    <td>2/1</td>
    <td>1.5X</td>
    <td>N/A</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>5</td>
    <td>3/1</td>
    <td>1.33X</td>
    <td>N/A</td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>6</td>
    <td>4/1</td>
    <td>1.25X</td>
    <td>2/2</td>
    <td>2X</td>
  </tr>
  <tr>
    <td>7</td>
    <td>4/1</td>
    <td>1.25X</td>
    <td>3/2</td>
    <td>1.6X</td>
  </tr>
  <tr>
    <td>8+</td>
    <td>4/1</td>
    <td>1.25X</td>
    <td>4/2</td>
    <td>1.5X</td>
  </tr>
</table>

<div data-type="note" class="note" id="pro-tip-P0iqcdcwT1"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Рекомендуется иметь хотя бы один дополнительный узел (или блок) в кластере, чем требуемое количество узлов/блоков для настройки EC.
  Это необходимо, для штатной обработки отказа устройства и нормального перераспределения данных. 
  Никакой дополнительной нагрузки на чтение не будет, как только срез данных будет восстановлен и данные будут перераспределены.
  Например, для среза данных 4/1 необходимо иметь как минимум 6 узлов в кластере для обеспечения отказа узла или же 6 блоков, для обеспечения отказа блока целиком.
  Таблица выше описывает наиболее оптимальные конфигурации кластеров для использования избыточного кодирования данных.</p>
</div>

<p>Кодирование происходит в фоновом режиме и использует возможности сервиса Curator MapReduce для распределения нагрузки по узлам кластера.&nbsp; 
  Так как это фоновая операция - влияние на производительность активных операций I/O не происходит.</p>

<p>Стандартная среда с использованием RF будет выглядеть следующим образом:</p>

<figure id="id-lNtGhwcATe"><img alt="Typical DSF RF Data Layout" class="iimagesv2ec_1png" src="imagesv2/ec_1.png">
<figcaption><span class="label">Рисунок 11-10. </span>Стандартное размещение данных при RF</figcaption>
</figure>

<p>В этом случае у нас есть данных для которых выполняется и RF2 и RF3, основные копии которых размещаются локально, а реплики - распределяются по всему кластеру.</p>

<p>Когда Curator выполняет полное сканирование он найдет подходящие группы экстентов, к которым можно применить кодирование.
  Подходящие - значит, что эти группы экстентов должны быть "холодными", т.е. их записали какое-то время назад.
  Платформа считает данные "холодными" согласно настройке сервиса Curator - Gflag: curator_erasure_code_threshold_seconds. 
  После того как подходящий кандидат был найден задача кодирования будет распределена при помощи подсистемы Chronos.</p>

<p>Ниже изображены два типа срезов данных с которыми работает EC - 4/1 и 3/2 :</p>

<figure id="id-4mtATYc8Tk"><img alt="DSF Encoded Strip - Pre-savings" class="iimagesv2ec_2png" src="imagesv2/ec_2.png">
<figcaption><span class="label">Рисунок 11-11. </span>EC - Пред-процессинг</figcaption>
</figure>

<p>После успешного кодирования среза данных и вычисления четности лишние реплики групп экстентов удаляются.</p>

<p>На следующем рисунке показана среда после запуска EC и эффективном хранении :</p>

<figure id="id-n3tMFNc1TN"><img alt="DSF Encoded Strip - Post-savings" class="iimagesv2ec_3png" src="imagesv2/ec_3.png">
<figcaption><span class="label">Рисунок 11-12. </span>EC - Пост-процессинг</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-mEiEtYcoTN"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>EC прекрасно работает с сжатием входящих данных, что обеспечит дополнительный уровень экономии пространства. Я использую такой подход в моих средах.</p>
</div>
</section>

<section data-type="sect2" id="compression-ONIqiMTA">
<h4>Компрессия</h4>

<!-- TODO: Add more context around reg vs. cold data -->

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/ERDqOCzDcQY">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//ERDqOCzDcQY"></iframe></div>

<p>Платформа Nutanix использует собственный механизм оптимизации пространства - Capacity Optimization Engine (COE).
  Он отвечает за выполнение преобразования данных для увеличения эффективности использования дисков.&nbsp; 
  На текущий момент компрессия - одна из основных возможностей COE обеспечивающая оптимизацию хранения данных.
  DSF обеспечивает компрессию входящих и хранимых данных, чтобы максимально отвечать запросам пользователей.  
  Начиная с 5.1 компрессия хранимых данных используется по умолчанию.</p>

<p>
  Компрессия входящих данных будет выполняться для последовательных потоков данных или больших операций I/O (>64K) в момент записи на хранилище экстентов (SSD + HDD).  
  Сюда включен так же процесс сброса данных из OpLog, и пропущенную им последовательную запись.
</p>

<div data-type="note" class="note" id="pro-tip-mEiEtYcoTN"><h6>Примечание</h6>
<h5>Компрессия OpLog</h5>

<p>С версии 5.0, OpLog будет выполнять компрессию всех входящих потоков записи >4K, которые показывают хороший уровень сжатия (Gflag: vdisk_distributed_oplog_enable_compression).  
  Это обеспечивает большую эффективность использования емкости OpLog и способствует стабильной производительности.</p>

<p>
  Когда происходит перемещение данных из OpLog в хранилище экстентов данные будут декомпрессированны, объединены и снова сжаты до объектов объемом 32K (начиная с 5.1).
</p>

<p>
  Эта функция постоянно включена и не требует пользовательской настройки.
</p>
</div>

<p>
  Компрессия хранимых данных будет применяться уже после записи данных на РХД,
  посредством запуска задач Curator. Задачи выполняются равномерно по всему кластеру.
  Когда включена компрессия для входящих данных, а операции I/O в основном случайные, то данные будут записаны в OpLog без компрессии,
  консолидированы и только потом для них выполнится компрессия перед записью на хранилище экстентов.
</p>

<p>
  Nutanix использует LZ4 и LZ4HC для компрессии данных Asterix и за его пределами. 
  До Asterix используется библиотека Google Snappy которая предоставляет хороший коэффициент сжатия данных с минимальным перерасходом вычислительных ресурсов,
  а так же выполняет процедуры компрессии/декомпрессии максимально быстро.
</p>

<p>
  Нормальные данные будут сжиматься с использованием LZ4 который предоставляет отличное соотношение компрессии и производительности.
  Для "холодных" данных будет использоваться LZ4HC для увеличения коэффициента сжатия.
</p>

<p>
  Храктеристики "холодных" данных:
</p>

<ul>
  <li>
    Нормальные данные: Не было запросов R/W более 3 дней (Gflag: curator_medium_compress_mutable_data_delay_secs)
  </li>
  <li>
    Неизменные данные (снимки):Не было запросов R/W более 1 дня (Gflag: curator_medium_compress_immutable_data_delay_secs)
  </li>
</ul>

<p>Ниже приведен пример того, как происходит взаимодействие компрессии входящих данных и путей данных РХД:</p>

<figure id="id-GRtVFZi0T1"><img alt="Inline Compression I/O Path" class="iimagesv2compression_1png" src="imagesv2/compression_1.png">
<figcaption><span class="label">Рисунок 11-13. </span>Компрессия входящих данных - пути I/O</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-1RiZtvidTQ"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Старайтесь всегда использовать компрессию входящих данных, так как происходит сжатие только больших операций или последовательных потоков записи, это никак не влияет на производительность при случайной записи.</p>

<p>
  Это так же позволит увеличить доступную для использования емкость на уровне SSD, при этом увеличив производительность и позволит большему количеству данных находиться на SSD. А так же, сжимая большие или последовательные данные сразу на входе достигается большая производительность при выполнении репликации,
  так как репликация будет выполняться для уже сжатых данных. Это значит что внутри кластера будет копироваться меньший объем данных, чем без использования компрессии.
</p>

<p>
	Компрессия входящих данных также хорошо сочетается с технологией избыточного кодирования.
</p>

</div>

<p>Для автономного сжатия все новые операции I/O записываются в несжатом состоянии и следуют обычному пути операций I/O на DSF.&nbsp; 
  После настраиваемой задержки данные могут быть сжаты. Сжатие может происходить в любом месте хранилища экстентов.
  Автономное сжатие использует Curator MapReduce, чтобы все узлы участвовали в процессе сжатия данных.&nbsp; 
  Сервис Cronos будет следить за задачами по сжатию данных.</p>

<p>Ниже показан пример как компрессия хранимых данных взаимодействует с путями I/O:</p>

<figure id="id-NMt1cmiYT8"><img alt="Offline Compression I/O Path" class="iimagesv2compression_2png" src="imagesv2/compression_2.png">
<figcaption><span class="label">Рисунок 11-14. </span>Компрессия хранимых данных - пути I/O</figcaption>
</figure>

<p>Для операций I/O на чтение данные сначала проходят декомпресиию в памяти и затем выполняется непосредственно операция I/O.</p>

<p>Вы можете посмотреть текущий коэффициент сжатия данных через пользовательский интерфейс Prism в секции "Storage" главного экрана.</p>
</section>

<section data-type="sect2" id="elastic-dedupe-engine-b1I4I5T5">
<h4>Эластичная дедупликация</h4>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/C-rp13cDpNw">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//C-rp13cDpNw"></iframe></div>

<p>Механизм The Elastic Dedupe Engine - реализован на программном уровне и позволяет обеспечивать дедупликацию на уровне Хранилища экстент, а так же повышать производительность Универсального Кэша.&nbsp; Потоки данных хэшируются с использованием SHA-1 хешей с гранулярностью в 16K.&nbsp; Хэш создается на этапе попадания данных в систему и сохраняется как часть метаданных.&nbsp; Примечание: Изначально мы использовали гранулярность в 4K, однако затем тесты 16K показали лучшее соотношение эффективности дедупликации и перерасход при работе метаданных.&nbsp; Дедуплицированные данные попадают в Универсальный кеш с гранулярностью в 4K.</p>

<p>Классические решения выполняют расчет хэшей в фоновом процессе после того, как данные записаны. Такой подход требует выполнять повторное чтение. Платформа Nutanix выполняет расчет хэшей на лету.&nbsp; Для копий данных, которые могут быть дедуплицированы на уровне хранения выполнять повторное чтение или их сканирование. Более того - лишние копии данных можно удалить.</p>

<p>
Чтобы сделать работу с метаданными более эффективной пересчет хешей постоянно мониторится для проверки на возможность дедуплицирования. хэши с низким значением счетчика будут сброшены для минимизации перерасхода при работе с метаданными. Для минимизации фрагментации полные экстенты будут дедуплицированы в первую очередь. 
</p>

<div data-type="note" class="note" id="pro-tip-N8iNFMIYT8"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Используйте дедупликацию для базовых образов (вы можете рассчитать для них хэши вручную используя vdisk_manipulator), чтобы воспользоваться преимуществами единого кэша.</p>

<p>Используйте дедупликацию для P2V / V2V,&nbsp;когда используется Hyper-V с ODX копирует данные целиком, или при клонировании между контейнерами (рекомендуется использовать один контейнер).</p>

<p>В большинстве других случаев сжатие даст наибольшую экономию мощности и должно использоваться вместо дедупликации.</p>
</div>

<p>На следующем рисунке показан пример того, как модуль Elastic Dedupe масштабирует и обрабатывает локальные запросы I/O:</p>

<figure class="large" id="id-NMt2fMIYT8"><img alt="Elastic Dedupe Engine - Scale" class="iimagesv2dedup_1png" src="imagesv2/dedup_1.png">
<figcaption><span class="label">Рисунок 11-16. </span>Эластичная дедупликация - Масштабирование</figcaption>
</figure>

<p>Снятие хэшей выполняется в процессе попадания данных на кластер для операций IO более 64K (при начале операции I/O или при сбросе из OpLog).&nbsp; Intel помогает ускорить процесс расчета SHA-1, с минимальным перерасходом процессорного времени. &nbsp; В случаях, когда хэши не были сняты для входящих I/O (например, имеют размер менее 64K) хэши будут сняты в фоновом процессе. &nbsp;Механизм Elastic Deduplication Engine охватывает как хранилище экстентов, так и универсальный кэш.&nbsp; По мере определения дедуплицированных данных, на основе копий с одинаковыми хэшами, фоновый процесс будет удалять "лишние" дубликаты используя сервис Curator, построенный на базе механизмов MapReduce. Для считываемых данных, выполняется передача данных в Универсальный кэш, который является многоуровневым.&nbsp; Любые последующие запросы к данным, имеющие тот же хэш, будут извлекаться непосредственно из кэша.&nbsp; Подробную информацию о структуре единого кэша смотрите в подразделе "Единый кэш" обзора путей ввода-вывода. </p>  

<div data-type="note" class="note" id="fingerprinted-vdisk-offsets-l9ijcWIATe"><h6>Примечание</h6>
<h5>Fingerprinted vDisk Offsets</h5>
<p>
	Начиная с версии 4.6.1, в ПО отсутствуют какие-либо огранчения на количество файлов (vDisk) которые могут быть дедуплицированы или сжаты.
</p>
<p>
  До версии 4.6.1 это количетсво было увеличено до 24GB благодаря более эффективной работе с метаданными.  
  До версии 4.5 только первые 12GB файла/vDisk могли быть сжаты.  
  Это было сделано, чтобы поддержать небольшой размер метаданных, и предполагалось, что в первых 12Gb может находится ОС - это очень похожие данные.
</p>
</div>

<p>На рисунке ниже изображена схема отношений Elastic Dedupe Engine и путей ввода-вывода:</p>

<figure id="id-lNtAUWIATe"><img alt="EDE I/O Path" class="iimagesv2dedup_2png" src="imagesv2/dedup_2.png">
<figcaption><span class="label">Рисунок 11-17. </span>EDE и пути ввода-вывода</figcaption>
</figure>

<p>Текущий уровень дедупликации можно посмотреть через интерфейс Prism, в разделе Storage.</p>

<div data-type="note" class="note" id="dedup-compression-42ikCZI8Tk"><h6>Примечание</h6>
<h5>Дедупликация + компрессия</h5>

<p>
  Начиная с версии 4.5 и дедупликация и компрессия могут быть включены для одного и того же контейнера.  
  В случае, если данные не могут быть дедуплицированны, продолжайте использовать компрессию.
</p>

</div>

<!-- End of EDE section -->
</section>

</section>

<section data-type="sect1" id="storage-tiering-and-prioritization-rkIqUgTO">
<h3>Распределение уровней хранения</h3>

<p>В главе посвященной распределению нагрузки по дискам, говорилось, что вся емкость распределена между всеми узлами и дисками в кластере Nutanix и 
    ILM будет использоваться для хранения горячих данных локально.&nbsp; Похожая концепция используется для работы с разными уровнями хранения,
    так для переноса данных между уровнями SSD и HDD механизм ILM будет отвечать за управление событиями перемещения данных. 
    Локальный уровень хранения SSD всегда будет иметь более высокий приоритет, для всех операций ввода-вывода ВМ запущенных на конкретном узле, 
    при этом ресурсы всех SSD всего кластера так же доступны для всех узлов.&nbsp; 
    Уровень SSD всегда обеспечивает максимальную производительность и очень важен для гибридных хранилищ.</p>
  
  <p>Приоритеты различных уровней хранения могут быть классифицирована следующим образом:</p>  

<figure id="id-GRtgTgU0T1"><img alt="DSF Tier Prioritization" class="iimagesv2tiering_1png" src="imagesv2/tiering_1.png">
<figcaption><span class="label">Рисунок 11-18. </span>Приоритеты уровней хранения</figcaption>
</figure>

<p>Специфические типы ресурсов (например SSD, HDD) собраны вместе и формируют уровень хранения на уровне всего кластера.&nbsp; 
    Это значит, что любой узел кластера может использовать всю емкость хранилища, в независимости локальная эта емкость или нет.</p>
  
<p>На следующем рисунке показан высокоуровневый пример того, как выглядит общее хранилище:</p>  

<figure class="large" id="id-1ntnF4UdTQ"><img alt="DSF Cluster-wide Tiering" class="iimagesv2tiering_2png" src="imagesv2/tiering_2.png">
<figcaption><span class="label">Рисунок 11-19. </span>DSF Уровень хранилища.</figcaption>
</figure>

<p>Стандартный вопрос - что будет, если локальные SSD будут заполнены?&nbsp; 
    Как и говорилось в разделе о балансировке нагрузке между дисками - ключевая идея состоит в том, чтобы постараться сохранить равномерную нагрузку между всеми дисками/узлами кластера.&nbsp; 
    В случае высокой нагрузки на локальные SSD, механизм балансировки начнет перемещать самые холодные данные на уровень HDD и другие SSD в кластере.&nbsp; 
    Таким образом количество свободного пространства на локальных SSD будет увеличено и позволит продолжить запись на локальные SSD и избежать передачи лишних данных по сети.&nbsp; 
    Ключевой момент - CVM будут использовать локальные SSD пока это возможно, для устранения любых потенциальных узких мест, и станут передавать данные по сети только в крайних случаях.</p>  

<figure class="large" id="id-2btrfvU0T3"><img alt="DSF Cluster-wide Tier Balancing" class="iimagesv2tiering_3png" src="imagesv2/tiering_3.png">
<figcaption><span class="label">Рисунок 11-20. </span>DSF Уровень хранилища - Балансировка нагрузки.</figcaption>
</figure>

<p>Другой случай - когда общий уровень загрузки превышает определенное пороговое значение [curator_tier_usage_ilm_threshold_percent (Default=75)] 
  когда DSF ILM выполнит запуск задания Curator, которое начнет перенос данных с уровня SSD на уровень HDD.&nbsp; 
  Данный процесс будет выполняться каждый раз, чтобы обеспечить нужное количество свободного пространства в соответствии с указанным пороговым значением
  [curator_tier_free_up_percent_by_ilm (Default=15)]. Данные для принудительного переноса на уровень HDD будут выбраны в соответствии с датой/временем последнего доступа к ним.
  В случае, когда уровень SSD загружен на 95%, 20% данных на этом уровне хранения подлежат переносу на уровень HDD (95% –&gt; 75%).&nbsp;</p>

<p>Однако, если уровень загрузки равен 80%, только 15% данных будут перенесены на уровень HDD.</p>


<figure class="large" id="id-lNtzIJUATe"><img alt="DSF Tier ILM" class="iimagesv2tiering_4png" src="imagesv2/tiering_4.png">
<figcaption><span class="label">Рисунок 11-21. </span>DSF балансировка между уровнями хранения</figcaption>
</figure>

<p>DSF ILM постоянно ведет наблюдение за профилем операций ввода-вывода и выполняет перенос данных по мере необходимости.</p>
</section>

<section data-type="sect1" id="disk-balancing-mkIOhPTq">
<h3>Балансировка дисков</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/atbkgrmpVNo">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//atbkgrmpVNo"></iframe></div>

<p>DSF разработан так, чтобы быть очень динамичной платформой, которая может реагировать на разные типы нагрузки и работать с разными типами узлов.
    Чтобы иметь возможность собирать в кластере узлы рассчитанные под высокие вычислительные нагрузки (3050,  и пр.) и рассчитанные на большие объемы хранения (60X0 и т.д.)&nbsp; 
    Обеспечение равномерного распределения данных является важным элементом при смешивании узлов с большими объемами хранения. 
    DSF имеет стандартную функцию, которая называется Балансировка дисков, и используется для равномерного распределения данных между дисками в кластере.&nbsp; 
    Балансировка дисков работает на узлах, с локальными данными на локальных дисках этих узлов и интегрирована с DSF ILM.&nbsp; 
    Цель - сохранять равномерное распределение данных между узлами, и следить за пороговыми значениями.</p>
  
<p>На следующем рисунке показан пример смешанного кластера (3050 + 6050) в "несбалансированном" состоянии:</p>  

<figure class="large" id="id-1ntzHGhdTQ"><img alt="Disk Balancing - Unbalanced State" class="iimagesv2disk_balancing_1png" src="imagesv2/disk_balancing_1.png">
<figcaption><span class="label">Рисунок 11-22. </span>Балансировка дисков - несбалансированное состояние</figcaption>
</figure>

<p>Балансировка дисков использует Curator и запускает процесс балансировки в случае если достигнуто пороговое значение 
    (например, загрузка локальных дисков узла достигла &gt; n %).&nbsp; В случае, если данные не сбалансированы, 
    Curator определит, какие данные должны быть перемещены, и распределит задачу по распределению между узлами кластера. 
    В случае, если кластер является однородным (например только из 3050), использование должно быть достаточно равномерным. 
    Однако, если определенные ВМ имеют больше данных чем остальные, то в распределении емкости между узлами может возникнуть перекос.&nbsp; 
    В этом случае, механизм балансировки запустит процесс перемещения холодных данных этого узла на остальные узлы в кластере. 
    В случае, если кластер неоднородный (например 3050 + 6020/50/70), или если узлы используется в режиме "только хранение", 
    то, скорее всего возникнет необходимость в перемещении данных.</p>
  
  <p>На следующем рисунке показан пример смешанного кластера после выполнения операции балансировки:</p>  

<figure class="large" id="id-3rt1fehlTb"><img alt="Disk Balancing - Balanced State" class="iimagesv2disk_balancing_2png" src="imagesv2/disk_balancing_2.png">
<figcaption><span class="label">Рисунок 11-23. </span>Балансировка дисков - сбалансированное состояние</figcaption>
</figure>

<p>В некоторых случаях, зазкачик может запускать узлы в режиме “storage-only”. В этом случае CVM будет выполнять задачи работы с хранилищем.&nbsp; 
  В этом случае, вся емкость ОЗУ узла будет отдана под большой кэш на чтение.</p>

<p>На следующем рисунке показана схема, в которой выделенный для хранения узел будет выглядеть в смешанном кластере, после балансировки, 
  когда на него перемещаются данные одной из активных ВМ:</p>

<figure class="large" id="id-MltQIBhpT1"><img alt="Disk Balancing - Storage Only Node" class="iimagesv2disk_balancing_3png" src="imagesv2/disk_balancing_3.png">
<figcaption><span class="label">Рисунок 11-24. </span>Балансировка дисков - узел в режиме “storage-only”</figcaption>
</figure>
</section>

<section data-type="sect1" id="snapshots-and-clones-vlInsNTB">
<h3>Снимки и клоны</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/uK5wWR44UYE">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//uK5wWR44UYE"></iframe></div>

<p>DSF имеет встроенную поддержку создания снимков и клонов, которые могут быть сделан посредством  VAAI, ODX, ncli, REST, Prism, и так далее.&nbsp; 
    Снимки и клоны использую алгоритм redirect-on-write, который является наиболее эффективным и действенным. 
    Как пояснялось в главе Структура Данных, ВМ состоит из файлов (vmdk/vhdx) которые именуются vDisk в Платформе Nutanix.&nbsp;</p>
  
  <p>vDisk состоит из эктсентов, которые являются логическими непрерывными последовательностями кусочков данных, 
    которые хранятся в группах экстентов, которые являются физически непрерывными данными, хранящимися в виде файлов на устройствах хранения. 
    Когда снимок или клон сделаны, базовый vDisk помечается как неизменяемый, и для записи/чтения создается другой vDisk.&nbsp; 
    В этот момент, оба vDisks имеют одинаковые карты блоков, которые представляют собой сопоставление метаданных виртуального диска с соответствующими экстентами.
    В отличие от традиционных подходов, использующие цепочки снимков (которые увеличивают задержки), каждый vDisk имеет собственную карту блоков.&nbsp; 
    Это позволяет избежать перерасхода ресурсов при работе с большой глубиной снимков, и позволяют создавать еще снимки без влияния на производительность.</p>
  
  <p>На следующем рисунке показан пример того, как это работает при создании моментального снимка (Примечание: 
    Я должен упомянуть, что использовал в качестве прототипа схемы NTAP, так как они показались мне достаточно детальными и понятными):</p>  

<figure class="large" id="id-3rteF9slTb"><img alt="Example Snapshot Block Map" class="iimagesv2snap_1png" src="imagesv2/snap_1.png">
<figcaption><span class="label">Рисунок 11-32. </span>Пример карты блоков снимка</figcaption>
</figure>

<p>Тот же метод применяется при выполнении моментального снимка или клонирования ранее привязанного или склонированного vDisk:</p>

<figure class="large" id="id-lNt3fNsATe"><img alt="Multi-snap Block Map and New Write" class="iimagesv2snap_2png" src="imagesv2/snap_2.png">
<figcaption><span class="label">Рисунок 11-33. </span>Множественные снимки и их карты блоков, процесс записи</figcaption>
</figure>

<p>Те же методы используются для снимков и/или клонов дисков ВМ или vDisk.&nbsp; 
    Когда ВМ или vDisk клонируются, текущая карта блоков блокируется и создается клон.&nbsp; 
    Эта процедура изменяет метаданные, но никаких процедур ввода-вывода при этом не выполняется.&nbsp; 
    Такой же метод при клонировании клонов; по существу, ранее клонированная виртуальная машина действует как "базовый vDisk" и при клонировании, 
    эта карта блоков заблокирована и созданы два " клона: один для клонируемой виртуальной машины, а другой для нового клона.&nbsp;  
    Максимальное количество клонов не ограничено.</p>
  
  <p>Они оба наследуют предыдущую карту блоков, и любые новые записи/обновления будут происходить на их отдельных картах блоков.</p>
    

<figure id="id-J2tNIms9T9"><img alt="Multi-Clone Block Maps" class="iimagesv2snap_3png" src="imagesv2/snap_3.png">
<figcaption><span class="label">Рисунок 11-34. </span>Карта блоков при множественных клонах</figcaption>
</figure>

<p>Как упоминалось ранее, каждая виртуальная машина/vDisk имеет свою собственную карту блоков.&nbsp; Таким образом, в приведенном выше примере все клоны из базовой виртуальной машины теперь будут владеть своей картой блоков, и любая запись/обновление будет происходить там.&nbsp;</p>

<p>На следующем рисунке показан пример того, как это выглядит:</p>

<figure id="id-OQtmCNsjTJ"><img alt="Clone Block Maps - New Write" class="iimagesv2snap_4png" src="imagesv2/snap_4.png">
<figcaption><span class="label">Рисунок 11-35. </span>Клон карты блоков - Новая запись</figcaption>
</figure>

<p>Любые последующие клоны или моментальные снимки виртуальной машины или виртуального диска приведут к блокировке исходной карты блоков и созданию новой для R/W-доступа.</p>
</section>

<section data-type="sect1" id="networking-and-io-YWI5FoTZ">
<h3>Сеть и ввод-вывод</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/Bz37Eu_TgxY">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//Bz37Eu_TgxY"></iframe></div>

<p>Платформа Nutanix не использует никаких специальных средств для обеспечения коммуникации между узлами, вместо этого используется стандартная сеть 10G.&nbsp; Все операции ввод-вывода ВМ запущенных в рамках кластера обслуживаются гипервизором в выделенной приватной сети.&nbsp; Запрос ввода-вывода будет обрабатываться гипервизором, который передаст запрос к приватному IP на локальной CVM.&nbsp; CVM выполнит репликацию записанных данных с другими узлами Nutanix, используя их внешние IP-адреса в публичной сети 10G. Для всех операций чтения, обработка будет осуществляться локально, и в большинстве случаев никаких дополнительных запросов по сети передаваться не будет. Это значит, что только по сети передается только трафик касающийся репликации данных между узлами кластера.&nbsp; CVM будет передавать запросы на запись другим CVM только в случае, если CVM не работает или локальные копии данных недоступны.&nbsp; Так же все задачи, запускаемые на всем кластере, такие как балансировка дисков будут временно создавать всплески нагрузки на сеть.</p>
<p>На следующем рисунке показан пример взаимодействия пути ввода-вывода виртуальной машины с приватной и общей сетью 10G:</p>
  

<figure class="large" id="id-Q4tBHoF5TX"><img alt="DSF Networking" class="iimagesv2net_iopng" src="imagesv2/net_io.png">
<figcaption><span class="label">Рисунок 11-54. </span>DSF Сеть</figcaption>
</figure>

<p>&nbsp;</p>
<!-- End of network and I/O -->
</section>

<section data-type="sect1" id="data-locality-qeI8t8Td">
<h3>Локальное размещение данных</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/ocLD5nBbUTU">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//ocLD5nBbUTU"></iframe></div>

<p>Для конвергентной платформы, ввод-вывод и локальное размещение данных является критическими для кластера и производительности ВМ.&nbsp; Как было описано в главе Пути ввода-вывода все операции чтения/записи обслуживаются локальной CVM, которая размещена на каждом гипервизоре и обслуживает ВМ на этих узлах.&nbsp; Данные ВМ обслуживаются локально через CVM и размещаются на локальных дисках под управлением CVM.&nbsp; При перемещении виртуальной машины с одного узла гипервизора на другой (например в случае события HA), недавно перенесенные данные ВМ будут обслуживаться так же - локальной CVM. Когда осуществляется чтение старых данных (хранимых теперь на удаленном узле/CVM), все операции ввод-вывода будут перенаправлены локальной CVM на удаленную.&nbsp; Все операции записи будут выполняться локально.&nbsp; DSF определит, что ввод-вывод происходит с другого узла, и перенесет данные локально в фоновом режиме, позволяя всем операциям чтения обслуживаться локально.&nbsp; Данные будут перенесены только при чтении, чтобы не генерировать дополнительной нагрузки на сеть.</p>  

<p>
  Локальное размещение данных имеет два типа:
</p>

<ul>
  <li>
    Локальное размещение кэша
    <ul>
      <li>
        Данные vDisk хранятся локально в Универсальном кэше. При этом экстенты vDisk могут находится на удаленном узле.
      </li>
    </ul>
  </li>
  <li>
    Локальное размещение экстентов
    <ul>
      <li>
        Экстенты vDisk находятся там же, где и сама ВМ
      </li>
    </ul>
  </li>
</ul>

<p>На следующем рисунке показан пример того, как данные будут "следовать" за виртуальной машиной при перемещении между узлами гипервизора:</p>

<figure class="large" id="id-J2trh3t9T9"><img alt="Data Locality" class="iimagesv2data_locality2png" src="imagesv2/data_locality2.png">
<figcaption><span class="label">Рисунок 11-55. </span>Локальное размещение данных</figcaption>
</figure>

<div data-type="note" class="note" id="thresholds-for-data-migration-Oxi4FOtjTJ"><h6>Примечание</h6>
<h5>Пороговое значение для миграции данных</h5>

<p>
  Локализация кэша происходит в режиме реального времени и определяется на основе данных о владении vDisk. Когда vDisk или ВМ перемещается от одного узла к другому, владельцем vDisk станет новая локальная CVM. После передачи прав новому владельцу - данные можно кэшировать локально в Едином кэше. Кэширование будет выполнятся на текущем владельце ВМ (например на новом удаленном узле). Копия сервиса Stargate который ранее работал с этими данными отдает токен vDisk, когда получает удаленные запросы ввода-вывода 300 или более секунд. Данный токен будет получен локальной копией Stargate. Согласованность кэша обеспечивается по мере необходимости, на стороне текущего владельца данных vDisk. 
</p>
  
<p>Локальность экстента является выборочной операцией, и группа экстентов будет перенесена при выполнении следующих действий:
  "3 запроса для случайных операций ввода-вывода, 10 запросов для последовательных в течении 10 минут, при этом множественные операции чтения каждые 10 секунд считаются как один запрос".  
</p>
</div>

<!-- End of data locality -->
</section>

<section data-type="sect1" id="shadow-clones-gnIEf8T1">
<h3>Теневые клоны</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/oqfFDMYQFJg">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//oqfFDMYQFJg"></iframe></div>

<p>Распределенное хранилище данных Платформы имеет функцию, которая называется ‘Shadow Clones’ / 'Теневые клоны', которая позволяет обеспечить распределенное кэширование конкретных vDisk или ВМ, соответствующих сценарию множественного чтения.&nbsp; Хороший пример - когда для VDI используются слинкованные копии (‘linked clones’), и запросы на чтение передаются основному образу или базовой ВМ.&nbsp; В случае использования VMware View, это называется 'replica disk' и его чтение выполняется при работе со всеми слинкованными копиями, в случае с XenDesktop это называется - MCS Master VM.&nbsp; Этот механизм будет так же использоваться для других сценариев множественного чтения данных (например серверы развертывания, репозитории и так далее.). Данные или локализация ввода-вывода является критическими для достижения максимальной производительности ВМ и является ключевой структурой DSF.&nbsp;</p>  

<p>При использовании теневых клонов, DSF будет вести наблюдение за трендами доступа к vDisk, как это делается для локализации данных.&nbsp; Однако, в случае если запросы приходят более чем от двух удаленных CVM (включая локальную CVM), и все эти запросы - запросы на чтение, vDisk будет помечен как неизменяемый.&nbsp; Как только диск будет помечен как неизменяемый, vDisk сможет быть закэширован локально каждой CVM, осуществляющей запросы на чтение (эта копия диска и называется - Теневым клоном). Что позволит направлять все запросы на чтение локально. В случае с VDI, это означает что базовая ВМ будет закеширована на каждом узле и доступ к ней будет осуществляться локально.&nbsp; Примечание:&nbsp; Данные будут перенесены только при чтении, чтобы не забивать сеть и обеспечить эффективное использование кэша.&nbsp; В случае изменения базовой виртуальной машины Теневые копии будут удалены, и процесс начнется заново.&nbsp; Механизм создания теневых копий включен по умолчанию (начиная с версии 4.0.2) и может быть включен/выключен посредством выполнения команды NCLI: ncli cluster edit-params enable-shadow-clones=&lt;true/false&gt;.</p>
    
<p>На следующем рисунке показан пример работы механизма Теневых клонов и возможности распределенного кэширования:</p>

<figure class="large" id="id-BAtyFEfJTv"><img alt="Shadow Clones" class="iimagesv2shadow_clonepng" src="imagesv2/shadow_clone.png">
<figcaption><span class="label">Рисунок 11-56. </span>Теневые клоны</figcaption>
</figure>
<!-- End of shadow clones -->
</section>

<section data-type="sect1" id="storage-layers-and-monitoring-EqIjiATz">
<h3>Уровни хранения и мониторинг</h3>

<p>Платформа Nutanix выполняет мониторинг хранилища на разные уровнях, начиная с ВМ и гостевых ОС, заканчивая физическими устройствами хранения.&nbsp; Знание этих различных уровней и как они связаны между собой очень важно при работе с платформой, и позволяет лучше понять, что происходит в каждый момент времени. На следующем рисунке показаны различные уровни мониторинга операций с хранилищем:</p> 

<figure id="id-Q4tMubi5TX"><img alt="Storage Layers" class="iimagesv2storage_layerspng" src="imagesv2/storage_layers.png">
<figcaption><span class="label">Рисунок 11-57. </span>Уровни хранения</figcaption>
</figure>

<p>&nbsp;</p>

<h5>Уровень виртуальных машин</h5>

<ul>
	<li>Роль: Метрики от гипервизора о виртуальной машине</li>
	<li>Описание: Метрики о ВМ или непосредственно с уровня гостевой ОС, которые напрямую передаются в систему от гипервизора, отображают информацию по производительности ВМ и операций ввода-вывода.</li>
	<li>Когда использовать: При устранении неполадок или поиске сведений об уровне ВМ</li>
</ul>

<h5>Уровень гипервизора</h5>

<ul>
	<li>Роль: Метрики от гипервизора</li>
	<li>Описание: Метрики с уровня гипервизора, передаваемые напрямую с гипервизора. Представляют наиболее точные метрики, отображаемые гипервизорами. &nbsp;Эти данные можно просмотреть для одного или нескольких узлов гипервизора или всего кластера целиком.&nbsp; Этот уровень обеспечит наиболее точные данные с точки зрения производительности платформы и должен быть использован в большинстве случаев.&nbsp; В некоторых сценариях гипервизор может объединять или разделять операции, поступающие от виртуальных машин, что может показать разницу в метриках, сообщаемых виртуальной машиной и гипервизором.&nbsp; Эти цифры также будут включать в себя хиты кэша, обслуживаемых Nutanix CVMs.</li>
	<li>Когда использовать: В большинстве случаев, так как это обеспечит наиболее подробные и ценные показатели.</li>
</ul>

<h5>Уровень контроллера</h5>

<ul>
	<li>Роль: Метрики от CVM</li>
	<li>Описание: Метрики уровня контроллеров которые поступают напрямую от CVM (например,&nbsp; страница Stargate 2009). Предоставляют информацию о фронт-энд операция NFS/SMB/iSCSI и любых бек-энд операциях (например, ILM, балансировка дисков, и пр.).&nbsp; Эти данные можно просмотреть для одной или нескольких виртуальных машин контроллера или кластера целиком.&nbsp; Метрики, которые видит уровень контроллера, обычно должны совпадать с метриками, которые видит уровень гипервизора, однако они будут включать любые бэк-энд операции . Эти цифры также включают и хиты кэша обслуживаемого в памяти.&nbsp; В некоторых случаях метрики (IOPS) могут не совпадать, так как клиент NFS / SMB / iSCSI может разделить большой ввод-вывод на несколько меньших операций ввода-вывода.&nbsp; Однако такие показатели, как пропускная способность, должны совпадать.</li>
	<li>Когда использовать: Подобно слою гипервизора, может использоваться, чтобы показать, сколько и каких бэк-энд операций происходит.</li>
</ul>

<h5>Уровень диска</h5>

<ul>
    <li>Роль: метрики от физических устройств хранения</li>
    <li>Описание: Метрики уровня диска извлекаются непосредственно из физических дисковых устройств (через CVM) и представляют то, что видит серверная часть.&nbsp; Они включают в себя данные, попадающие в OpLog или хранилище экстентов. Все операции, при которых выполняются операции ввода-вывода на диски.&nbsp; Эти данные можно просмотреть для одного из нескольких дисков, дисков для определенного узла или для всех дисков в кластере.&nbsp; В обычных случаях предполагается, что операции на диске должны соответствовать числу входящих операций записи, а также операций чтения, не выполняемых из кэша.&nbsp; Все операции чтения обсуживаемые кэшем не будут учтены, так как они не обслуживаются дисковыми устройствами.</li>
    <li>Когда использовать: Когда нужно понять, как много операций ввода-вывода обслуживаются дисками.</li>  
</ul>

<div data-type="note" class="note" id="metric-and-stat-retention-yGiRUBioTz"><h6>Примечание</h6>
<h5>Метрики и хранение статистики</h5>

<p>Метрики и данные по ним хранятся локально в течение 90 дней в Prism Element.  Для Prism Central и Insights, данные могут храниться бесконечно долго (это зависит от доступного пространства).</p>
</div>
<!-- End of storage layers and monitoring -->
</section>

<!--END of Acroplis DSF Section -->
</section>

<section data-type="chapter">
<h2>Сервисы</h2>

<section data-type="sect1">
<h3>Гостевые утилиты (NGT)</h3>
<p>
  Гостевые утилиты Nutanix (NGT) это гостевое ПО для размещения на уровне гостевых ОС, которое позволяет получить возможности расширенного управления ВМ через платформу Nutanix.
</p>

<p>
  Решение поставляется в виде инсталлятора NGT который нужно установить на уровне гостевых ОС и фреймфорка Guest Tools который обеспечивает взаимодействие гостевых утилит и платформы.
</p>

<p>
  Установщик NGT состоит из следующих компонент:
</p>

<ul>
  <li>
    Сервис агентского ПО
  </li>
  <li>
    Сервис самостоятельного восстановления (SSR) или восстановления на уровне файлов (FLR) CLI
  </li>
  <li>
    Драйверы мобильности - драйверы VirtIO для AHV
  </li>
  <li>
    Агент VSS и драйверы для ВМ с Windows
  </li>
  <li>
    Поддержка консистентных снимков на уровне приложений для Linux
  </li>
</ul>

<p>
    Эта структура состоит из нескольких высокоуровневых компонентов:
</p>

<ul>
  <li>
      Сервис взаимодействия с Гостевыми агентами
    <ul>
      <li>
        Шлюз между ПО плафтормы Nutanix и Гостевым агентом.  
        ПО распределено между всеми CVM, в качестве мастера выбирается экземпляр, где на текущий момент находится VIP кластера.
      </li>
    </ul>
  </li>
  <li>
      Гостевой агент
    <ul>
      <li>
        ПО размещаемое внутри гостевой ОС виртуальной машины.
        Управляет локальными задачами, такими как VSS или SSR, взаимодействует с Сервисом Гостевых Инструментов
      </li>
    </ul>
  </li>
</ul>

<p>
    На рисунке показана высокоуровневая схема размещения компонент:
</p>
<figure><img alt="Guest Tools Mapping" src="imagesv2/ngt_1.png">
<figcaption><span class="label">Рисунок. </span>Размещение компонент взаимодействия с гостевыми ОС</figcaption>
</figure>

<h5>Гостевые утилиты</h5>
<p>
  Сервис взаимодействия с Гостевым агентом (Guest Tools Service) состоит из двух основных компонент:
</p>

<ul>
  <li>
    NGT-мастер
  <ul>
    <li>
      Обслуживает запросы приходящие от NGT-прокси и интерфесов ПО Acropolis.
      NGT-мастер выбирается динамически среди узлов кластера, в случае отказа текущего мастера - выбирается новый.
      Сервис слушает внутренний порт 2073.
    </li>
  </ul>
  </li>
  <li>
    NGT-прокси
  <ul>
    <li>
      Запускается на всех CVM и перенаправляет запросы к NGT-мастеру для выполнения желаемых действий.
      Текущая CVM работающая в режиме Prism Leader (там, где VIP) будет обрабатывать все запросы от Гостевых утилит.
      Сервис слушает внешний порт 2074.
    </li>
  </ul>
  </li>
</ul>

<div data-type="note" class="note" id="metric-and-stat-retention-yGiRUBioTz"><h6>Примечание</h6>
<h5>Текущий NGT-мастер</h5>

<p>Вы можете определить IP-адрес CVM, на которой расположен сервис NGT-мастер, выполнив следующую команду на любой CVM:</p>

<p class="codetext">
  nutanix_guest_tools_cli get_master_location
</p>
</div>

<p>
  Размещение ролей показано на изображении ниже:
</p>
<figure><img alt="Guest Tools Service" src="imagesv2/ngt_2.png">
<figcaption><span class="label">Рисунок. </span>Сервис взаимодействия с Агентским ПО</figcaption>
</figure>

<h5>Гостевой агент</h5>
<p>
  Гостевой агент состоит из следующих высокоуровневых компонентов, как упоминалось выше:
</p>

<figure><img alt="Guest Agent" src="imagesv2/ngt_3.png">
<figcaption><span class="label">Рисунок. </span>Гостевой агент</figcaption>
</figure>



<h5>Связь и безопасность</h5>
<p>Взаимодействие Гостевых агентов и Сервиса взаимодействия с Гостевыми агентами осуществляется через IP-адрес кластера с использованием SSL. 
    Для инсталляций, где компоненты кластера Nutanix и пользовательские ВМ находятся в разных сетях, убедитесь, что возможно следующее:</p> 
<ul>
  <li>
    Убедитесь, что работает маршрутизация из сетей ВМ к адресу кластера
  </li>
  или
  <li>
    Создайте правило межсетевого экрана (NAT), обеспечивающее доступ для клиентских ВМ к IP-адресу кластера на порт 2074.
  </li>
</ul>

<p>
  Сервис взаимодействия с Гостевыми агентами выступает в роли удостоверяющего центра (CA) и отвечает за создание пар сертификатов для каждой ВМ с установленным Гостевым агентом. Сертификат интегрируется в образ ISO, и подключается к ВМ. Далее он используется в процессе инсталляции Гостевого агента.  
</p>

<h5>Установка NGT-агента</h5>
<p>
  Установка NGT-агента может быть выпролнена через Prism или CLI/Scripts (ncli/REST/PowerShell).
</p>

<p>
  Для инсталляции через Prism, нужно перейти на страницу 'VM', выбрать нужную ВМ и нажать кнопку 'Enable NGT':
</p>
<figure><img alt="Enable NGT for VM" src="imagesv2/Prism/ngt/ngt_deploy_1.png">
<figcaption><span class="label">Рисунок. </span>Установка NGT-агента</figcaption>
</figure>

<p>
  Нажмите 'Yes' для начала инсталляции NGT:
</p>
<figure><img alt="Enable NGT Prompt" src="imagesv2/Prism/ngt/ngt_deploy_2.png">
<figcaption><span class="label">Рисунок. </span>Установка NGT-агента</figcaption>
</figure>

<p>
  ВМ должна иметь CD-ROM, куда может быть подключен сгенерированный инсталлятором ISO образ, содержащий уникальные сертификаты:
</p>
<figure><img alt="Enable NGT - CD-ROM" src="imagesv2/Prism/ngt/ngt_deploy_3a.png">
<figcaption><span class="label">Рисунок. </span>Установка NGT-агента - CD-ROM</figcaption>
</figure>

<p>
  Данный ISO будет виден на уровне гостевой ОС:
</p>
<figure><img alt="Enable NGT - CD-ROM in OS" src="imagesv2/Prism/ngt/ngt_deploy_3b.png">
<figcaption><span class="label">Рисунок. </span>Установка NGT-агента - CD-ROM in OS</figcaption>
</figure>

<p>
  Нажмите дважды на CD, чтобы начать установку ПО.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Фоновая инсталляция</h5>
<p>
  Вы можете выполнить фоновую инсталляцию запустив следующую команду (из точки монтирования CD-ROM):
</p>

<p class="codetext">
  NutanixGuestTools.exe /quiet /l log.txt ACCEPTEULA=yes
</p>
</div>

<p>
  Следуйте по шагам мастера инсталляции и примите лицензионное соглашение:
</p>
<figure><img alt="Enable NGT - Installer" src="imagesv2/Prism/ngt/ngt_deploy_4.png">
<figcaption><span class="label">Рисунок. </span>Установка NGT-агента - Установщик</figcaption>
</figure>

<p>
  Как часть ПО будут так же установлены Python, PyWin и драйверы Nutanix Mobility.
</p>

<p>
  После завершения установки требуется выполнить перезагрузку.
</p>

<p>
  После успешной установки и перезагрузки, вы увидите следующие элементы в 'Programs and Features':
</p>
<figure><img alt="Enable NGT - Installed Programs" src="imagesv2/Prism/ngt/ngt_deploy_5a.png">
<figcaption><span class="label">Рисунок. </span>Установка NGT-агента - Установленное ПО</figcaption>
</figure>

<p>
  Сервис отвечающий за NGT-агента и VSS Hardware Provider будут так же запущены:
</p>
<figure><img alt="Enable NGT - Services" src="imagesv2/Prism/ngt/ngt_deploy_5b.png">
<figcaption><span class="label">Рисунок. </span>Enabled NGT - Сервисы</figcaption>
</figure>

<p>
  NGT установлен и может быть использован.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Массовая установка NGT</h5>

<p>Вместо установки NGT для каждой машины отдельно, можно выполнить интеграцию ПО прямо в базовый образ ВМ.</p>

<p>
 Выполните следующие действия для установки NGT в базовый образ ВМ:
</p>
<ol>
  <li>
    Установите NGT в базовую ВМ и обеспечьте связь
  </li>
  <li>
    Склонируйте новую ВМ из базовой ВМ
  </li>
  <li>
    Смонтируйте ISO образ NGT к каждому клону (требуется для генерации пар сертификатов)
    <ul>
      <li>
        Например: ncli ngt mount vm-id=&lt;CLONE_ID&gt; или через Prism
      </li>
    </ul>
    <ul>
      <li>
        Скоро мы предоставим автоматизированный способ инсталляции :)
      </li>
    </ul>
  </li>
  <li>
    Включите клоны ВМ
  </li>
</ol>

<p>
  Когда склонированная виртуальная машина будет загружена, она найдет ISO NGT и скопирует соответствующие файлы конфигурации и новые сертификаты, 
  а затем начнет взаимодействовать с Сервисом взаимодействия с Гостевыми агентами.
</p>
</div>
<!-- end of ngt section -->
</section>
<section data-type="sect1" id="volumes-api-wrIRHzTv">
<h3>Настройка параметров OS</h3>
<p>
  Nutanix поддерживает настройку параметров ОС через утилиты CloudInit и Sysprep. CloudInit используется для первичной настройки серверов с Linux при старте ВМ. 
  Sysprep используется для первоначальной настройки ОС Windows.
</p>

<p>
  Некоторые типичные области применения:
</p>
<ul>
  <li>
    Настройка имени узла
  </li>
  <li>
    Установка пакетов
  </li>
  <li>
    Добавление пользователей / управление ключами
  </li>
  <li>
    Запуск пользовательских скриптов автоматизации
  </li>
</ul>

<h5>Поддерживаемые конфигурации</h5>

<p>
  Решение применимо к гостевым ОС семейства Linux , включая версии перечисленные ниже: 
</p>

<ul>
  <li>
    Гипервизоры:
    <ul>
      <li>
        AHV
      </li>
    </ul>
  </li>
  <li>
    Операционные системы:
    <ul>
      <li>
        Linux - Большинство современных дистрибутивов
      </li>
    </ul>
    <ul>
      <li>
        Windows - Большинство современных дистрибутивов
      </li>
    </ul>
  </li>
</ul>

<h5>Пререквизиты</h5>

<p>
  Для использования CloudInit необходимо:
</p>

<ul>
  <li>
    Пакет CloudInit должен быть установлен в гостевую ОС
  </li>
</ul>

<p>
  Sysprep доступен в Windows по умолчанию
</p>

<h5>Установка пакета</h5>

<p>
  CloudInit может быть установлен следующим образом:
</p>

<p>
  Для ОС на базе Red Hat (CentOS, RHEL)
</p>

<p class="codetext">yum -y install CloudInit</p>

<p>
  Для ОС на базе Debian (Ubuntu)
</p>

<p class="codetext">apt-get -y update; apt-get -y install CloudInit</p>

<p>
  Sysprep является частью базовой инсталляции ОС Windows.
</p>


<h5>Настройка образа</h5>

<p>
  Добавление пользовательского скрипта настройки ОС может быть выполнено через Prism или REST API.
  Эта опция становится доступной при создании или клонировании ВМ:
</p>

<figure><img alt="Custom Script - Input Options" src="imagesv2/Prism/customscript.png">
<figcaption><span class="label">Рисунок. </span>Пользовательские скрипты - опции настройки</figcaption>
</figure>

<p>
  Путь к пользовательскому скрипту может быть указан несколькими способами:
</p>

<ul>
  <li>
    Путь на хранилище ADSF
    <ul>
      <li>
        Использование ранее загруженного файла
      </li>
    </ul>
  </li>
  <li>
    Загрузка файла
    <ul>
      <li>
        Загрузка файла, который будет выполнен
      </li>
    </ul>
  </li>
  <li>
    Набор или вставка текста скрипта
    <ul>
      <li>
        Скрипт CloudInit или содержимое Unattend.xml
      </li>
    </ul>
  </li>
</ul>

<p>
  Nutanix передает пользовательский скрипт в CloudInit или Sysprep, он будет выполнен при первом старте ВМ. Для этого формируется ISO с данным скриптом, которой в последующем подключается в гостевой ОС. После выполнения скрипта данный ISO будет отключен.  
</p>

<h5>Форматирование ввода</h5>

<p>
    Платформа поддерживает широкий перечень форматов для пользовательских сценариев автоматизации. Давайте перечислим некоторые варианты.
</p>

<h6>Пользовательский скрипт (CloudInit - Linux)</h6>
<p>
  Пользовательский скрипт который будет выполняться в конце процесса загрузки ОС (а-ля "rc.local-like").
</p>

<p>
  Скрипт будет начинаться как любой сценарий bash: "#!".
</p>

<p>
  Ниже приведен пример простого скрипта:
</p>
<p class="codetext">#!/bin/bash </br>
touch /tmp/fooTest</br>
mkdir /tmp/barFolder</p>

<h6>Подключаемый файл (CloudInit - Linux)</h6>
<p>
  Подключаемый файл содержит URL адреса файлов. Каждый доступный файл будет выполнен так же, как это делается для Пользовательских скриптов.
</p>

<p>
  Скрипт будет начинаться с: "#include".
</p>

<p>
  Ниже показан пример такого скрипта:
</p>

<p class="codetext">
#include </br>
http://s3.amazonaws.com/path/to/script/1</br>
http://s3.amazonaws.com/path/to/script/2</p>

<h6>Сценарий cloud-config (CloudInit - Linux)</h6>
<p>
  Сценарии cloud-config наиболее распространенный тип сценариев, специфичный для утилиты CloudInit.
</p>

<p>
  Скрипт должен начинаться с: "#cloud-config"
</p>

<p>
  Ниже показан пример такого скрипта:
</p>

<pre>
<p class="codetext">
#cloud-config

# Установить имя ВМ
hostname: foobar

# Добавить пользователей
users:
  - name: nutanix
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    ssh-authorized-keys:
      - ssh-rsa: &lt;PUB KEY&gt;
    lock-passwd: false
    passwd: &lt;PASSWORD&gt;

# Автоматически обновить все пакеты
package_upgrade: true
package_reboot_if_required: true

# Инсталляция LAMP
packages:
  - httpd
  - mariadb-server
  - php
  - php-pear
  - php-mysql

# Затем выполнить команду
runcmd:
  - systemctl enable httpd</p></pre>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Проверка выполнения CloudInit</h5>
<p>
  Журнал CloudInit находится тут /var/log/cloud-init.log and cloud-init-output.log.
</p></div>


<h6>Unattend.xml (Sysprep - Windows)</h6>
<p>
  Файл unattend.xml который использует Sysprep для настройки образа при первом старте, подробнее можно почитать : <a href="https://technet.microsoft.com/en-us/library/cc721940(v=ws.10).aspx">тут</a>
</p>

<p>
  Скрипт будет начинаться с: "&lt;?xml version="1.0" ?&gt;".
</p>

<p>
  Пример содержания файла unattend.xml:
</p>
<pre>
<p class="codetext">
&lt;?xml version="1.0" ?&gt;
&lt;unattend xmlns="urn:schemas-microsoft-com:unattend"&gt;
   &lt;settings pass="windowsPE"&gt;
      &lt;component name="Microsoft-Windows-Setup" publicKeyToken="31bf3856ad364e35"
language="neutral" versionScope="nonSxS" processorArchitecture="x86"&gt;
         &lt;WindowsDeploymentServices&gt;
            &lt;Login&gt;
               &lt;WillShowUI&gt;OnError&lt;/WillShowUI&gt;
               &lt;Credentials&gt;
                     &lt;Username&gt;username&lt;/Username&gt;
                     &lt;Domain&gt;Fabrikam.com&lt;/Domain&gt;
                     &lt;Password&gt;my_password&lt;/Password&gt;
                  &lt;/Credentials&gt;
               &lt;/Login&gt;
            &lt;ImageSelection&gt;
               &lt;WillShowUI&gt;OnError&lt;/WillShowUI&gt;
               &lt;InstallImage&gt;
                  &lt;ImageName&gt;Windows Vista with Office&lt;/ImageName&gt;
                  &lt;ImageGroup&gt;ImageGroup1&lt;/ImageGroup&gt;
                  &lt;Filename&gt;Install.wim&lt;/Filename&gt;
               &lt;/InstallImage&gt;
                  &lt;InstallTo&gt;
                  &lt;DiskID&gt;0&lt;/DiskID&gt;
                  &lt;PartitionID&gt;1&lt;/PartitionID&gt;
               &lt;/InstallTo&gt;
            &lt;/ImageSelection&gt;
         &lt;/WindowsDeploymentServices&gt;
         &lt;DiskConfiguration&gt;
            &lt;WillShowUI&gt;OnError&lt;/WillShowUI&gt;
               &lt;Disk&gt;
                  &lt;DiskID&gt;0&lt;/DiskID&gt;
                  &lt;WillWipeDisk&gt;false&lt;/WillWipeDisk&gt;
                  &lt;ModifyPartitions&gt;
                     &lt;ModifyPartition&gt;
                        &lt;Order&gt;1&lt;/Order&gt;
                        &lt;PartitionID&gt;1&lt;/PartitionID&gt;
                        &lt;Letter&gt;C&lt;/Letter&gt;
                        &lt;Label&gt;TestOS&lt;/Label&gt;
                        &lt;Format&gt;NTFS&lt;/Format&gt;
                        &lt;Active&gt;true&lt;/Active&gt;
                        &lt;Extend&gt;false&lt;/Extend&gt;
                     &lt;/ModifyPartition&gt;
                  &lt;/ModifyPartitions&gt;
            &lt;/Disk&gt;
         &lt;/DiskConfiguration&gt;
      &lt;/component&gt;
      &lt;component name="Microsoft-Windows-International-Core-WinPE" publicKeyToken="31bf3856ad364e35"
language="neutral" versionScope="nonSxS" processorArchitecture="x86"&gt;
         &lt;SetupUILanguage&gt;
            &lt;WillShowUI&gt;OnError&lt;/WillShowUI&gt;
            &lt;UILanguage&gt;en-US&lt;/UILanguage&gt;
         &lt;/SetupUILanguage&gt;
         &lt;UILanguage&gt;en-US&lt;/UILanguage&gt;
      &lt;/component&gt;
   &lt;/settings&gt;
&lt;/unattend&gt;
</p>
</pre>

<!-- end of cloud init section -->
</section>

<section data-type="sect1" id="volumes-api-wrIRHzTv">
<h3>Блочные сервисы</h3>

<p>Функция платформы - Acropolis Block Services (ABS) предоставляет хранилище внешним клиентам (гостевым ОС, физическим узлам, контейнерам, и так далее.) через iSCSI.</p>

<p>Это позволяет любой ОС получить доступ к хранилищу и воспользоваться его преимуществами.&nbsp; 
  При таком использовании хранилища взаимодействие с ним происходит напрямую, без использования гипервизора.&nbsp;</p>

<p>Основные варианты использования ABS:</p>

<ul>
	<li>Общие разделы
	<ul>
		<li>Oracle RAC, Microsoft Failover Clustering и т.д.</li>
	</ul>
	</li>
	<li>Диски как таковые
	<ul>
		<li>Когда данные являются критичными, а контекст (ВМ/контейнер) не является постоянными</li>
		<li>Контейнеры, OpenStack, и так далее.</li>
	</ul>
	</li>
	<li>Предоставление iSCSI
	<ul>
		<li>Аппаратные узлы</li>
		<li>Развернутый на vSphere продукт Exchange (для поддержки компанией Microsoft)</li>
	</ul>
	</li>
</ul>

<h5>Поддерживаемые ОС</h5>

<p>
  Решение соответствует спецификации iSCSI, поддерживаемыми являются  те ОС, которые были проверены QA.
</p>

<ul>
  <li>
    Microsoft Windows Server 2008 R2, 2012 R2
  </li>
  <li>
    Redhat Enterprise Linux 6.0+
  </li>
</ul>

<h5>Компоненты Блочных сервисов</h5>

<p>Следующие компоненты представляют собой Acropolis Block Services:</p>

<ul>
  <li><strong>Data Services IP:</strong>IP-адрес используемый для подключения iSCSI (Начиная с версии 4.7)</li>
	<li><strong>Volume Group:</strong> Группа томов для которых может выполняться централизованное управление, создание снимков, и применение политик</li>
	<li><strong>Disk(s):</strong> Устройства собираемые в группы (видны как LUN для iSCSI-таргета)</li>
	<li><strong>Attachment:</strong> Обеспечивает привязку и доступ для конкретного IQN к группе томов</li>
  <li><strong>Secret(s):</strong> Используется для аутентификации CHAP/Mutual CHAP</li>
</ul>

<p>Примечание: Группы томов - не что иное как обычный vDisk на РХД.</p>

<h5>Пререквизиты</h5>
<p>
  Прежде чем заниматься конфигурацией мы должны настроить IP-адрес, через который будет осуществляться подключение томов и аутентификация iSCSI-таргета.
</p>

<p>
  Данный адрес настраивается на странице 'Cluster Details' (Иконка с шестеренкой -> Cluster Details):
</p>

<figure><img alt="Block Services - Data Services IP" src="imagesv2/Prism/blockservices/abs_0.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - IP-адрес сервиса</figcaption>
</figure>

<p>
  Так же это можно настроить и через NCLI / API:
</p>

<p class="codetext">ncli cluster edit-params external-data-
services-ip-address=&lt;DATA SERVICES IP ADDRESS&gt;</p>


<h5>Создание iSCSI-таргета</h5>

<p>Для использования Блочных сервисов, первое, что мы сделаем, это создадим "группу томов", которая является iSCSI-таргетом.</p>

<p>
На странице 'Storage' нажмите на кнопку '+ Volume Group' в правом углу:
</p>
<figure><img alt="Block Services - Add Volume Group" src="imagesv2/Prism/blockservices/abs_1.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Создание группы томов</figcaption>
</figure>

<p>
  Откроется меню для детальной настройки группы томов:
</p>
<figure><img alt="Block Services - Add VG Details" src="imagesv2/Prism/blockservices/abs_2.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Создание группы томов</figcaption>
</figure>

<p>
  Далее нужно нажать на кнопку '+ Add new disk' для добавления дисков к iSCSI-таргету (будут видны как тома - LUN:
</p>

<p>
  В появившемся меню нужно выбрать контейнер, где будет расположен создаваемый том и указать размер тома/диска:
</p>
<figure><img alt="Block Services - Add Disk" src="imagesv2/Prism/blockservices/abs_3.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Создание тома/диска</figcaption>
</figure>

<p>
  Далее - нажать 'Add' и повторить действия столько раз, сколько дисков нужно создать.
</p>

<p>
  После того, как мы указали детали и добавили диски, мы подключим группу томов к виртуальной машине или инициатору IQN.  
  Это позволит дать ВМ доступ к iSCSI-таргету (запросы от неизвестных инициаторов будут отклонены):
</p>
<figure><img alt="Block Services - Add Initiator IQN / VM" src="imagesv2/Prism/blockservices/abs_4.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - IQN инициатор / ВМ</figcaption>
</figure>

<p>
  Нажмите 'Save'. Настройка группы томов закончена!
</p>

<p>
  Все эти действия могут быть выполнены через ACLI / API:
</p>
<p># Создание группы томов</p>

<p class="codetext">vg.create &lt;VG Name&gt;</p>

<p># Добавление дисков в группу томов</p>

<p class="codetext">Vg.disk_create &lt;VG Name&gt; container=&lt;CTR Name&gt; create_size=&lt;Disk size, e.g. 500G&gt;</p>

<p># Подключение IQN инициатора к группе томов</p>

<p class="codetext">Vg.attach_external &lt;VG Name&gt; &lt;Initiator IQN&gt;</p>
</div>

<h5>Высокая доступность (HA)</h5>
<p>
  Как говорилось ранее, адрес сервиса iSCSI (Data Services IP) используется для обнаружения.
  Это позволяет использовать единственный адрес, вместо того, чтобы указывать адрес какой-то отдельной CVM.
</p>
<p>
  IP адрес Блочных сервисов будет назначен текущему мастеру iSCSI. Это позволяет убедиться, что портал обнаружения всегда доступен.
</p>

<p>
  При конфигурации iSCSI-инициатора указывается IP-адрес сервиса iSCSI (Data Services IP).
  По запросу на подключение платформа выполнит перенаправление запроса iSCSI на активный сервис Stargate.
</p>
<figure><img alt="Block Services - Login Redirect" src="imagesv2/abs_login_1.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Перенаправление запроса</figcaption>
</figure>

<p>
  В случае, когда активный сервис Stargate падает, инициатор пытается снова осуществить подключение к iSCSI через Data Services IP,
  и его зарос будет перенаправлен к рабочей копии Stargate.
</p>
<figure><img alt="Block Services - Failure Handling" src="imagesv2/abs_login_2.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Обработка отказов</figcaption>
</figure>

<p>
  Если ранее упавший сервис Stargate восстанавливается, текущий активный Stargate заморозит I/O и сбросит активные сессии iSCSI.
  Когда инициатор снова попробует подключиться к iSCSI-таргету, запрос будет перенаправлен к восстановленному сервису Stargate.
</p>
<figure><img alt="Block Services - Failback" src="imagesv2/abs_login_3.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Восстановление</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Мониторинг работоспособности и значения по умолчанию</h5>

<p>При работе с Блочными сервисами состояние Stargate определяется сервисом Zookeeper, т.е. по тому же механизму что и для РХД в целом.</p>

<p>
  Стандартный интервал для восстановления - 120 секунд. Это значит, что если восстановленный сервис Stargate проработает более 2х минут - платформа выполнит заморозку I/O и переключит клиентов iSCSI.
  Как это описано чуть выше.
</p>
</div>

<p>
  Используя этот механизм можно исключить необходимость настраивать на клиенте MPIO для обеспечения HA.
  Когда инициатор подключается к iSCSI-таргету можно не ставить галку 'Enable multi-path':
</p>
<figure><img alt="Block Services - No MPIO" src="imagesv2/abs_iscsi_connect.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - без MPIO</figcaption>
</figure>

<h5>Multi-Pathing</h5>
<p>
  Протокол iSCSI подразумевает одну сессию iSCSI между iSCSI-инициатором и iSCSI-таргетом. Это значит, что мы имеем отношения один-к-одному между iSCSI-таргетом и Stargate.
</p>

<p>
  Начиная с версии ПО 4.7, по умолчанию 32 виртуальных iSCSI-таргета будут автоматически созданы для каждого инициатора и присвоены для каждого диска добавленного в группу томов.
  Таким образом обеспечивается iSCSI-таргетом на каждый диск. Ранее создавалось несколько групп томов с одним диском каждая.
</p>
<p>
  Если посмотреть в детальные настройки группы томов через ACLI/API можно увидеть параметр определяющий 32 виртуальных таргета для каждого подключения:
</p>

<pre>
<p class="codetext">
  attachment_list {
      external_initiator_name: "iqn.1991-05.com.microsoft:desktop-foo"
      target_params {
        num_virtual_targets: 32
      }
    }
</p>
</pre>

<p>
  Для примера создадим группу томов содержащую в себе три диска. 
  При выполнении обнаружения на стороне клиента видно отдельного таргета для каждого диска (суффикс в формате '-tgt[int]'):
</p>
<figure><img alt="Block Services - Virtual Target" src="imagesv2/abs_iscsi_target.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Виртуальный iSCSI-таргет</figcaption>
</figure>

<p>
  Так для каждого диска создается собственная сессия iSCSI и появляется возможность распределять сессии iSCSI между разными сервисами Stargates, увеличивая масштабируемость и производительность:
</p>
<figure><img alt="Block Services - Multi-Path" src="imagesv2/abs_path_2.png">
<figcaption><span class="label">Рисунок. </span>Блочные сервисы - Multi-Path</figcaption>
</figure>
<p>
    Балансировка нагрузки происходит во время установки сеанса iSCSI (вход в систему iSCSI) для каждого таргета.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Активные пути</h5>
<p>
  Вы можете посмотреть активные сервисы Stargate на которых находятся виртуальные iSCSI-таргеты выполнив команду:
</p>

<p class="codetext">
  # Windows <br />
  Get-NetTCPConnection -State Established -RemotePort 3205 <br />
  <br />
  # Linux <br />
  iscsiadm -m session -P 1
</p>
</div>

<p>
  С версии 4.7 используется простая хэш-функция для распределения таргетов между узлами кластера.
  В версии 5.0 эта функция интергирована в Acropolis Dynamic Scheduler, который выполняет ребалансировку сессий, если это необходимо.
  Мы продолжим поиски алгоритмов, которые позволят нам добиться больше оптимизации процессов. 
  Также можно установить предпочтительный узел, который будет использоваться, пока он находится в работоспособном состоянии.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>SCSI UNMAP (TRIM)</h5>
<p>
  ABS поддерживает команды SCSI UNMAP (TRIM), согласно спецификации T10. Эта команда используется для освобождения емкости, при удалении блоков.
</p>
</div>

<!-- End of vol api section -->
</section>

<section data-type="sect1">
<h3>Файловые сервисы</h3>
<p>Файловые сервисы позволяют пользователю использовать платформу Nutanix в качестве отказоустойчивого файлового сервера.  
  Предоставляется доступ в единое пространство имен, где пользователи могут размещать свои домашние каталоги и файлы.</p>

<h5>Поддерживаемые конфигурации</h5>

<p>
  Данные сервисы могут работать на платформах следующим конфигурациям:
</p>
<ul>
  Гипервизоры:
  <li>
    AHV
  </li>
  <li>
    ESXi (Asterix и далее)
  </li>
</ul>
<ul>
    Файловые протоколы:
  <li>
    CIFS 2.1
  </li>
</ul>
<ul>
    Совместимые функции:
  <li>
    Async-DR
  </li>
</ul>

<h5>Компоненты файловых сервисов</h5>

<p>Функция состоит из следующих высокоуровневых компонент:</p>

<ul>
  <li>
    Файловый сервер
    <ul>
      <li>
        Высокоуровневое пространство имен. Каждый файловый сервер будем иметь свой набор File Services VMs (FSVM)
      </li>
    </ul>
  </li>
  <li>
    Общая папка
    <ul>
      <li>
        Общая папка доступна пользователям. Каждый файловый сервер может иметь по несколько таких папок (e.g. например - общая папка отдела)
      </li>
    </ul>
  </li>
  <li>
    Папка
    <ul>
      <li>
        Папка для хранения файлов. Папки являются общими для FSVM
      </li>
    </ul>
  </li>
</ul>

<p>
    На рисунке показана высокоуровневая схема данного сервиса:
</p>
<figure><img alt="File Services Mapping" src="imagesv2/fs_1.png">
<figcaption><span class="label">Рисунок. </span>Файловые сервисы</figcaption>
</figure>

<p>
  Файловые сервисы следуют такой же методологии распределенности как и вся платформа Nutanix, для обеспечения отказоуствойчивости и масштабирования.
  Минимум 3 FSVM будут запущены как часть файловых сервисов.
</p>

<p>
    На рисунке показана подробная схема компонентов данного сервиса:
</p>
<figure><img alt="File Services Detail" src="imagesv2/fs_2.png">
<figcaption><span class="label">Рисунок. </span>Файловые сервисы в деталях</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Поддерживаемые протоколы</h5>

<p>С версии 4.6, SMB (до версии 2.1) единственный поддерживаемый протокол для взаимодействия клиентов и сервера.</p>
</div>

<p>
  ВМ обеспечивающие файловые сервисы запускаются в процессе конфигурации сервисов.
</p>

<p>
  На рисунке показана схема размещения FSVMs на платформе:
</p>
<figure><img alt="File Services Detail" src="imagesv2/fs_2b.png">
<figcaption><span class="label">Рисунок. </span>Архитектура размещения FSVM</figcaption>
</figure>

<h5>Аутентификация и авторизация</h5>
<p>
  Файловые сервисы полностью интегрированы с  Microsoft Active Directory (AD) и DNS.
  Это позволяет использовать все возможности по аутентификации и авторизации присущие AD.
  Все настройки разрешений на файлы, управление пользователями или группами выполняется с использованием Windows MMC.
</p>

<p>
  Следующие объекты AD/DNS будут созданы в процессе инсталляции: 
</p>

<ul>
  <li>
    Аккаунт компьютера для AD для файлового сервера
  </li>
  <li>
    AD SPN для файлового сервера и каждой FSVM
  </li>
  <li>
    DNS запись для файлового сервера указывающая на все FSVM
  </li>
  <li>
    DNS записи для каждой FSVM
  </li>
</ul>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Разрешения AD на создание файлового сервиса</h5>

<p>Пользовательский аккаунт с правами администратора домена или эквивалентными должен использоваться для создания файлового сервиса и объектов AD и DNS.</p>
</div>

<h5>Высокая доступность (HA)</h5>

<p>
  Каждая FSVM использует API платформы для работы с дисками, для работы с хранилищем через iSCSI на уровне гостевой ОС.
  Это позволяет каждой FSVM подключаться к любому iSCSI-таргету, что полезно при отказе FSVM.
</p>

<p>
    На рисунке показана схема работы FSVM с хранилищем:
</p>
<figure><img alt="FSVM Storage" src="imagesv2/fs_3.png">
<figcaption><span class="label">Рисунок. </span>FSVM и хранилище</figcaption>
</figure>

<p>
  Для обеспечения доступности путей используется DM-MPIO. При этом активный путь - локальный:
</p>
<figure><img alt="FSVM MPIO" src="imagesv2/fs_4a.png">
<figcaption><span class="label">Рисунок. </span>FSVM MPIO</figcaption>
</figure>

<p>
  В случае отказа локальной CVM, DM-MPIO активирует один из путей к удаленной CVM, которая обеспечит дальнейшие IO.
</p>
<figure><img alt="FSVM MPIO Failover" src="imagesv2/fs_4b.png">
<figcaption><span class="label">Рисунок. </span>FSVM MPIO Отказоустойчивость</figcaption>
</figure>

<p>
  Когда локальная CVM восстанавливается, локальный путь снова активируется и операции ввода-вывода становятся локальными.
</p>

<p>
 При нормальной работе, каждая FSVM будет взаимодействовать с ее собственной группой томов для хранения данных и держать пассивное подключение к другим группам. 
 Каждая FSVM будет иметь IP к которому будут подключаться клиенты. Клиентам не нужно знать конкретный адрес каждой FSVM, DFS перенаправит их подключение к нужной FSVM, которая обслуживает их папки.
</p>

<figure><img alt="FSVM Normal Operation" src="imagesv2/fs_5.png">
<figcaption><span class="label">Рисунок. </span>FSVM - нормальная работа</figcaption>
</figure>

<p>
  В случае отказа FSVM группа томов и ее IP будут временно переданы работоспособный FSVM для бесперебойной работы сервиса.
</p>

<p>
    На рисунке показана схема передачи IP адреса FSVM и группы томов:
</p>

<figure><img alt="FSVM Failure Scenario" src="imagesv2/fs_6.png">
<figcaption><span class="label">Рисунок. </span>FSVM сценарий отказа</figcaption>
</figure>

<p>
  Когда отказавшая FSVM вернется в рабочее состояние она снова получит свой IP адрес и папки, начнет обрабатывать запросы клиента.
</p>
<!-- End of File services section -->
</section>
<section data-type="sect1">
<h3>Служба Контейнеров</h3>
<p>
  Платформа Nutanix предоставляет возможность управления персистентными контейнерами на базе <a href="https://www.docker.com/" target="_blank">Docker</a>.
  Ранее имелась возможность запуска Docker на платформе Nutanix, однако персистентность данных была проблемой, учитывая эфемерную природу контейнеров.
</p>

<p>
  Контейнерные технологии, такие как Docker-это другой подход к виртуализации оборудования. При традиционной виртуализации каждая виртуальная машина имеет собственную операционную систему (ОС), но использует общее оборудование. Контейнеры, которые включают в себя приложения и их зависимости, запускаются как изолированные процессы которые используют одно и то же ядро ОС.
</p>

<p>
  В следующей таблице показано простое сравнение виртуальных машин и контейнеров:
</p>

<table>
  <tr>
    <th>Метрики</th>
    <th>ВМ</th>
    <th>Контейнеры</th>
  </tr>
  <tr>
    <td>Тип виртуализации</td>
    <td>Виртуализация аппаратного обеспечения</td>
    <td>Виртуализация ядра ОС</td>
  </tr>
  <tr>
    <td>Перерасход</td>
    <td>Тяжелый</td>
    <td>Легкий</td>
  </tr>
  <tr>
    <td>Скорость подготовки</td>
    <td>Медленнее (секунды или минуты)</td>
    <td>Быстрее / в реальном времени (мкс или мс)</td>
  </tr>
  <tr>
    <td>Накладные расходы</td>
    <td>Ограниченная производительность</td>
    <td>Нативная производительность</td>
  </tr>
  <tr>
    <td>Безопасность</td>
    <td>Полностью изолированные (более безопасно)</td>
    <td>Изолированны на уровне процессов (менее безопасно)</td>
  </tr>
</table>

<h5>Поддерживаемая конфигурация</h5>

<p>
    Решение применимо к следующим конфигурациям:
</p>
<ul>
  Гипервизор:
  <li>
    AHV
  </li>
</ul>
<ul>
  Система контейнеризации*:
  <li>
    Docker 1.13
  </li>
</ul>
<p>
  *начиная с версии 4.7, решение поддерживает интеграцию хранилища только с контейнерами Docker. 
  Однако вы можете разместить любую другую систему контейнеризации в рамках ВМ на платформе.
</p>

<h5>Конструкции службы Контейнеров</h5>
<p>
  Следующие элементы составляют службы контейнеров Acropolis:
</p>
<ul>
  <li>
    <strong>Nutanix Docker Machine Driver:</strong> Управляет созданием узлов через Docker Machine и Сервис образов Acropolis.
  </li>
  <li>
    <strong>Nutanix Docker Volume Plugin:</strong> Отвечает за взаимодействие Блочного сервиса для создания, монтирования, форматирования и подключения томов к контейнерам.
  </li>

</ul>
<p>
  Следующие элементы составляют Docker (Примечание: не обязательно использовать все):
</p>
<ul>
  <li><strong>Docker Image:</strong> Образ контейнера </li>
  <li><strong>Docker Registry:</strong> Хранилище образов контейнеров</li>
  <li><strong>Docker Hub:</strong> Публичное хранилище образов контейнеров</li>
  <li><strong>Docker File:</strong> Текстовый файл, описывающий структуру и состав образа контейнера</li>
  <li><strong>Docker Container:</strong> Запущенный экземпляр контейнера из образа </li>
  <li><strong>Docker Engine:</strong> Создает, доставляет и запускает контейнеры</li>
	<li><strong>Docker Swarm:</strong> Подсистема кластеризации и планирования</li>
	<li><strong>Docker Daemon:</strong> Управляет запросами от клиента Docker, выполняет сборку и запуска контейнеров</li>
	<li><strong>Docker Store:</strong> Магазин приложений, для доверенных контейнеров готовых к продуктивным нагрузкам</li>
</ul>

<h5>Архитектура</h5>
<p>
  Nutanix на текущий момент использует Docker Engine запущенный в ВМ, которые создаются при помощи сервиса Docker Machine. Эти машины могут работать в сочетании с обычными виртуальными машинами на платформе.
</p>
<figure><img alt="Docker - High-level Architecture" src="imagesv2/docker_1.png">
<figcaption><span class="label">Рисунок. </span>Docker - Верхнеуровневая архитектура</figcaption>
</figure>

<p>
  Nutanix предоставляет плагин для Docker по работе с томами, который занимается созданием, форматированием и подключением томов к контейнерам используя Блочные сервисы Acropolis.
  Это обеспечивает возможность подключения постоянных томов к контейнерам, и сохранность данных на них в случае перезагрузки / перемещения.
</p>

<p>
  Персистентность данных достигается путем использования ПО Nutanix Volume Plugin, которое использует Блочные сервисы Acropolis, чтобы подключить том к узлу/контейнеру:
</p>
<figure><img alt="Docker - Block Services" src="imagesv2/docker_2.png">
<figcaption><span class="label">Рисунок. </span>Docker - Блочные сервисы</figcaption>
</figure>

<h5>Пререквизиты</h5>
<p>
  Чтобы воспользоваться Службой Контейнеров необходимо:
</p>
<ul>
  <li>
    Кластер Nutanix с ПО AOS 4.7 или более поздним
  </li>
  <li>
    Образ CentOS 7.0+ или Rhel 7.2+ OS с установленным пакетом iscsi-initiator-utils должен быть загружен на Платформу и доступен через  Acropolis Image Service
  </li>
  <li>
    IP-адрес Службы данных должен быть сконфигурирован
  </li>
  <li>
    Docker Toolbox должен быть установлен на клиентской машине, для выполнения конфигурации
  </li>
  <li>
    Nutanix Docker Machine Driver должен быть добавлен к PATH на клиентской ВМ
  </li>
</ul>

<h5>Создание узла Docker</h5>
<p>
  Если все способновуент пререквизитам, то можно перейти к первым шагам по развертыванию улов Docker используя Docker Machine:
</p>

<p class="codetext">
docker-machine -D create -d nutanix \ </br>
--nutanix-username &lt;PRISM_USER&gt; --nutanix-password &lt;PRISM_PASSWORD&gt; \ </br>
--nutanix-endpoint &lt;CLUSTER_IP&gt;:9440 --nutanix-vm-image &lt;DOCKER_IMAGE_NAME&gt; \ </br> --nutanix-vm-network &lt;NETWORK_NAME&gt; \ </br>
--nutanix-vm-cores &lt;NUM_CPU&gt; --nutanix-vm-mem &lt;MEM_MB&gt; \ </br>
  &lt;DOCKER_HOST_NAME&gt;
</p>

<p>
  Ниже представлена высокоуровневая схема рабочих процессов выполняемых на серверной части:
</p>
<figure><img alt="Docker - Host Creation Workflow" src="imagesv2/docker_4.png">
<figcaption><span class="label">Рисунок. </span>Docker - Создание узла</figcaption>
</figure>

<p>
  Следующие шаг - подключение по SSH к созданному узлу Docker:
</p>

<p class="codetext">
  docker-machine ssh &lt;DOCKER_HOST_NAME&gt;
</p>

<p>
  Для инсталляции Nutanix Docker Volume Plugin выполните команду:
</p>
docker plugin install ntnx/nutanix_volume_plugin PRISM_IP=<PRISM-IP> DATASERVICES_IP=<DATASERVICES-IP> PRISM_PASSWORD=<PRISM-PASSWORD> PRISM_USERNAME=<PRISM-USERNAME> DEFAULT_CONTAINER=<Nutanix-STORAGE-CONTAINER> --alias nutanix
</p>

<p>
  После выполнения команды вы увидите, что плагин установлен и запущен:
</p>
<pre>
<p class="codetext">
[root@DOCKER-NTNX-00 ~]# docker plugin ls
ID        	    Name              Description    		        Enabled
37fba568078d        nutanix:latest    Nutanix volume plugin for docker  true
</p>
</pre>

<h5>Создание контейнера Docker</h5>
<p>
  Когда узел Docker готов и плагин установлен и работает, вы можете начинать создание контейнеров с персистентными дисками.
</p>

<p>
  Том с использованием Блочной службы Acropolis могут быть созданы обычными командами Docker. Например:
</p>
<p class="codetext">
docker volume create \ </br>
 &lt;VOLUME_NAME&gt;  --driver nutanix
</br>
Например: </br>
docker volume create PGDataVol --driver nutanix
</p>

<p>
Следующая команда может быть использована для создания контейнера с персистентным томом:
</p>
<p class="codetext">
docker run -d --name &lt;CONTAINER_NAME&gt; \ </br>
-p &lt;START_PORT:END_PORT&gt;  --volume-driver nutanix \ </br>
-v &lt;VOL_NAME:VOL_MOUNT_POINT&gt; &lt;DOCKER_IMAGE_NAME&gt; </br>
</br>
Example: </br>
docker run -d --name postgresexample -p 5433:5433 --volume-driver nutanix -v PGDataVol:/var/lib/postgresql/data postgres:latest
</p>

<p>
  Ниже представлена высокоуровневая схема рабочих процессов выполняемых на серверной части:
</p>
<figure><img alt="Docker - Container Creation Workflow" src="imagesv2/docker_3.png">
<figcaption><span class="label">Рисунок. </span>Docker - Создание контейнера</figcaption>
</figure>

<p>
  Теперь у вас есть контейнер с персистентным диском!
</p>

<!-- End of Container services section -->
</section>

<!-- End of services section -->
</section>

<section data-type="chapter">
<h2>Резервное копирование и DR</h2>
<p>
  Nutanix обеспечивает встроенные механизмы резервного копирования и катастрофоустойчивости (DR), которые могут быть использованы для резервного копирования и восстановления пользовательских ВМ и других данных на РХД.
</p>

<p>
  Мы рассмотрим следующие пункты в следующих разделах:
</p>

<ul>
  <li>
    Объекты реализации
  </li>
  <li>
    Защищаемые объекты
  </li>
  <li>
    Резервное копирование и восстановление
  </li>
  <li>
    Репликации и катастрофоустойчивость
  </li>
</ul>

<p>
  Примечание: Несмотря на то, что Nutanix имеет встроенные функции СРК и DR традиционные решения могут так же использоваться (например Commvault, Rubrik, и так далее), в своей работе данные решения используют встроенные механизмы Nutanix - VSS, snapshots).
</p>

<h3>Объекты реализации</h3>

<p>Встроенные функции DR и СРК работают со следующими объектами:</p>

<h5>Домен защиты (PD)</h5>

<ul>
	<li>Роль: Группа ВМ и/или файлов для защиты</li>
	<li>Описание: Группа ВМ и/или файлов для которых выполняется совместная репликация по расписанию.&nbsp; Домен защиты может защищать весь контейнер или же вы можете выбрать конкретные ВМ/файлы</li>
</ul>

<div data-type="note" class="note" id="pro-tip-W1i5uRuRT2"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Создавайте разные Домены защиты для разные сервисов, согласно желаемому RPO/RTO.&nbsp; Для распространения файлов вы также можете использовать PD, добавив в него целевые файлы.</p>
</div>

<h5>Группа консистентности (CG)</h5>

<ul>
	<li>Роль: Группа ВМ или файлов, собранные в группу, для обеспечения сквозной целостности</li>
	<li>Описание: ВМ и/или файлы собранные в логическую группу, для выполнения снимков, имеющих сквозную целостность между собой.&nbsp;
    Например, при восстановлении вся информационная система размещенная на разных ВМ собранных в группу консистентности сохранит целостность при восстановлении. Домен доступности может содержать несколько групп консистентности</li>
</ul>

<div data-type="note" class="note" id="pro-tip-yGioHpuoTz"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Группируйте ВМ в зависимости от сервисов развернутых на них, для обеспечения целостности всей ИС (например приложения и их БД)</p>
</div>

<h5>Расписание создания снимков</h5>

<ul>
	<li>Роль: Расписание создания снимков и выполнения репликации</li>
	<li>Описание: Расписание выполнения репликации и создания снимков для ВМ в доменах защиты и группах консистентности</li>
</ul>

<div data-type="note" class="note" id="pro-tip-Y2iwfquqTR"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Расписание должно соответствовать желаемому RPO</p>
</div>

<h5>Политика хранения</h5>

<ul>
	<li>Роль: Количество локальных или удаленных снимков для хранения</li>
	<li>Описание: Политика определяет количество локальных или удаленных снимков для хранения.&nbsp; Примечание: Удаленная площадка должна быть настроена для настройки задач репликации и политик хранения.</li>
</ul>

<div data-type="note" class="note" id="pro-tip-EaiQIYuoTo"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Политика хранения должна быть равна желаемому количеству точек восстановления ВМ/файлов</p>
</div>

<h5>Удаленный сайт</h5>

<ul>
	<li>Роль: Удаленный кластер Nutanix</li>
	<li>Описание: Удаленный кластер Nutanix, который используется как цель для создания РК и снимков для выполнения восстановления в случае катастрофы.</li>
</ul>

<div data-type="note" class="note" id="pro-tip-bOi5hDuyTp"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Убедитесь, что удаленный кластер имеет нужное количество вычислительных ресурсов и ресурсов хранения для восстановления ВМ.&nbsp; В некоторых случаях репликация/DR между серверными стойками так же имеет смысл.</p>
</div>

<p>Ниже представлена высокоуровневая схема отношений между перечисленными компонентами:</p>

<figure id="id-EGtOhYuoTo"><img alt="DR Construct Mapping" class="iimagesv2dr_2png" src="imagesv2/dr_2.png">
<figcaption><span class="label">Рисунок 11-37. </span>DR - компоненты</figcaption>
</figure>

<h3>Объекты защиты</h3>

<p>
  Вы можете защищать объекты такие как виртуальные машины, группы машин или файлы:
</p>

<p>
  На странице Data Protection нужно выбрать вкладку + Protection Domain -> Async DR:
</p>

<figure><img alt="DR - Async PD" class="iimagesv2dr_2png" src="imagesv2/Prism/backup/backup_0.png">
<figcaption><span class="label">Рисунок. </span>DR - Ассинхронный домен защиты</figcaption>
</figure>

<p>
  Укажите имя домена защиты и нажмите 'Create'
</p>

<figure><img alt="DR - Create PD" class="iimagesv2dr_2png" src="imagesv2/Prism/backup/backup_1.png">
<figcaption><span class="label">Рисунок. </span>DR - Создание домена защиты</figcaption>
</figure>

<p>
  Теперь укажите объекты для защиты:
</p>

<figure><img alt="DR - Async PD" class="iimagesv2dr_2png" src="imagesv2/Prism/backup/backup_2.png">
<figcaption><span class="label">Рисунок. </span>DR - Ассинхронный домен защиты</figcaption>
</figure>

<p>
  Нажмите 'Protect Selected Entities'
</p>

<figure><img alt="DR - Protect Entities" class="iimagesv2dr_2png" src="imagesv2/Prism/backup/backup_3.png">
<figcaption><span class="label">Рисунок. </span>DR - Protect Entities</figcaption>
</figure>

<p>
  Защищенные объекты отображаются в секции 'Protected Entities'
</p>

<figure><img alt="DR - Protected Entities" class="iimagesv2dr_2png" src="imagesv2/Prism/backup/backup_4.png">
<figcaption><span class="label">Рисунок. </span>DR - Защищенные объекты</figcaption>
</figure>

<p>
  Нажмите 'Next', затем click 'Next Schedule' для настройки расписания создания снимков и выполнения репликации
</p>

<p>
  Настройте требуемую частоту выполнения снимков и глубину их хранения, а так же удаленные кластеры для передачи копий
</p>

<figure><img alt="DR - Create Schedule" class="iimagesv2dr_2png" src="imagesv2/Prism/backup/backup_5.png">
<figcaption><span class="label">Рисунок. </span>DR - Настройка расписания</figcaption>
</figure>

<p>
  Нажмите 'Create Schedule' для завершения настройки расписания.
</p>

<div data-type="note" class="note" id="pro-tip-Y2iwfquqTR"><h6>Примечание</h6>
<h5>Множественные задачи расписания</h5>

<p>Существует также возможность создания множественных снимков и задач репликации. Например, если вы хотите иметь отдельную политику создания локальных снимков раз в час и копирования на удаленную площадку раз в день.</p>
</div>

<p>Важно отметить, что весь контейнер может быть защищен разом. Однако, платформа предлагает возможнсть гранулярной настройки репликации - на уровне ВМ и файлов.</p>

<h3>Работа с резервными копиями</h3>
<p>
  Для создания снимков Платформа использует встроенные функции РХД по работе со снимками и встроенные сервисы Cerebro и Stargate.
  Эти снимки так же имеют свои собственные карты блоков, для большей эффективности использования хранилища и сокращения перерасхода ресурсов.
  Более подробно об этом можно прочитать в разделе "Снимки и клоны".
</p>

<p>
  Стандартные операции РК и восстановления включают:
</p>

<ul>
  <li>
    Snapshot: Создание снимка и репликация
  </li>
  <li>
    Restore: Восстановление ВМ / Файлов из снимков (заменяет оригинальный объект)
  </li>
  <li>
    Clone: Процедура похожая на восстановление, однако в результате создается новый объект, оригинал при этом не затрагивается
  </li>
</ul>

<p>
  На странице Data Protection, в разделе 'Protecting Entities', можно увидеть созданные ранее домены защиты.
</p>
<figure><img alt="DR - View PDs" class="iimagesv2dr_2png" src="imagesv2/Prism/restore/restore_1.png">
<figcaption><span class="label">Рисунок. </span>DR - домены защиты</figcaption>
</figure>

<p>
  Выбрав один из доменов защиты вы получите перечень действий, который можно с ним осуществить:
</p>
<figure><img alt="DR - PD Actions" class="iimagesv2dr_2png" src="imagesv2/Prism/restore/restore_2.png">
<figcaption><span class="label">Рисунок. </span>DR - PD Actions</figcaption>
</figure>

<p>
  Если вы нажмете 'Take Snapshot' то будет сделан дополнительный снимок, который может быть передан на удаленную площадку:
</p>
<figure><img alt="DR - Take Snapshot" class="iimagesv2dr_2png" src="imagesv2/Prism/restore/restore_3.png">
<figcaption><span class="label">Рисунок. </span>DR - Создание снимка</figcaption>
</figure>

<p>
  Так же вы можете выполнить операцию 'Migrate', которая выполнит восстановление домена защиты на удаленной площадке:
</p>
<figure><img alt="DR - Migrate" class="iimagesv2dr_2png" src="imagesv2/Prism/restore/restore_4.png">
<figcaption><span class="label">Рисунок. </span>DR - Migrate</figcaption>
</figure>

<p>
  В случае выполнения ручной миграции система сделает новый снимок и затем передаст его на удаленную площадку.
</p>

<div data-type="note" class="note" id="pro-tip-05i5cRT9"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Начиная с Asterix и далее вы можете использовать репликацию данных одного узла для защиты данных.</p>
</div>

<p>
  Так же на таблице ниже вы можете посмотреть все существующие снимки домена защиты:
</p>
<figure><img alt="DR - Local Snapshots" class="iimagesv2dr_2png" src="imagesv2/Prism/restore/restore_5.png">
<figcaption><span class="label">Рисунок. </span>DR - Локальные снимки</figcaption>
</figure>

<p>
  Тут вы можете восстановить снимок или создать клон:
</p>
<figure><img alt="DR - Restore Snapshot" class="iimagesv2dr_2png" src="imagesv2/Prism/restore/restore_6.png">
<figcaption><span class="label">Рисунок. </span>DR - Восстановление снимков</figcaption>
</figure>
<p>
  Если вы выбрали 'Create new entities' будет выполнено клонирование снимков в домене защиты, и созданы новые объекты с указанным при восстановлении префиксом. Если будет выбрана опция 'Overwrite existing entities' восстанавливаемые копии заменят текущие объекты.
</p>

<div data-type="note" class="note" id="pro-tip-bOi5hDuyTp"><h6>Примечание</h6>
<h5>Хранение копий на локальном хранилище / Storage only backup target</h5>

<p>Для хранения резервных или архивных копий можно использовать удаленный кластер Nutanix, который состоит из узлов "storage only".</p>
</div>

<!-- End of backup and restore section -->

<h3>Целостные снимки на уровне приложений</h3>

<p>
  Nutanix предоставляет встроенный механизм VmQueisced Snapshot Service для выполнения снимков обеспечивающих целостность на уровне пользовательских приложений, разворачиваемых в гостевых ОС.
</p>

<div data-type="note" class="note" id="pro-tip-Y2iwfquqTR"><h6>Примечание</h6>
<h5>Служба создания снимков VmQueisced(VSS)</h5>

<p>VSS обычно является типичным сервисом в ОС Windows - Volume Shadow Copy Service. Однако, наше решение решение работает как с Windows, так и с Linux и мы решили назвать его VmQueisced Snapshot Service.</p>
</div>

<h5>Поддерживаемые конфигурации</h5>

<p>
  Данное решение применимо к гостевым ОС на базе Windows и Linux, включая версии перечисленные ниже:
</p>

<ul>
  <li>
    Гипервизоры:
    <ul>
      <li>
        ESX
      </li>
      <li>
        AHV
      </li>
    </ul>
  </li>
  <li>
    Windows
    <ul>
      <li>
        2008R2, 2012, 2012R2
      </li>
    </ul>
  </li>
  <li>
    Linux
    <ul>
      <li>
        CentOS 6.5/7.0
      </li>
      <li>
        RHEL 6.5/7.0
      </li>
      <li>
        OEL 6.5/7.0
      </li>
      <li>
        Ubuntu 14.04+
      </li>
      <li>
        SLES11SP3+
      </li>
    </ul>
  </li>
</ul>

<h5>Пререквизиты</h5>

<p>
  Для использования функции создания снимков Nutanix VSS требуется:
</p>

<ul>
  <li>
    Платформа Nutanix
    <ul>
      <li>
        VIP-адрес кластера должен быть сконфигурирован
      </li>
    </ul>
  </li>
  <li>
    Гостевая ОС / ВМ
    <ul>
      <li>
        ПО NGT должно быть установлено
      </li>
      <li>
        VIP-адрес CVM должен быть доступен по порту 2074
      </li>
    </ul>
  </li>
  <li>
    Конфигурация катастрофоустойчивости 
    <ul>
      <li>
        Пользовательская ВМ должна быть в Домене защиты, опция 'Use application consistent snapshots' должна быть включена</li>
    </ul>
  </li>
</ul>

<h5>Архитектура</h5>

<p>
  Начиная с версии 4.6 данная функция доступна посредством ПО Nutanix Hardware VSS которое устанавливается в гостевые ОС вместе с Гостевыми утилитами.
  Более подробно о Гостевых утилитах Nutanix можно прочитать в соответствующей секции.
</p>

<p>
  Ниже показана верхнеуровневая архитектура сервиса VSS:
</p>

<figure><img alt="Nutanix Hardware VSS Provider" class="iimagesv2dr_1png" src="imagesv2/ngt_vss_1.png">

<p>
  Вы можете вручную выполнить создание целостного снимка, следуя стандартному пути создания снимка, но отметив опцию 'Use application consistent snapshots', когда выбираете ВМ для защиты.
</p>

<div data-type="note" class="note"><h6>Примечание</h6>
<h5>Включение/Выключение Nutanix VSS</h5>

<p>Когда для пользовательских ВМ включены Гостевые утилиты Nutanix NGT функция VSS становится доступной по умолчанию. Однако, вы можете выключить эту функцию, выполнив команду:</p>
<p class="codetext">ncli ngt disable-applications application-names=vss_snapshot vm_id=&lt;VM_ID&gt;</p>
</div>

<h5>Архитектура Windows VSS</h5>

<p>
  Решение VSS Nutanix интегрируется с встроенной функцией VSS ОС Windows VSS. Ниже показана верхнеуровневая схема связи функций:
</p>

<figure><img alt="Nutanix VSS - Windows Architecture" src="imagesv2/ngt_vss_2.png">
<figcaption><span class="label">Рисунок. </span>Nutanix VSS - Архитектура сервисов Windows</figcaption>
</figure>

<p>
  Как только NGT установлены вы можете увидеть статус агента NGT и сервиса VSS Hardware Provider:
</p>

<figure><img alt="VSS Hardware Provider" src="imagesv2/Prism/ngt/ngt_deploy_5b.png">
<figcaption><span class="label">Рисунок. </span>Сервис VSS - Hardware Provider</figcaption>
</figure>

<h5>Архитектура Linux VSS</h5>

<p>
  Решение для Linux похоже на решение для Windows, однако вместо MS VSS используются скрипты. 
</p>

<p>
 Ниже показана архитектура и схема взаимодействия компонент:
</p>

<figure><img alt="Nutanix VSS - Linux Architecture" src="imagesv2/ngt_vss_3.png">
<figcaption><span class="label">Рисунок. </span>Nutanix VSS - Архитектура Linux</figcaption>
</figure>

<p>
  Скрипты выполняющие временную заморозку и последующий возврат к рабочему состоянию расположены тут:
</p>
<ul>
  <li>
    Пре-скрипт: /sbin/pre_freeze
  </li>
  <li>
    Пост-скрипт: /sbin/post-thaw
  </li>
</ul>

<div data-type="note" class="note" id="pro-tip-bOi5hDuyTp"><h6>Примечание</h6>
<h5>Eliminating ESXi Stun</h5>

<p>ESXi имеет встроенную возможность создания целостных снимков на уровне гостевых приложений используя ПО VMware Guest Tools.  
  Однако, в процессе создания таких снимков создаются дельта-диски и ESXi подвисает на момент изменения маппинга виртуальных дисков к новым дельта-дискам, которые начнут обрабатывать IO.
  То же самое происходит, когда снимки удаляются.</p>

<p>
  В момент подвисания - гостевая ОС не может выполнять никакие операции и по факту находится в зависшем состоянии - не проходят ping пакеты, операции IO остановлены.
  Длительность подвисания зависит от количества vmdks и скорости операций на хранилище данных.
</p>

<p>
  С Nutanix VSS мы позволяем полностью обойти создание снимков VMware, а значит полностью исключаем такое поведение системы и ВМ. Предоставляя большую стабильность и скорость работы для ВМ.
</p>
</div>

<!-- End of app consistent snapshots -->
<section data-type="sect1">
<h3>Репликация и катастрофоустойчивость (DR)</h3>

<p>Краткий рассказ на английском языке доступен по <a href="https://youtu.be/AoKwKI7CXIM">ссылке</a></p>

<div class="video-container"><iframe allowfullscreen frameborder="0" src="https://www.youtube.com/embed//AoKwKI7CXIM"></iframe></div>

<p>Nutanix предоставляет встроенные механизмы для выполнения репликации данных между кластерами и подсистему обеспечения катастрофоустойчивости.
    Эта функциональность реализуется на базе встроенного механизма создания снимков. Данный механизм описан в разделе Снимки и клоны.&nbsp; 
    Компонент платформы Cerebro отвечает за управление DR и репликацией, компонент запускается на всех узлах кластера, и осуществляет выбор мастера, который управляет задачами репликации.&nbsp; 
    В случае если текущий мастер Cerebro выйдет из строя, другой экземпляр сервиса будет выбран для этой роли. Страница Cerebro - &lt;CVM IP&gt;:2020.
    DR может быть разбит на несколько основных объектов:</p>
    
<ul>
	<li>Топология репликации</li>
	<li>Жизненный цикл репликации</li>
	<li>Глобальная дедупликация</li>
</ul>

<h5>Топология репликации</h5>

<p>Существует несколько топологий репликации: Один к одному, один ко многим, все ко всем.&nbsp;В отличие от традиционных решений, которые в основном предлагают репликацию один к одному или один ко многим, Nutanix обеспечивает гибкую топологию многие ко многим.</p>  

<figure id="id-lNtJtGuATe"><img alt="Example Replication Topologies" class="iimagesv2dr_1png" src="imagesv2/dr_1.png">
<figcaption><span class="label">Рисунок 11-36. </span>Примеры топологий репликации</figcaption>
</figure>

<p>Естественно, вы можете выбрать ту топологию, которая наиболее полно отвечает запросам вашей компании.</p>

<h5>Жизненный цикл репликации</h5>

<p>Механизм репликации Nutanix использует сервис Cerebro упомянутый выше.&nbsp; 
    Сервис Cerebro состоит из “Cerebro Master” который размещается на одной из функционирующих CVM, и Cerebro Slaves которые работают на всех остальных CVM.&nbsp; 
    В случае выхода из строя текущего мастера, новый мастер будет выбран автоматически.</p>  

<p>Cerebro Master отвечает за управление задачами и передачи их сервисам Cerebro Slaves, а так же за координацию задач Cerebro Master, при выполнении репликации.</p>

<p>В процессе репликации Cerebro Master поймет какие данные должны быть отреплицированны, и передаст задачу Cerebro Slaves который в свою очередь сообщит сервису Stargate какие данные реплицировать и куда.</p>

<p>
  Реплицируемые данные защищаются на разных уровнях в процессе. Экстенты считываются на кластере источнике, с них снимаются контрольные суммы, чтобы убедиться что данные целостные. Затем с переданных экстентов на кластере назначения так же снимаются контрольные суммы и осуществляется проверка целостности. Протокол TCP используемый для передачи снимков обеспечивает конститентность на уровне сети.
</p>

<p>На следующем рисунке показано представление этой архитектуры:</p>

<figure id="id-d0tPFVu8TW"><img alt="Replication Architecture" class="iimagesv2dr_3png" src="imagesv2/dr_3.png">
<figcaption><span class="label">Рисунок 11-38. </span>Репликация</figcaption>
</figure>

<p>Кроме того можно настроить доступ к удаленному кластеру через прокси, который будет использоваться как входная точка для всего трафика репликации и DR от кластера.</p>

<div data-type="note" class="note" id="pro-tip-xkiRf1uDTz"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Когда используется конфигурация с прокси - всегда используйте VIP адрес целевого кластера, он будет доступным даже в случае выхода из строя CVM</p>
</div>

<p>Ниже показана схема репликации при использовании прокси:</p>

<figure id="id-xbtMc1uDTz"><img alt="Replication Architecture - Proxy" class="iimagesv2dr_4png" src="imagesv2/dr_4.png">
<figcaption><span class="label">Рисунок 11-39. </span>Репликация - Прокси</figcaption>
</figure>

<p>В некоторых случаях можно настроить доступ на удаленный кластер через SSH тоннель, через него будет проходить вест трафик репликации.</p>

<div data-type="note" class="note" id="note-kpirUduBT3"><h6>Примечание</h6>
<h5>Примечание</h5>
Такая конфигурация не должна использоваться для продуктивных инсталляций. Кластерные VIP-адреса должны использоваться для обеспечения доступности.
</div>

<p>Ниже показана схема репликации при использовании SSH тоннеля:</p>

<figure id="id-k1tvCduBT3"><img alt="Replication Architecture - SSH Tunnel" class="iimagesv2dr_5png" src="imagesv2/dr_5.png">
<figcaption><span class="label">Рисунок 11-40. </span>Репликация - Тоннель SSH</figcaption>
</figure>

<h5>Глобальная дедупликация</h5>

<p>Как говорилось ранее в разделе Elastic Deduplication Engine, Платформа имеет встроенные функции обеспечивающие дедупликацию данных, простым обновлением указателей в метаданных.
  Тот же концепт применяется к функции репликации и DR. Перед отправкой данных с кластера на кластер Платформа осуществить запрос на кластер назначения, и проверит есть ли там хэш.
  Наличие хэша подразумевает наличие данных, это значит что данные передавать не надо. Будет выполнено лишь обновление хэшей. Если же данных нет - такие данные будут сжаты и отправлены на кластер назначения. 
  После передачи - данные существуют на обоих кластерах и могут использоваться для дедупликации</p>

<p>На следующем рисунке показан пример развертывания трех кластеров, где каждый кластер содержит один из нескольких доменов защиты (PD):</p>

<figure id="id-1nteRSkudTQ"><img alt="Replication Deduplication" class="iimagesv2dr_6png" src="imagesv2/dr_6.png">
<figcaption><span class="label">Рисунок 11-41. </span>Репликация - Дедупликация</figcaption>
</figure>

<div data-type="note" class="note" id="note-3Nim3HyulTb"><h6>Примечание</h6>
<h5>Примечание</h5>

<p>Дедупликация должна быть включена для контейнера кластера источника и для кластера назначения.</p>
</div>
</section>

<section data-type="sect1" id="nearsync-yAI5T1Td">
<h3>Синхронизация NearSync</h3>

<p>Используя традиционные возможности асинхронной репликации Nutanix предоставил поддержку почти синхронной репликации (NearSync).</p>

<p>NearSync обеспечивает - минимальное влияние на задержку I/O (как при асинхронной репликации) и обеспечивает очень низкий RPO (как при синхронной репликации).
  Т.е. пользователи могут иметь очень низкий RPO без перерасхода ресурсов, как при синхронной репликации.
</p>

<p>Эта функция использует новую технологию создания снимков - LWS (light-weight snapshot). В отличие от традиционных снимков vDisk которые используются для асинхронной репликации,
  данная технология использует маркеры и полностью реализована на базе OpLog (в отличие от классических снимков на базе Хранилища экстентов).</p>

<p>Мы добавили новый сервис Mesos для управления снимками и абстрагирования от сложности полных/инкрементальных снимков. Cerebro при этом все так же используется для управления политиками и высокоуровневыми компонентами.
Mesos при этом отвечает за взаимодействие с Stargate и контроль за жизненным циклом LWS.</p>

<p>Ниже представлена высокоуровневая архитектура компонентов NearSync:</p>


<figure id="id-1nteRSkudTQ"><img alt="NearSync Components" class="iimagesv2dr_6png" src="imagesv2/nearsync_1.png">
<figcaption><span class="label">Рисунок. </span>NearSync - взаимодействие компонент</figcaption>
</figure>


<p>Когда пользователь настраивает частоту выполнении снимка меньшую или равную 15 минутам, NearSync будет использоваться автоматически. 
    В этот момент, будет сделан инициализационный снимок и передан на кластер/кластеры назначения.
    Если данная задача будет выполнена менее чем за 60 минут, следующий снимок будет сразу же сделан и скопирован в дополнение к запуску репликации моментальных снимков LWS.
    Как только второй снимок будет скопирован, все уже скопированные снимки LWS станут валидными и Платформа поймет, что NearSync может использоваться.</p>
  
  <p>На следующем рисунке показан пример временной шкалы от включения NearSync для начала функционирования:</p>  

<figure id="id-1nteRSkudTQ"><img alt="NearSync Replication Lifecycle" class="iimagesv2dr_6png" src="imagesv2/nearsync_2.png">
<figcaption><span class="label">Рисунок. </span>Жизненный цикл репликации NearSync</figcaption>
</figure>

<p>После того, как стало понятно, что NearSync может использовать - снимки vDisk будут выполняться каждый час. 
    Вместо отправки полного снимка на удаленный сайт, удаленный сайт собирает снимок vDisk на базе базового снимка vDisk и изменений, передаваемых с помощью LWS.
  </p>
  
  <p>
    В случае, если NearSync какое-то время не может выполняться, в связи с тем, что репликация LWS занимает более 60 минут, система автоматически переключится на выполнение классических снимков vDisk.
    Если один из снимков выполнился за менее чем 60 минут, система сразу же выполнит еще один снимок и начнет репликацию LWS. Как только полный снимок будет выполнен, снимок LWS будет признан валидным и система снова переключится на NearSync.
    Т.е. процесс похож на тот, который выполняется при первичном включении этой функции.
  </p>
  <p>Когда снимок LWS восстанавливается, система выполнит клонирование последнего снимка vDisk и применит к нему все требуемые изменения из LWS.</p>
  
  <p>Ниже показано, каким образом осуществляется восстановление снимков на базе LWS:</p>  

<figure id="id-1nteRSkudTQ"><img alt="vDisk Restore from LWS" class="iimagesv2dr_6png" src="imagesv2/nearsync_3.png">
<figcaption><span class="label">Рисунок. </span>Восстановление vDisk из LWS</figcaption>
</figure>

</section>

<section data-type="sect1" id="metro-availability-eqInSNTE">
<h3>Метро-кластер</h3>

<p>Платформа Nutanix обеспечивает встроенную возможность создания растянутого между ЦОД кластера. &nbsp;При такой инсталляции вычислительные ресурсы кластера делятся на две части между ЦОД и имеют общий ресурс хранения.</p>

<p>Такой подход обеспечивает расширение домена HA, с одного ЦОД на два. И позволяет обеспечить близкие к нулю показатели RTO и RPO.</p>

<p>При такой инсталляции каждый ЦОД имеет свой собственный кластер Nutanix, однако контейнеры логически "растянуты" между ними с использованием синхронной репликации.</p>

<p>Ниже показана высокоуровневая архитектура метро-кластера:</p>

<figure id="id-lNt4HySATe"><img alt="Metro Availability - Normal State" class="iimagesv2metro_1png" src="imagesv2/metro_1.png">
<figcaption><span class="label">Рисунок 11-45. </span>Метро-кластер - Нормальное состояние</figcaption>
</figure>

<p>В случае выхода из строя одного кластера, будет обработано событие HA при котором все ВМ будут перезапущены на работоспособном кластере. Восстановление работоспособности кластера, вышедшего из строя обычно ручной процесс.
    Однако начиная с версии Asterix восстановление после сбоя может быть автоматизировано встроенными средствами и сконфигурировано через Prism.</p>
  
<p>Ниже показан пример отказа одного из кластеров:</p>  

<figure id="id-J2tMfaS9T9"><img alt="Metro Availability - Site Failure" class="iimagesv2metro_2png" src="imagesv2/metro_2.png">
<figcaption><span class="label">Рисунок 11-46. </span>Метро-кластер - Отказ кластера / ЦОД</figcaption>
</figure>

<p>В случае проблем соединения между кластерами - каждый из них продолжит функционировать независимо друг от друга.&nbsp; 
    Как только соединение будет восстановлено - кластеры выполнят синхронизацию накопившихся изменений и синхронная репликация будет снова восстановлена. </p>
  
  <p>Ниже показан пример проблем соединения:</p>
  
  

<figure id="id-OQt3IxSjTJ"><img alt="Metro Availability - Link Failure" class="iimagesv2metro_3png" src="imagesv2/metro_3.png">
<figcaption><span class="label">Рисунок 11-47. </span>Метро-кластер - проблемы соединения</figcaption>
</figure>
<!-- End of metro avail section -->
</section>

<section data-type="sect1" id="cloud-connect-yAI5T1Td">
<h3>Функция Cloud Connect</h3>

<p>
    Функция Cloud Connect построена на базе встроенных механизмов DR и репликации РХД. Данная функция расширяет возможности репликации до облачных провайдеров (Amazon Web Services, Microsoft Azure).&nbsp;
    Примечание: Сейчас возможно выполнение резервного копирования / репликации данных.</p>
  
  <p>Облачная инфраструктура провайдера может быть добавлена как удаленный сайт, так же через Prism.&nbsp; 
     Как только облачная платформа будет добавлена, Платформа автоматически развернет один узел кластера как ВМ в EC2 (сейчас используется m1.xlarge) или Azure (сейчас - D3). Эта ВМ будет использоваться как логическая точка входа.
  </p>
  
  <p>ВМ в облачном сервисе представляет собой ПО работающее на основе Acropolis для локальных кластеров.&nbsp;Это значит, что будут использоваться все встроенные функции репликации (глобальная дедупликация, репликация изменений и так далее).</p>
  
  <p>В случае, если несколько кластеров используют функцию Cloud Connect, они будут A) использовать одну и ту же ВМ, или B) поднимут по отдельной ВМ.</p>
  
  <p>
    Хранилище для ВМ будет реализовано при помощи "cloud disk", который по сути является диском на S3 (AWS) или BlobStore (Azure). Данные при этом хранятся как обычные egroups, которые по сути являются файлами на объектном хранилище.
  </p>
  
  <p>Ниже представлена схема, где в качестве удаленной площадки используется инфраструктура облачного провайдера:</p>  

<figure id="id-lNtnF1TATe"><img alt="Cloud Connect - Region" class="iimagesv2cloudconn_1png" src="imagesv2/cloudconn_1.png">
<figcaption><span class="label">Рисунок 11-42. </span>Cloud Connect - Облачная инфраструктура провайдера</figcaption>
</figure>

<p>Поскольку облачный удаленный сайт похож на любой другой удаленный сайт Nutanix, кластер может реплицироваться в несколько регионов, если требуется более высокая доступность</p>

<figure id="id-Q4tgfVT5TX"><img alt="Cloud Connect - Multi-region" class="iimagesv2cloudconn_2png" src="imagesv2/cloudconn_2.png">
<figcaption><span class="label">Рисунок 11-43. </span>Cloud Connect - Несколько облачных инфраструктур</figcaption>
</figure>

<p>
    Те же политики репликации и хранения используются для репликации данных с помощью Cloud Connect.&nbsp;
    По мере устаревания или истечения срока действия данных / моментальных снимков облачный кластер будет очищать данные по мере необходимости.</p>
  
  <p>Если репликация не происходит часто (например, ежедневно или еженедельно), платформа может быть сконфигурирована для включения облачных ВМ до запланированной репликации и выключать их после ее выполнения</p>
  
  <p>Данные, реплицированные в любой облачный регион, также можно извлечь и восстановить в любой существующий или только что созданный кластер Nutanix с настроенными удаленными облачными узлами:</p>
    
<figure id="id-OQtXU1TjTJ"><img alt="Cloud Connect - Restore" class="iimagesv2cloudconn_3png" src="imagesv2/cloudconn_3.png">
<figcaption><span class="label">Рисунок 11-44. </span>Cloud Connect - Восстановаление</figcaption>
</figure>
</section>

<!-- Replication and DR section -->
</section>

<!-- End of backup and dr section -->
</section>

<section data-type="chapter" id="application-mobility-fabric-coming-soon-3jIvS1">
<h2>Подсистема мобильности приложений</h2>

<p>В ближайшее время этот раздел будет обновлен!</p>
</section>

<section data-type="chapter" id="administration-lkInFl">
<h2>Администрирование</h2>

<section data-type="sect1" id="important-pages-M2IosXFw">
<h3>Важные страницы</h3>

<p>Ниже перечислены важные технологические страницы интерфейса Nutanix. Они позволяют получить детальную статистику по нагрузке и другую служебную информацию.&nbsp; 
  Все URL формируются следующим образом: http://&lt;Nutanix CVM IP/DNS&gt;:&lt;Port/path (mentioned below)&gt;&nbsp; 
  Например: http://MyCVM-A:2009&nbsp; Примечание: если вы находитесь в другой подсети, IPtables должен быть отключен на CVM для доступа к страницам.</p>

<h5>2009</h5>

<p>Это служебная страница сервиса Stargate, которая позволяет получить данные о работе с данными, должна использоваться только опытными пользователями.&nbsp;</p>

<h5>2009/latency</h5>

<p>Это служебная страница сервиса Stargate, которая предоставляет данные по задержкам при работе с данными.</p>

<h5>2009/vdisk_stats</h5>

<p>Это служебная страница сервиса Stargate, которая предоставляет данные по работе с файлами, включая размер операций I/O, задержки, количество операций записи (e.g., OpLog, eStore), 
    количество операций чтения (cache, SSD, HDD.) и так далее.</p>

<h5>2009/h/traces</h5>

<p>Это служебная страница сервиса Stargate, используемая для отслеживания операций.</p>

<h5>2009/h/vars</h5>

<p>Это служебная страница сервиса Stargate, используемая для мониторинга различных счетчиков.</p>

<h5>2010</h5>

<p>Это служебная страница сервиса Curator, используемая для мониторинга работы Curator.</p>

<h5>2010/master/control</h5>

<p>Это служебная страница сервиса Curator, используемая для ручного запуска заданий сервиса Curator.</p>

<h5>2011</h5>

<p>Это служебная страница сервиса Chronos, используемая для мониторинга задач назначенных сервисом Curator.</p>

<h5>2020</h5>

<p>Это служебная страница сервиса Cerebro, используемая для мониторинга статуса доменов защиты, операций репликаций и DR.</p>

<h5>2020/h/traces</h5>

<p>Это служебная страница сервиса Cerebro, используемая для мониторинга текущего статуса операций доменов защиты и репликаций.</p>

<h5>2030</h5>

<p>Это основная служебная страница Acropolis, отображает информацию о узлах, текущих задачах и сетевых настройках.</p>

<h5>2030/sched</h5>

<p>Информационная страница Acropolis, отображает информацию о ВМ, распределении нагрузки между узлами.&nbsp; 
  Страница отображает информацию о доступных ресурсах узлов и ВМ запущенных на них.</p>

<h5>2030/tasks</h5>

<p>Информационная страница Acropolis, отображает информацию о статусах задач.&nbsp; 
  Вы можете нажать на любой UUID задачи, для получения детальной информации в формате JSON.</p>

<h5>2030/vms</h5>

<p>Информационная страница Acropolis, отображает информацию о статусах ВМ.&nbsp; 
    Вы можете нажать на имя ВМ для подключения к ее консоли.</p>
</section>

<section data-type="sect1" id="cluster-commands-QMIMupFQ">
<h3>Команды управления кластером</h3>

<h5>Проверка статуса кластера</h5>

<p class="codedescription">Описание: Проверка статуса кластера из CLI</p>

<p class="codetext">cluster status</p>

<h5>Проверка статуса CVM</h5>

<p class="codedescription">Описание: Проверка статуса CVM из CLI</p>

<p class="codetext">genesis status</p>

<p>Проверка статуса обновления</p>

<p class="codetext">upgrade_status</p>

<h5>Проверка статуса обновления узлов</h5>

<h5>Проверка статуса обновления гипервизора</h5>

<p class="codedescription">Описание: Проверка статуса обновления гипервизора из CLI на любой CVM</p>

<p class="codetext">host_upgrade_status</p>

<p>Детальный журнальный файл (на каждой CVM)</p>

<p class="codetext">~/data/logs/host_upgrade.out</p>

<h5>Перезапуск сервисов кластера из CLI</h5>

<p class="codedescription">Описание: Перезапуск сервисов кластера из CLI</p>

<p>Остановка сервисов</p>

<p class="codetext">cluster stop &lt;Service Name&gt;</p>

<p>Запуск остановленных сервисов</p>

<p class="codetext">cluster start&nbsp; #Примечание: Запустятся все остановленные сервисы</p>

<h5>Запуск сервисов кластера из CLI</h5>

<p class="codedescription">Описание: Start stopped cluster services from the CLI</p>

<p>Запуск остановленных сервисов</p>

<p class="codetext">cluster start&nbsp; #Примечание: Все остановленные сервисы будут запущены</p>

<p>или</p>

<p>Запуск выделенного сервиса</p>

<p class="codetext">Запуск конкретного сервиса: cluster start&nbsp; &lt;Service Name&gt;</p>

<h5>Перезапуск локальных сервисов из CLI</h5>

<p class="codedescription">Описание: Перезапуск одного из локальных сервисов кластера из CLI</p>

<p>Остановка сервиса</p>

<p class="codetext">genesis stop &lt;Service Name&gt;</p>

<p>Запуск сервиса</p>

<p class="codetext">cluster start</p>

<h5>Запуск локальных сервисов из CLI</h5>

<p class="codedescription">Описание: Запуск остановленных сервисов кластера из CLI</p>

<p class="codetext">cluster start #Примечание: команда запустит все остановленные сервисы</p>

<h5>Добавление узла через cmdline</h5>

<p class="codedescription">Описание: добавление узла в кластер из CLI</p>

<p class="codetext">ncli cluster discover-nodes | egrep "Uuid" | awk '{print $4}' | xargs -I UUID ncli cluster add-node node-uuid=UUID</p>

<h5>Отображение id кластера</h5>

<p class="codedescription">Описание: Показать ID текущего кластера</p>

<p class="codetext">zeus_config_printer | grep cluster_id</p>

<h5>Открытие порта</h5>

<p class="codedescription">Описание: Открытие порта через IPtables</p>

<p class="codetext">sudo vi /etc/sysconfig/iptables
<br>
-A INPUT -m state --state NEW -m tcp -p tcp --dport &lt;PORT&gt; -j ACCEPT
<br>
sudo service iptables restart</p>

<h5>Проверка Теневых копий</h5>

<p class="codedescription">Описание: Отображение Теневых копий в формате:&nbsp; name#id@svm_id</p>

<p class="codetext">vdisk_config_printer | grep '#'</p>

<h5>Сброс статистики по задержкам</h5>

<p class="codedescription">Описание: Сброс данных о задержках (&lt;CVM IP&gt;:2009/latency)</p>

<p class="codetext">allssh "wget 127.0.0.1:2009/latency/reset"</p>

<h5>Отображение информации о vDisk</h5>

<p class="codedescription">Описание: Отображение информации о vDisk, включая информацию о имени, id, размере, iqn и так далее</p>

<p class="codetext">vdisk_config_printer</p>

<h5>Отображение количества объектов на РХД (vDisks)</h5>

<p class="codedescription">Описание: Отображения количество существующих на данный момент объектов на РХД</p>

<p class="codetext">vdisk_config_printer | grep vdisk_id | wc -l</p>

<h5>Получение детально информации о vDisk</h5>

<p class="codedescription">Описание: Отображение ID egroup vDisks-ов, размер, преобразование данных и экономию пространства, размещение реплик и так далее</p>

<p class="codetext">vdisk_usage_printer -vdisk_id=&lt;VDISK_ID&gt;</p>

<h5>Запуск сканирования сервиса Curator через CLI</h5>

<p class="codedescription">Описание: Запуск сканирования сервиса Curator через CLI</p>

<p class="codetext"># Полное сканирование <br />
  allssh "wget -O - "http://localhost:2010/master/api/client/StartCuratorTasks?task_type=2";"</p>
<p class="codetext"># Частичное сканирование <br />
  allssh "wget -O - "http://localhost:2010/master/api/client/StartCuratorTasks?task_type=3";"</p>
<p class="codetext"># Обновление информации <br />
  allssh "wget -O - "http://localhost:2010/master/api/client/RefreshStats";"</p>

<h5>Проверка реплицируемых данных через CLI</h5>

<p class="codedescription">Описание: Проверка реплицируемых данных при помощи curator_cli</p>
<p class="codetext">curator_cli get_under_replication_info summary=true</p>


<h5>Сжатие кольца метаданных</h5>

<p class="codedescription">Описание: Сжатие кольца метаданных</p>

<p class="codetext">allssh "nodetool -h localhost compact"</p>

<h5>Отображение версии NOS</h5>

<p class="codedescription">Описание: Find the NOS&nbsp; version (Примечание: can also be done using NCLI)</p>

<p class="codetext">allssh "cat /etc/nutanix/release_version"</p>

<h5>Отображение версии CVM</h5>

<p class="codedescription">Описание: Find the CVM image version</p>

<p class="codetext">allssh "cat /etc/nutanix/svm-version"</p>

<h5>Ручное снятие хэшей с объектов РХД (vDisk)</h5>

<p class="codedescription">Описание: Ручное снятие хэшей с объектов РХД (для дедупликации)&nbsp; Примечание: функция дедупликации должна быть включена для контейнера</p>

<p class="codetext">vdisk_manipulator –vdisk_id=&lt;vDisk ID&gt; --operation=add_fingerprints</p>

<h5>Ручное снятие хэшей всех объектов РХД (vDisk)</h5>

<p class="codedescription">Описание: Ручное снятие хэшей всех объектов РХД (для дедупликации)&nbsp; Примечание: функция дедупликации должна быть включена для контейнера</p>

<p class="codetext">for vdisk in `vdisk_config_printer | grep vdisk_id | awk  {'print $2'}`; do vdisk_manipulator -vdisk_id=$vdisk --operation=add_fingerprints; done</p>

<h5>Вывод данных Factory_Config.json для всех узлов кластера</h5>

<p class="codedescription">Описание: Выводит содержимое factory_config.jscon для всех узлов кластера</p>

<p class="codetext">allssh "cat /etc/nutanix/factory_config.json"</p>

<h5>Обновление версии NOS для конкретного узла кластера</h5>

<p class="codedescription">Описание: Обновление версии NOS для конкретного узла кластера</p>

<p class="codetext">~/cluster/bin/cluster -u &lt;NEW_NODE_IP&gt; upgrade_node</p>

<h5>&nbsp;Список объектов (vDisk) на РХД</h5>

<p class="codedescription">Описание: Список объектов РХД и информации по ним</p>

<p class="codetext">Nfs_ls</p>

<p>Вывод справочной информации</p>

<p class="codetext">Nfs_ls --help</p>

<h5>Установка утилит Nutanix Cluster Check (NCC)</h5>

<p class="codedescription">Описание: Установка утилиты Nutanix Cluster Check (NCC), которые выполняют тестирование состояния кластера</p>

<p>Загрузка NCC с портала поддержки Nutanix (portal.nutanix.com)</p>

<p>SCP .tar.gz в директорию /home/nutanix</p>

<p>Разархивация NCC .tar.gz</p>

<p class="codetext">tar xzmf &lt;ncc .tar.gz file name&gt; --recursive-unlink</p>

<p>Запуск скрипта инсталляции</p>

<p class="codetext">./ncc/bin/install.sh -f &lt;ncc .tar.gz file name&gt;</p>

<p>Создание ссылок</p>

<p class="codetext">source ~/ncc/ncc_completion.bash
<br>
echo "source ~/ncc/ncc_completion.bash" &gt;&gt; ~/.bashrc</p>

<h5>Запуск NCC</h5>

<p class="codedescription">Описание: Запуск утилиты Nutanix Cluster Check (NCC), которые выполняют тестирование состояния кластера.&nbsp; Это действие должно быть первым этапом диагностики кластера / ПО / оборудования.</p>

<p>Убедитесь, что NCC установлены</p>

<p>Запустите проверки NCC</p>

<p class="codetext">ncc health_checks run_all</p>

<h5>Проверьте статус выполнения задач</h5>

<p class="codetext">progress_monitor_cli -fetchall</p>

<h5>Вы можете удалить задачу используя эту же утилиту</h5>

<p class="codetext">progress_monitor_cli --entity_id=&lt;ENTITY_ID&gt; --operation=&lt;OPERATION&gt; --entity_type=&lt;ENTITY_TYPE&gt; --delete<br />
# Примечание: все данные требуется вводить в нижнем регистре, символ k нужно удалить</p>
</section>

<section data-type="sect1" id="metrics-and-thresholds-J5IvTbFw">
<h3>Показатели и пороговые значения</h3>

<p>В следующем разделе будут рассмотрены специфичные метрики и пороговые значения для серверной части Nutanix.&nbsp; Раздел скоро будет обновлен</p>
</section>

<section data-type="sect1" id="gflags-4aI2S1Fg">
<h3>Gflags</h3>

<p>Раздел скоро будет обновлен!</p>
</section>

<section data-type="sect1" id="troubleshooting-andamp-advanced-administration-BNI4HzFZ">
<h3>Поиск и устранение неисправностей</h3>

<h5>Поиск журналов Acropolis</h5>

<p class="codedescription">Описание: Поиск журналов Acropolis для кластера</p>

<p class="codetext">allssh "cat ~/data/logs/Acropolis.log"</p>

<h5>Поиск журналов ошибок для кластера</h5>

<p class="codedescription">Описание: Поиск журналов ошибок уровня ERROR</p>

<p class="codetext">allssh "cat ~/data/logs/&lt;COMPONENT NAME or *&gt;.ERROR"</p>

<p>Пример для Stargate</p>

<p class="codetext">allssh "cat ~/data/logs/Stargate.ERROR"</p>

<h5>Поиск журналов фатальных ошибок для кластера</h5>

<p class="codedescription">Описание: Поиск журналов ошибок уровня FATAL</p>

<p class="codetext">allssh "cat ~/data/logs/&lt;COMPONENT NAME or *&gt;.FATAL"</p>

<p>Пример для Stargate</p>

<p class="codetext">allssh "cat ~/data/logs/Stargate.FATAL"</p>

<section data-type="sect2" id="using-the-2009-page-stargate-gnIoCmh3FP">
<h4>Using the 2009 Page (Stargate)</h4>

<p>В большинстве случаев, Prism должен быть в состоянии дать вам всю необходимую информацию и данные.
  &nbsp;Однако,&nbsp;в определенных сценариях, или если вам нужны более подробные данные, вы можете использовать страницу Stargate - 2009.
  &nbsp;Для перехода на данную страницу используйте &lt;IP-адрес CVM&gt;:2009</p>

<div data-type="note" class="note" id="accessing-back-end-pages-JZiaukClH9F5"><h6>Примечание</h6>
<h5>Доступ к внутренним страницам</h5>

<p>Если вы находитесь в другом сегменте сети (подсеть L2), вам нужно добавить правило в IPtables для доступа к внутренним страницам.</p>
</div>

<p>В верхней части страницы находится общие сведения, которые показывают различные данные о кластере:</p>

<figure id="id-J2tbSkClH9F5"><img alt="2009 Page - Stargate Overview" class="iimagesv22009pagesstargate_overview2png" src="imagesv2/2009Pages/stargate_overview2.png">
<figcaption><span class="label">Рисунок 14-1. </span>Страница 2009 - Общие данные</figcaption>
</figure>

<p>В этом разделе есть две ключевые области, на которые я обращаю внимание, первая - очереди ввода-вывода, тут показано количество выполненных/невыполненных операций.</p>

<p>На рисунке показана секция с общими данными:</p>

<figure id="id-OQtPtBCgHJF9"><img alt="2009 Page - Stargate Overview - Queues" class="iimagesv22009pagesstargate_io_queuespng" src="imagesv2/2009Pages/stargate_io_queues.png">
<figcaption><span class="label">Рисунок 14-2. </span>Страница 2009 - очереди ввода-вывода</figcaption>
</figure>

<p>Вторая часть-это сведения о едином кэше, которые показывают сведения о размерах кэша и частоте обращений.</p>

<p>На рисунке показана секция с данными о едином кэше:</p>

<figure id="id-rQtAcNCJHDF9"><img alt="2009 Page - Stargate Overview - Unified Cache" class="iimagesv22009pagesstargate_contentcache2png" src="imagesv2/2009Pages/stargate_contentCache2.png">
<figcaption><span class="label">Рисунок 14-3. </span>Страница 2009 - Единый кэш</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-vAiPIXC9HxF5"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>В идеальных случаях частота обращений должна быть выше 80-90%+, если рабочая нагрузка является большой так обеспечивается наилучшая производительность для чтения.</p>
</div>

<p>Примечание: значения указаны для каждого экземпляра Stargate / CVM</p>

<p>Следующий раздел 'Cluster State' - показывает подробные данные о различных Stargates в кластере и уровень использования ими дисков.</p>

<p>На рисунке показаны сервисы Stargates и информация по использованию дисков&nbsp;(доступно/всего):</p>

<figure class="large" id="id-yrtysdCvHzFk"><img alt="2009 Page - Cluster State - Disk Usage" class="iimagesv22009pagesstargate_diskutilpng" src="imagesv2/2009Pages/stargate_diskUtil.png">
<figcaption><span class="label">Рисунок 14-4. </span>Страница 2009 - Использование дисков</figcaption>
</figure>

<p>Следующий раздел 'NFS Slave' показывает различную статистику по vDisk.</p>

<p>На рисунке показаны объекты РХД - vDisk и информация по I/O:</p>

<figure class="large" id="id-YatzSeCwHRFm"><img alt="2009 Page - NFS Slave - vDisk Stats" class="iimagesv22009pagesstargate_vdiskstatpng" src="imagesv2/2009Pages/stargate_vdiskStat.png">
<figcaption><span class="label">Рисунок 14-5. </span>Страница 2009 -  статистика по объектам РХД</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-EaigHoCDHoFE"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>При рассмотрении любых потенциальных проблем производительности я всегда смотрю на следующее:</p>

<ol>
	<li>Avg. latency</li>
	<li>Avg. op size</li>
	<li>Avg. outstanding</li>
</ol>

<p>Страница vdisk_stats содержит более детальную информацию.</p>
</div>
</section>
<section data-type="sect2" id="using-the-2009vdisk_stats-page-EqIdsRHzFo">
<h4>Использование страницы 2009/vdisk_stats</h4>

<p>Страница 2009 vdisk_stats -это подробная страница, которая предоставляет дополнительные данные для каждого vDisk. &nbsp;Эта страница включает в себя данные о задержках, размерах операций I/O и типах нагрузки.</p>

<p>Вы можете перейти на страницу vdisk_stats, нажав на "vDisk ID" в левом столбце.</p>

<p>На странице показан пример информации по vDisk:</p>

<figure class="large" id="id-4mt2S0seHkFY"><img alt="2009 Page - Hosted vDisks" class="iimagesv22009pagesstargate_hostedvdiskbriefpng" src="imagesv2/2009Pages/stargate_hostedVdiskBrief.png">
<figcaption><span class="label">Рисунок 14-6. </span>Страница 2009 - подробная информация по объектам расходов</figcaption>
</figure>

<p>Перейдя на страницу vdisk_stats вы получите доступ ко всем данным статистики по vDisk. 
  &nbsp;Примечание: Данные на странице обновляются в реальном времени, чтобы получить текущие данные - обновите страницу.</p>

<p>Основная секция страницы - 'Ops and Randomness', показывает шаблоны операций I/O - случайные или последовательные.</p>

<p>На рисунке показан пример секции 'Ops and Randomness':</p>

<figure id="id-b8tRf2sBHpFJ"><img alt="2009 Page - vDisk Stats - Ops and Randomness" class="iimagesv22009pagesstargate_opsrandomnesspng" src="imagesv2/2009Pages/stargate_opsRandomness.png">
<figcaption><span class="label">Рисунок 14-7. </span>Страница 2009 - секция Ops and Randomness</figcaption>
</figure>

<p>Следующая секция показывает информацию по задержкам при операции чтения и записи от ВМ</p>

<p>На рисунке показан пример секции 'Frontend Read Latency':</p>

<figure class="large" id="id-WmtNI9sGH2Fm"><img alt="2009 Page - vDisk Stats - Frontend Read Latency" class="iimagesv22009pagesstargate_readlat_fepng" src="imagesv2/2009Pages/stargate_readLat_FE.png">
<figcaption><span class="label">Рисунок 14-8. </span>Страница 2009 - Задержки при чтении</figcaption>
</figure>

<p>На рисунке показан пример секции 'Frontend Write Latency':</p>

<figure class="large" id="id-jRtph0smHaFr"><img alt="2009 Page - vDisk Stats - Frontend Write Latency" class="iimagesv22009pagesstargate_writelat_fepng" src="imagesv2/2009Pages/stargate_writeLat_FE.png">
<figcaption><span class="label">Рисунок 14-9. </span>Страница 2009 - Задержки при записи</figcaption>
</figure>

<p>Следующая секция показывает размеры операций I/O.</p>

<p>На рисунке показан пример секции 'Read Size Distribution':</p>

<figure class="large" id="id-wntMupsQHQF1"><img alt="2009 Page - vDisk Stats - Read I/O Size" class="iimagesv22009pagesstargate_readsizepng" src="imagesv2/2009Pages/stargate_readSize.png">
<figcaption><span class="label">Рисунок 14-10. </span>Страница 2009 - размер операций чтения</figcaption>
</figure>

<p>На рисунке показан пример секции 'Write Size Distribution':</p>

<figure class="large" id="id-qMt9Sys5HQFB"><img alt="2009 Page - vDisk Stats - Write I/O Size" class="iimagesv22009pagesstargate_writesizepng" src="imagesv2/2009Pages/stargate_writeSize.png">
<figcaption><span class="label">Рисунок 14-11. </span>Страница 2009 - размер операций записи</figcaption>
</figure>

<p>Секция 'Working Set Size' показывает размеры рабочих наборов за последние 2 минуты и 1 час.&nbsp;Информация делится на данные по операциям чтения и записи.</p>

<p>На рисунке показан пример секции  'Working Set Sizes':</p>

<figure id="id-aztmtms9HWFZ"><img alt="2009 Page - vDisk Stats - Working Set" class="iimagesv22009pagesstargate_workingsetpng" src="imagesv2/2009Pages/stargate_workingSet.png">
<figcaption><span class="label">Рисунок 14-12. </span>Страница 2009 - рабочий набор</figcaption>
</figure>

<p>Секция 'Read Source' показывает детальную информацию данные из какого уровня хранения исходят обслуживаемые операции I/O.</p>

<p>На рисунке показан пример секции 'Read Source':</p>

<figure id="id-oPtmcRsRH3Fm"><img alt="2009 Page - vDisk Stats - Read Source" class="iimagesv22009pagesstargate_readsourcepng" src="imagesv2/2009Pages/stargate_readSource.png">
<figcaption><span class="label">Рисунок 14-13. </span>Страница 2009 - Источники чтения</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-dbi8IEsnHWF4"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Если вы видите высокие задержки при операциях чтения - обратите внимания, что является источником и где обслуживается данный vDisk. &nbsp;В большинстве случаев, высокие задержки могут возникать при чтении с HDD.</p>
</div>

<p>В секции 'Write Destination' показано куда направлены операции записи.</p>

<p>На рисунке показан пример секции 'Write Destination':</p>

<figure id="id-Vrt9CNs5HzFk"><img alt="2009 Page - vDisk Stats - Write Destination" class="iimagesv22009pagesstargate_writedestpng" src="imagesv2/2009Pages/stargate_writeDest.png">
<figcaption><span class="label">Рисунок 14-14. </span>Страница 2009 - Write Destination</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-xkiVsosqHzF5"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>
  Случайные операции I/O будут записаны в Oplog, последовательные операции I/O будут выполняться напрямую на Хранилище экстентов.</p>
</div>

<p>Еще одна интересная точка наблюдения - данные, которые переносятся из HDD на SSD через ILM. 
  &nbsp;Таблица 'Extent Group Up-Migration' показаны данные, которые были перенесены в последние 300, 3,600 и 86,400 секунд.</p>

<p>На рисунке показан пример секции 'Extent Group Up-Migration':</p>

<figure id="id-0Ot9SZs9HJF3"><img alt="2009 Page - vDisk Stats - Extent Group Up-Migration" class="iimagesv22009pagesstargate_egroupilmpng" src="imagesv2/2009Pages/stargate_eGroupILM.png">
<figcaption><span class="label">Рисунок 14-15. </span>Страница 2009 - Перенос данных</figcaption>
</figure>
</section>
<section data-type="sect2" id="using-the-2010-page-curator-aOIJu5HnFW">
<h4>Использование страницы 2010 (Curator)</h4>

<p>Страница 2010 предоставляет детальную информацию по работе сервиса Curator. &nbsp;Тут показаны детальные данные по задачам, сканированию и другие связанные задачи.&nbsp;</p>

<p>На страницу можно по пасть по адресу http://&lt;IP-адрес CVM&gt;:2010 &nbsp;Примечание: Если вы попали не на мастер сервис - нажмите ссылку 'Curator Master: '. &nbsp;</p>

<p>В верхней части страницы будет показывать различные подробности о мастер-сервисе Curator в том числе - время работы, версии ПО и т.д.</p>

<p>В секции 'Curator Nodes' показана информация о всех узлах кластера, ролях, состояниях. &nbsp; Это будут узлы Curator, по которым распределяются задачи от мастера.</p>

<p>На рисунке показан пример таблицы 'Curator Nodes':</p>

<figure id="id-n3t5FrurHNFB"><img alt="2010 Page - Curator Nodes" class="iimagesv22010pagescurator_nodes2png" src="imagesv2/2010Pages/curator_nodes2.png">
<figcaption><span class="label">Рисунок 14-16. </span>Страница 2010 - Узлы Curator</figcaption>
</figure>

<p>Следующая таблица 'Curator Jobs' покажет выполненные или текущие задачи сервиса.&nbsp;</p>

<p>Есть два основных типа заданий, которые включают частичное сканирование, которое может выполняться каждые 60 минут, и полное сканирование, которое может выполняться каждые 6 часов. &nbsp;Примечание: сроки будут варьироваться в зависимости от загруженности платформы и так далее.</p>

<p><span style="letter-spacing: 0.01em; line-height: 1.3em;">Сканирование будет выполняться периодически по расписанию, но может быть вызвано и событием кластера.</span></p>

<p>Вот некоторые события, которые могут вызывать сканирование:</p>

<ul>
	<li>Периодическое сканирование (обычное состояние)</li>
	<li>Сбой диска / узла / блока</li>
	<li>Дисбаланс ILM</li>
	<li>Дисбаланс дисков / уровней хранения</li>
</ul>

<p>На рисунке показан пример таблицы 'Curator Jobs':</p>

<figure id="id-yrtkhpuvHzFk"><img alt="2010 Page - Curator Jobs" class="iimagesv22010pagescurator_jobs2png" src="imagesv2/2010Pages/curator_jobs2.png">
<figcaption><span class="label">Рисунок 14-17. </span>Страница 2010 - Curator Jobs</figcaption>
</figure>

<p>В таблице показаны некоторые высокоуровневые действия, выполняемые в рамках задач:</p>

<table border="1" cellpadding="1" cellspacing="1" style="width: 100%;">
	<thead>
		<tr>
			<th scope="col"><strong>Активность</strong></th>
			<th scope="col"><strong>Полное сканирование</strong></th>
			<th scope="col"><strong>Частичное сканирование</strong></th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th scope="row">ILM</th>
			<td>X</td>
			<td>X</td>
		</tr>
		<tr>
			<th scope="row">Disk Balancing</th>
			<td>X</td>
			<td>X</td>
		</tr>
		<tr>
			<th scope="row">Компрессия</th>
			<td>X</td>
			<td>X</td>
		</tr>
		<tr>
			<th scope="row">Дедупликация</th>
			<td>X</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<th scope="row">Избыточное кодирование</th>
			<td>X</td>
			<td>&nbsp;</td>
		</tr>
		<tr>
			<th scope="row">Очистка мусора</th>
			<td>X</td>
			<td>&nbsp;</td>
		</tr>
	</tbody>
</table>

<p>Нажимая на 'Execution id' вы можете попасть на страницу с детальными данными по задачам.</p>

<p>В верхней части страницы отображаются различные сведения о задании, включая тип и причину задачи, продолжительность выполнения.</p>

<p>В таблице "Background Task Stats" отображаются различные сведения о типе задач, их количестве и приоритете.</p>

<p>На рисунке показан пример таблицы:</p>

<figure id="id-aztPFgu9HWFZ"><img alt="2010 Page - Curator Job - Details" class="iimagesv22010pagesjob_details2png" src="imagesv2/2010Pages/job_details2.png">
<figcaption><span class="label">Рисунок 14-18. </span>Страница 2010 - Подробности по задачам</figcaption>
</figure>

<p>На рисунке показан пример таблицы 'Background Task Stats':</p>

<figure id="id-9otmfbuNH3Fx"><img alt="2010 Page - Curator Job - Tasks" class="iimagesv22010pagesjob_tasks2png" src="imagesv2/2010Pages/job_tasks2.png">
<figcaption><span class="label">Рисунок 14-19. </span>Страница 2010 - Задачи</figcaption>
</figure>

<p>В таблице 'MapReduce Jobs' показаны актуальные задачи MapReduce запущенные каждым сервисом Curator. &nbsp;Частичное сканирование - это одна задача MapReduce, полное сканирование запустит четыре задач MapReduce.</p>

<p>На рисунке показан пример таблицы 'MapReduce Jobs':</p>

<figure class="large" id="id-ZptlI1uvHkFQ"><img alt="2010 Page - MapReduce Jobs" class="iimagesv22010pagescurator_mrjobs2png" src="imagesv2/2010Pages/curator_mrjobs2.png">
<figcaption><span class="label">Рисунок 14-20. </span>Страница 2010 - Задачи MapReduce</figcaption>
</figure>

<p>При нажатии на 'Job id' вы перейдете на страницу с детальной информацией о задачах MapReduce. Тут будет показан статус задач,&nbsp;различные счетчики и другие подробные данные.</p>

<p>На рисунке показан пример некоторых счетчиков:</p>

<figure id="id-zmt5CAuxHlFm"><img alt="2010 Page - MapReduce Job - Counters" class="iimagesv22010pagesjob_counters2png" src="imagesv2/2010Pages/job_counters2.png">
<figcaption><span class="label">Рисунок 14-21. </span>Страница 2010 - Счетчики</figcaption>
</figure>

<p>Следующая часть страницы содержит секции 'Queued Curator Jobs' и 'Last Successful Curator Scans'. Таблицы показывают Эти таблицы показывают, когда задачи периодического сканирования будут запущены и информация по последнему успешному сканированию.</p>

<p>На рисунке показаны секции 'Queued Curator Jobs' и 'Last Successful Curator Scans':</p>

<figure class="large" id="id-0OtyTyu9HJF3"><img alt="2010 Page - Queued and Successful Scans" class="iimagesv22010pagescurator_queue_lastsuccessful2png" src="imagesv2/2010Pages/curator_queue_lastsuccessful2.png">
<figcaption><span class="label">Рисунок 14-22. </span>Страница 2010 - Запланированные и выполненные задачи сканирования</figcaption>
</figure>
<!-- End of Using the 2010 (curator) page section -->
</section>

<section data-type="sect2" id="using-the-curator-cli-aOasdfq234FW">
<h4>Расширенная информация CLI</h4>
<p>Интерфейс Prism обеспечивает всей необходимой информацией для устранения неполадок и мониторинга производительности. Однако, могут быть случаи, где вы хотите получить более подробную информацию, которая имеется на служебных страницах, упомянутых выше, или через CLI.</p>

<h5>vdisk_config_printer</h5>
<p>
  Команда vdisk_config_printer покажет информацию по всем vdisk в кластере.
</p>

<p>
  Я выделил некоторые важные поля для вывода данной команды:
</p>

<ul>
  <li>
    Vdisk ID
  </li>
  <li>
    Vdisk name
  </li>
  <li>
    Parent vdisk ID (if clone or snapshot)
  </li>
  <li>
    Vdisk size (Bytes)
  </li>
  <li>
    Container id
  </li>
  <li>
    To remove bool (to be cleaned up by curator scan)
  </li>
  <li>
    Mutability state (mutable if active r/w vdisk, immutable if snapshot)
  </li>
</ul>

<p>
  Ниже приведен пример вывода команды:
</p>

<pre>
<p class="codetext">
nutanix@NTNX-13SM35210012-A-CVM:~$ vdisk_config_printer | more
...
vdisk_id: 1014400
vdisk_name: "NFS:1314152"
parent_vdisk_id: 16445
vdisk_size: 40000000000
container_id: 988
to_remove: true
creation_time_usecs: 1414104961926709
mutability_state: kImmutableSnapshot
closest_named_ancestor: "NFS:852488"
vdisk_creator_loc: 7
vdisk_creator_loc: 67426
vdisk_creator_loc: 4420541
nfs_file_name: "d12f5058-f4ef-4471-a196-c1ce8b722877"
may_be_parent: true
parent_nfs_file_name_hint: "d12f5058-f4ef-4471-a196-c1ce8b722877"
last_modification_time_usecs: 1414241875647629
...
</p>
</pre>

<h5>vdisk_usage_printer -vdisk_id=&lt;VDISK ID&gt;</h5>
<p>
  Команда vdisk_usage_printer предоставляет детальную информацию по vdisk, его экстентам extents и egroups.
</p>
<p>
  Я выделил некоторые важные поля для вывода данной команды:
</p>

<ul>
  <li>
    Egroup ID
  </li>
  <li>
    Egroup extent count
  </li>
  <li>
    Untransformed egroup size
  </li>
  <li>
    Transformed egroup size
  </li>
  <li>
    Transform ratio
  </li>
  <li>
    Transformation type(s)
  </li>
  <li>
    Egroup replica locations (disk/cvm/rack)
  </li>
</ul>

<p>
  Ниже приведен пример вывода команды:
</p>

<pre>
<p class="codetext">
nutanix@NTNX-13SM35210012-A-CVM:~$ vdisk_usage_printer -vdisk_id=99999
    Egid  # eids  UT Size    T Size ... T Type  Replicas(disk/svm/rack)
 1256878  64      1.03 MB   1.03 MB ... D,[73 /14/60][184108644 /184108632/60]
 1256881  64      1.03 MB   1.03 MB ... D,[73 /14/60][152 /7/25]
 1256883  64      1.00 MB   1.00 MB ... D,[73 /14/60][184108642 /184108632/60]
 1055651  4       4.00 MB   4.00 MB ... none[76 /14/60][184108643 /184108632/60]
 1056604  4       4.00 MB   4.00 MB ... none[74 /14/60][184108642 /184108632/60]
 1056605  4       4.00 MB   4.00 MB ... none[73 /14/60][152 /7/25]
 ...
</p>
</pre>

<p>
  Примечание: Обратите внимание на размер egroup, дедупликация против ее отсутствия (1 против 4MB). Как упоминалось в разделе 'Data Structure' для дедуплицированных данных, размер 1MB для egroup используется для исключения проблем с фрагментацией, которые могут появляться в связи с дедупликацией данных.
</p>

<h5>curator_cli display_data_reduction_report</h5>
<p>
  Команда curator_cli display_data_reduction_report возвращает детальную информацию о сокращении расхода емкости хранилища для каждого контейнера
</p>
<p>
  Я выделил некоторые важные поля для вывода данной команды:
</p>
<ul>
  <li>
    Container ID
  </li>
  <li>
    Technique (transform applied)
  </li>
  <li>
    Pre reduction Size
  </li>
  <li>
    Post reduction size
  </li>
  <li>
    Saved space
  </li>
  <li>
    Savings ratio
  </li>
</ul>

<p>
    Ниже приведен пример вывода команды:
</p>
<pre>
<p class="codetext">
nutanix@NTNX-13SM35210012-A-CVM:99.99.99.99:~$ curator_cli display_data_reduction_report
Using curator master: 99.99.99.99:2010
E0404 13:26:11.534024 26791 rpc_client_v2.cc:676] RPC connection to 127.0.0.1:9161 was reset: Shutting down
Using execution id 68188 of the last successful full scan
+--------------------------------------------------------------------------------------+
| Container Id | Technique      | Pre Reduction | Post Reduction | Saved     | Ratio   |
+--------------------------------------------------------------------------------------+
| 988          | Clone          | 4.88 TB       | 2.86 TB        | 2.02 TB   | 1.70753 |
| 988          | Snapshot       | 2.86 TB       | 2.22 TB        | 656.92 GB | 1.28931 |
| 988          | Dedup          | 2.22 TB       | 1.21 TB        | 1.00 TB   | 1.82518 |
| 988          | Compression    | 1.23 TB       | 1.23 TB        | 0.00 KB   | 1       |
| 988          | Erasure Coding | 1.23 TB       | 1.23 TB        | 0.00 KB   | 1       |
| 26768753     | Clone          | 764.26 GB     | 626.25 GB      | 138.01 GB | 1.22038 |
| 26768753     | Snapshot       | 380.87 GB     | 380.87 GB      | 0.00 KB   | 1       |
| 26768753     | Dedup          | 380.87 GB     | 380.87 GB      | 0.00 KB   | 1       |
| 26768753     | Compression    | 383.40 GB     | 383.40 GB      | 0.00 KB   | 1       |
| 26768753     | Erasure Coding | 383.40 GB     | 245.38 GB      | 138.01 GB | 1.56244 |
| 84040        | Clone          | 843.53 GB     | 843.53 GB      | 0.00 KB   | 1       |
| 84040        | Snapshot       | 741.06 GB     | 741.06 GB      | 0.00 KB   | 1       |
| 84040        | Dedup          | 741.06 GB     | 741.06 GB      | 0.00 KB   | 1       |
| 84040        | Compression    | 810.44 GB     | 102.47 GB      | 707.97 GB | 7.9093  |
...
+---------------------------------------------------------------------------------------------+
| Container Id | Compression Technique | Pre Reduction | Post Reduction | Saved     | Ratio   |
+---------------------------------------------------------------------------------------------+
| 84040        | Snappy                | 810.35 GB     | 102.38 GB      | 707.97 GB | 7.91496 |
| 6853230      | Snappy                | 3.15 TB       | 1.09 TB        | 2.06 TB   | 2.88713 |
| 12199346     | Snappy                | 872.42 GB     | 109.89 GB      | 762.53 GB | 7.93892 |
| 12736558     | Snappy                | 9.00 TB       | 1.13 TB        | 7.87 TB   | 7.94087 |
| 15430780     | Snappy                | 1.23 TB       | 89.37 GB       | 1.14 TB   | 14.1043 |
| 26768751     | Snappy                | 339.00 MB     | 45.02 MB       | 293.98 MB | 7.53072 |
| 27352219     | Snappy                | 1013.88 MB    | 90.32 MB       | 923.55 MB | 11.2253 |
+---------------------------------------------------------------------------------------------+
</p>
</pre>

<h5>curator_cli get_vdisk_usage lookup_vdisk_ids=&lt;COMMA SEPARATED VDISK ID(s)&gt;</h5>
<p>
  Команда curator_cli display_data_reduction_report возвращает детальную информацию о сокращении расхода емкости хранилища для vDisk
</p>
<p>
  Я выделил некоторые важные поля для вывода данной команды:
</p>
<ul>
  <li>
    Vdisk ID
  </li>
  <li>
    Exclusive usage (Data referred to by only this vdisk)
  </li>
  <li>
    Logical uninherited (Data written to vdisk, may be inherited by a child in the event of clone)
  </li>
  <li>
    Logical dedup (Amount of vdisk data that has been deduplicated)
  </li>
  <li>
    Logical snapshot (Data not shared across vdisk chains)
  </li>
  <li>
    Logical clone (Data shared across vdisk chains)
  </li>
</ul>

<p>
    Ниже приведен пример вывода команды:
</p>
<pre>
<p class="codetext">
Using curator master: 99.99.99.99:2010
VDisk usage stats:
+------------------------------------------------------------------------------------------------------+
| VDisk Id  | Exclusive usage | Logical Uninherited | Logical Dedup | Logical Snapshot | Logical Clone |
+------------------------------------------------------------------------------------------------------+
| 254244142 | 596.06 MB       | 529.75 MB           | 6.70 GB       | 11.55 MB         | 214.69 MB     |
| 15995052  | 599.05 MB       | 90.70 MB            | 7.14 GB       | 0.00 KB          | 4.81 MB       |
| 203739387 | 31.97 GB        | 31.86 GB            | 243.09 MB     | 0.00 KB          | 4.72 GB       |
| 22841153  | 147.51 GB       | 147.18 GB           | 0.00 KB       | 0.00 KB          | 0.00 KB       |
...
</p>
</pre>

<h5>curator_cli get_egroup_access_info</h5>
<p>
  Команда curator_cli get_egroup_access_info используется для получения подробной информации о количестве egroups в каждом сегменте на основе последнего чтения / записи. 
  Эта информация может быть использована для оценки количества групп, которые могут быть подходящими кандидатами для использования избыточного кодирования.
</p>
<p>
    Я выделил некоторые важные поля для вывода данной команды:
</p>
<ul>
  <li>
    Container ID
  </li>
  <li>
    Access \ Modify (secs)
  </li>
</ul>

<p>
    Ниже приведен пример вывода команды:
</p>
<pre>
<p class="codetext">
Using curator master: 99.99.99.99:2010
Container Id: 988
+----------------------------------------------------------------------------------------------------------------------------------------+
| Access \ Modify (secs) | [0,300) | [300,3600) | [3600,86400) | [86400,604800) | [604800,2592000) | [2592000,15552000) | [15552000,inf) |
+----------------------------------------------------------------------------------------------------------------------------------------+
| [0,300)                | 345     | 1          | 0            | 0              | 0                | 0                  | 0              |
| [300,3600)             | 164     | 817        | 0            | 0              | 0                | 0                  | 0              |
| [3600,86400)           | 4       | 7          | 3479         | 7              | 6                | 4                  | 79             |
| [86400,604800)         | 0       | 0          | 81           | 7063           | 45               | 36                 | 707            |
| [604800,2592000)       | 0       | 0          | 15           | 22             | 3670             | 83                 | 2038           |
| [2592000,15552000)     | 1       | 0          | 0            | 10             | 7                | 35917              | 47166          |
| [15552000,inf)         | 0       | 0          | 0            | 1              | 1                | 3                  | 144505         |
+----------------------------------------------------------------------------------------------------------------------------------------+
...
</p>
</pre>
<!-- End advanced vdisk info section -->
</section>

</section>
</section>
</div>

<!-- End of Book of Acropolis -->

<div data-type="part" id="book-of-ahv">
<h1><span class="label">Part IV. </span>Книга AHV</h1>

<section data-type="chapter" id="ahv-architecture-NYIMsy">
<h2>Архитектура</h2>

<section data-type="sect1" id="node-architecture-lkIms8h3">
<h3>Архитектура узла</h3>

<p>При использовании AHV, CVM запускается в виде ВМ и дисков, которые подключены к ней напрямую - PCI passthrough.&nbsp; Таким образом контроллер PCI и все диски работают напрямую с CVM, минуя уровень гипервизора.&nbsp; AHV собран на базе CentOS KVM. Для гостевых ВМ используется полная виртуализация аппаратного обеспечения.</p>

<figure id="id-ZptDuNsvHk"><img alt="AHV Node" class="iimagesv2acrop_nodepng" src="imagesv2/acrop_node.png">
<figcaption><span class="label">Рисунок 13-1. </span>Узел AHV</figcaption>
</figure>

<p>AHV реализован на базе CentOS KVM, и расширяет базовую функциональность такими функциями как HA, живая миграция и так далее.&nbsp;</p>

<p>AHV подтвержден по программе Microsoft Server Virtualization Validation Program и сертифицирован для запуска Microsoft OS и приложений на его основе.</p>
</section>

<section data-type="sect1" id="kvm-architecture-M2I0uWHw">
<h3>Архитектура KVM</h3>

<p>KVM состоит из следующих основных компонент:</p>

<ul>
	<li>KVM-kmod
	<ul>
		<li>Модуль ядра</li>
	</ul>
	</li>
	<li>Libvirtd
	<ul>
		<li>API, демон и утилита управления для KVM и QEMU.&nbsp; Все коммуникации между ПО Acropolis и KVM / QEMU выполняются через libvirtd.</li>
	</ul>
	</li>
	<li>Qemu-kvm
	<ul>
		<li>Эмулятор и виртуализатор машины, который выполняется в пространстве пользователя для каждой виртуальной машины (domain).&nbsp; В AHV он используется для аппаратной виртуализации, а виртуальные машины работают как HVM.</li>
	</ul>
	</li>
</ul>

<p>На следующем рисунке показана связь между различными компонентами:</p>

<figure id="id-zmtoSEuxHl"><img alt="KVM Component Relationship" class="iimagesv2kvm_overviewpng" src="imagesv2/kvm_overview.png">
<figcaption><span class="label">Рисунок 13-2. </span>KVM - связь между компонентами</figcaption>
</figure>

<p>Коммуникации между Acropolis и KVM осуществляются через Libvirt.&nbsp;</p>

<div data-type="note" class="note" id="processor-generation-compatibility-AmibF4u9Hx"><h6>Примечание</h6>
<h5>Поддержка разных поколений ЦПУ</h5>

<p>Аналогично функции vMotion (EVC) от компании VMware, которая позволяет виртуальным машинам перемещаться между разными поколениями процессоров; AHV определит наименьшее поколение процессоров в кластере и ограничит все домены QEMU до этого уровня. Это позволяет смешивать поколения процессоров в кластере AHV и обеспечивает возможность динамической миграции между узлами.</p>
</div>

</section>

<section data-type="sect1" id="configuration-maximums-and-scalability-QMImTwHQ">
<h3>Максимумы и масштабируемость</h3>

<p>Применяются следующие максимумы конфигурации и ограничения масштабируемости:</p>

<ul>
	<li>Максимальный размер кластера: <strong>N/A – соответствует максимумам для кластера Nutanix</strong></li>
	<li>Максимальное кол-во ЦПУ на ВМ: <strong>По количеству физических ядер на узле</strong></li>
	<li>Максимальное кол-во ОЗУ на ВМ: <strong>Минимум 4 ТБ или доступной физической памяти узла</strong></li>
  <li>
    Максимальный размер дисков ВМ: <strong>9EB* (Exabyte)</strong>
  </li>
	<li>Максимальное кол-во ВМ на узел: <strong>N/A – По количеству ОЗУ</strong></li>
	<li>Максимальное кол-во ВМ на кластер: <strong>N/A – По количеству ОЗУ</strong></li>
</ul>
</section>

<p>
  *AHV не имеет традиционного для ESXi / Hyper-V уровня работы с хранилищем; все диски презентуются ВМ как сырые блочные устройства SCSI. 
   Это значит, что размер диска ограничевает только максимальный размер файла для РХД - 9 Экзабайт.
</p>

<section data-type="sect1" id="networking-J5IbSVHw">
<h3>Сетевая подсистема</h3>

<p>AHV использует Open vSwitch (OVS) для обеспечения ВМ сетью.&nbsp; Сети ВМ настраиваются через Prism / ACLI, каждый интерфейс VM подключается как tap интерфейс к OVS.</p>

<p>На следующем рисунке показана концептуальная схема архитектуры OVS:</p>

<figure id="id-89t5TmSAHZ"><img alt="Open vSwitch Network Overview" class="iimagesv2acrop_netpng" src="imagesv2/acrop_net.png">
<figcaption><span class="label">Рисунок 13-3. </span>Open vSwitch</figcaption>
</figure>

<h4>Типы сетевых интерфейсов ВМ</h4>
<p>
  AHV поддерживает следующие типы сетевых интерфейсов виртуальных машин:
</p>
<ul>
  <li>
    Access (по умолчанию)
  </li>
  <li>
    Trunk (с версии 4.6 и выше)
  </li>
</ul>
<p>
    По умолчанию сетевые адаптеры виртуальных машин будут создаваться как Access-интерфейсы, однако можно настроить и Trunk-интерфейс.
</p>

<p>
  Транковые интерфейсы могут быть добавлены с помощью следующей команды:
</p>

<p class="codetext">vm.nic_create &lt;VM_NAME&gt; vlan_mode=kTrunked trunked_networks=&lt;ALLOWED_VLANS&gt; network=&lt;NATIVE_VLAN&gt;</p>

<p>
  Например:
</p>

<p class="codetext">vm.nic_create fooVM vlan_mode=kTrunked trunked_networks=10,20,30 network=vlan.10</p>

<!-- End of book of ahv architecture section -->
</section>

<!-- End of book of ahv architecture section -->
</section>

<section data-type="sect1" id="how-it-works-4aIRHwHg">
<h2>Как это работает</h2>

<section data-type="sect2" id="iscsi-multi-pathing-BNIZsRHzHv">
<h3>Пути хранения</h3>

<p>
 AHV не использует традиционного стека по работе с хранилищем, как ESXi или Hyper-V. Все диски передаются виртуальным машинам как сырые блочные устройства SCSI. Это позволяет упростить и оптимизировать путь ввода-вывода.
</p>

<div data-type="note" class="note" id="processor-generation-compatibility-AmibF4u9Hx"><h6>Примечание</h6>
<h5>Примечание</h5>

<p>Acropolis абстрагирует от пользователя такие сущности как kvm, virsh, qemu, libvirt, и iSCSI и берет на себя управление всеми компонентами. Это позволяет пользователю сфокусировать свое внимание на работе с ВМ через Prism / ACLI. Информация ниже представлена в информационных целях, крайне не рекомендуется вручную работать с virsh, libvirt и так далее.</p>
</div>

<p>Каждый узел AHV запускает iSCSI редиректор, который регулярно проверяет работоспособность сервиса Stargate по всему кластеру через команды NOP.</p>

<p>В журнале iscsi_redirector log (расположен в папке /var/log/ на узле AHV), вы можете увидеть статус проверок:</p>

<pre>
<p class="codetext">
2017-08-18 19:25:21,733 - INFO - Portal 192.168.5.254:3261 is up
...
2017-08-18 19:25:25,735 - INFO - Portal 10.3.140.158:3261 is up
2017-08-18 19:25:26,737 - INFO - Portal 10.3.140.153:3261 is up
</p>
</pre>

<p>Примечание: Локальный сервис Stargate доступен по внутреннему адресу 192.168.5.254 .</p>

<p>
 Ниже показано, что iscsi_redirector слушает 127.0.0.1:3261:
</p>

<pre>
<p class="codetext">
[root@NTNX-BEAST-1 ~]# netstat -tnlp | egrep tcp.*3261
Proto ... Local Address   Foreign Address   State     PID/Program name
...
tcp   ... 127.0.0.1:3261  0.0.0.0:*         LISTEN    8044/python
...
</p>
</pre>

<p>QEMU настроен с редиректором iSCSI как  iSCSI-таргет портал. &nbsp; 
  В случае запроса на подключение,редиректор перенаправит этот запрос работоспособному сервису Stargate (локальный в приоритете).</p>

<figure id="id-0OtzTbs9HWH3"><img alt="iSCSI Multi-pathing - Normal State" class="iimagesv2iscsi_mp_1png" src="imagesv2/iscsi_mp_1.png">
<figcaption><span class="label">Рисунок 13-4. </span>iSCSI Multi-pathing - Нормальное состояние</figcaption>
</figure>

<p>

  Так в конфигурации ВМ мы можем видеть следующую секцию:

</p>

<pre>
<p class="codetext">
&lt;devices&gt;
...
&lt;disk type='network' device='lun'&gt;
  &lt;driver name='qemu' type='raw' cache='none' error_policy='report' io='native'/&gt;
  &lt;source protocol='iscsi' name='iqn.2010-06.com.nutanix:vmdisk-16a5../0'&gt;
    &lt;host name='127.0.0.1' port='3261'/&gt;
  &lt;/source&gt;
  &lt;backingStore/&gt;
  &lt;target dev='sda' bus='scsi'/&gt;
  &lt;boot order='1'/&gt;
  &lt;alias name='scsi0-0-0-0'/&gt;
  &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
&lt;/disk&gt;
...
&lt;/devices&gt;
</p>
</pre>

<p>
Предпочтительный тип контроллера - virtio-scsi (стандартный для устройств SCSI). Устройства IDE использовать не рекомендуется. Чтобы использовать virtio для гостевых ОС семейства Windows требуется установить драйверы мобильности или гостевые утилиты Nutanix. Современные дистрибутивы Linux имеют встроенные драйверы virtio.
</p>

<pre>
  <p class="codetext">
...
&lt;controller type='scsi' index='0' model='virtio-scsi'&gt;
 &lt;driver max_sectors='2048'/&gt;
 &lt;alias name='scsi0'/&gt;
 &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
&lt;/controller&gt;
...
  </p>
</pre>

<p>В случае, если активный сервис Stargate становится недоступным, редиректор iSCSI пометит его как сбойный.&nbsp; При попытке QEMU подключиться к iSCSI редиректор перенаправит запрос к рабочему сервису Stargate на другом узле кластера.</p>

<figure id="id-DktkHYsnHrHZ"><img alt="iSCSI Multi-pathing - Local CVM Down" class="iimagesv2iscsi_mp_2png" src="imagesv2/iscsi_mp_2.png">
<figcaption><span class="label">Рисунок 13-5. </span>iSCSI Multi-pathing - Локальная CVM недоступна</figcaption>
</figure>

<p>Как только локальная CVM и сервис Stargate будут восстановлены, удаленный Stargate отключит все соединения для удаленных сессий iSCSI.&nbsp; QEMU выполнит попытку подключения снова и будет отправлен на локальный Stargate.</p>

<figure id="id-GRtbtxsbH9H8"><img alt="iSCSI Multi-pathing - Local CVM Back Up" class="iimagesv2iscsi_mp_3png" src="imagesv2/iscsi_mp_3.png">
<figcaption><span class="label">Рисунок 13-6. </span>iSCSI Multi-pathing - Локальная CVM восстановлена</figcaption>
</figure>

<h4>Стандартный путь I/O</h4>
<p>Как в любом гипервизоре или ОС в AHV представлен целый спектр компонентов работающих в пространстве ядра и пользовательском пространстве. Прежде чем читать дальше рекомендуется прочитать раздел "Пользовательское пространство и пространство ядра", чтобы понимать чем они отличаются.</p>

<p>Когда ВМ выполняет операцию I/O происходит следующее (некоторые шаги были исключены для ясности): </p>
<ol>
  <li>ОС на ВМ выполняет команду SCSI на виртуальном устройстве</li>
  <li>Virtio-scsi принимает запросы и размещает их в ОЗУ гостевой ОС</li>
  <li>Запросы обрабатываются главным циклом QEMU</li>
  <li>Libiscsi проверяет каждый запрос и передает его дальше</li>
  <li>Сетевой уровень пересылает запросы локальному CVM (или удаленному, если локальный недоступен)</li>
  <li>Stargate обрабатывает запросы</li>
</ol>

<p>Ниже представлена примерная схема этого процесса:</p>

<figure id="id-sfadf2"><img alt="AHV VirtIO Data Path - Classic" class="iimage42i_mp_3png" src="imagesv2/frodo_1.png">
<figcaption><span class="label">Рисунок. </span>AHV VirtIO - путь данных</figcaption>
</figure>

<p>

    Глядя на XML домена, мы видим, что он использует эмулятор qemu-kvm:

</p>

<pre>
<p class="codetext">
...
&lt;devices&gt;
&lt;emulator>/usr/libexec/qemu-kvm&lt;/emulator&gt;
...
</p>
</pre>

<p>
  На локальном узле AHV, вы можете увидеть, что qemu-kvm установил сеансы с работающим сервисом Stargate, используя локальный мост и IP.
  Для внешних коммуникаций, будет использован внешний адрес узла, Stargate на котором используется.  Примечание: Будет создано одно подключение на каждое дисковое устройство (см. PID 24845)
</p>

<pre>
<p class="codetext">
[root@NTNX-BEAST-1 log]# netstat -np | egrep tcp.*qemu
Proto ... Local Address       Foreign Address     State       PID/Program name
tcp   ... 192.168.5.1:50410  192.168.5.254:3261   ESTABLISHED 25293/qemu-kvm
tcp   ... 192.168.5.1:50434  192.168.5.254:3261   ESTABLISHED 23198/qemu-kvm
tcp   ... 192.168.5.1:50464  192.168.5.254:3261   ESTABLISHED 24845/qemu-kvm
tcp   ... 192.168.5.1:50465  192.168.5.254:3261   ESTABLISHED 24845/qemu-kvm
...
</p>
</pre>

<p>В этом пути есть несколько неэффективных моментов, так основной цикл является однопоточным и libiscsi проверяет каждую команду SCSI.</p>


<h4>Пути I/O. Frodo</h4>
<p>По мере того как технологии хранения данных продолжают развиваться и становятся более эффективными, должны развиваться и мы. Учитывая тот факт, что мы полностью контролируем AHV и стек Nutanix, это была область возможностей.</p>

<p>В общем, Frodo - это очень сильно оптимизированный путь ввода-вывода для AHV, который обеспечивает более высокую пропускную способность, меньшую задержку и меньшую нагрузку на процессор.</p>

<p>Когда ВМ выполняет оперцию I/O происходит следующее (некоторые шаги были исключены для ясности): </p>
<ol>
  <li>ОС на ВМ выполняет команду SCSI на виртуальном устройстве</li>
  <li>Virtio-scsi принимает запросы и размещает их в ОЗУ гостевой ОС</li>
  <li><b>Запросы обрабатываются Frodo</b></li>
  <li><b>Кастомный libiscsi добавляет iscsi заголовок и перенаправляет запросы</b></li>
  <li>Сетевой уровень пересылает запросы локальному CVM (или удаленному, если локальный недоступен)</li>
  <li>Stargate обрабатывает запросы</li>
</ol>

<p>TНиже представлена примерная схема этого процесса</p>

<figure id="id-sfadf2"><img alt="AHV VirtIO Data Path - Classic" class="iimage42i_mp_3png" src="imagesv2/frodo_2.png">
<figcaption><span class="label">Рисунок. </span>AHV VirtIO - путь данных с использованием Frodo</figcaption>
</figure>

<p>Следующий путь похож на традиционный ввод-вывод, за исключением нескольких ключевых отличий:</p>

<ul>
  <li>Основной цикл Qemu заменен на Frodo (vhost-user-scsi)</li>
  <li>Frodo предоставляет несколько виртуальных очередей (VQ) гостю (по одной на vCPU)</li>
  <li>Использует несколько потоков для виртуальных машин с несколькими vCPU</li>
  <li>Libiscsi заменен нашей собственной гораздо более легкой версией</li>
</ul>

<p>Гостевая ОС заметит, что теперь у него есть несколько очередей для дисковых устройств, кроме того, будет просто заметно улучшение производительности. 
    В некоторых случаях мы видели снижение перерасхода процессора на 25% для выполнения I/O и увеличения производительности до 3x раз по сравнению со стандартными средствами Qemu!
    По сравнению с другим гипервизором мы видели падение нагрузки на процессор для операций I/O до 3 раз.</p>

<p>На узле AHV, вы увидите процесс frodo для каждой запущенной виртуальной машины (процесс qemu-kvm):</p>

<pre>
<p class="codetext">
[root@drt-itppc03-1 ~]# ps aux | egrep frodo
... /usr/libexec/qemu-kvm ... -chardev socket,id=frodo0,fd=3 \
  -device vhost-user-scsi-pci,chardev=frodo0,num_queues=16...

... /usr/libexec/frodo ... 127.0.0.1:3261 -t iqn.2010-06.com.nutanix:vmdisk...
...
</p> 
</pre>

<p>

    Если посмотреть XML конфигурацию ВМ, мы видим, что он использует frodo:

</p>

<pre>
<p class="codetext">
...
&lt;devices&gt;
&lt;emulator>/usr/libexec/frodo&lt;/emulator&gt;
...
</p>
</pre>

<div data-type="note" class="note" id="pro-tip-21irfmTqHRHb"><h6>Примечание</h6>
<h5>Совет от создателей</h5>
<p>Чтобы воспользоваться преимуществами нескольких потоков обеспечиваемых Frodo, необходимо иметь >= 2 vCPU для целевой ВМ.</p>

<p>Это можно охарактеризовать:</p>
<ul>
  <li>
    1 vCPU UVM:
    <ul><li>1 поток Frodo / сеанс на дисковое устройство</li></ul>
  </li>
  <li>>= 2 vCPU UVM:
    <ul><li>2 потока Frodo / сеансов на дисковое устройство</li></ul>
  </li>
</ul>
</div>

<p>
    На локальном узле AHV, вы можете увидеть, что qemu-kvm установил сеансы с работающим сервисом Stargate, используя локальный мост и IP.
    Для внешних коммуникаций, будет использован внешнеий адрес узла, Stargate на котором используется. 
</p>

<pre>
<p class="codetext">
[root@NTNX-BEAST-1 log]# netstat -np | egrep tcp.*frodo
Proto ... Local Address       Foreign Address     State       PID/Program name
tcp   ... 192.168.5.1:39568  192.168.5.254:3261   ESTABLISHED 42957/frodo
tcp   ... 192.168.5.1:39538  192.168.5.254:3261   ESTABLISHED 42957/frodo
tcp   ... 192.168.5.1:39580  192.168.5.254:3261   ESTABLISHED 42957/frodo
tcp   ... 192.168.5.1:39592  192.168.5.254:3261   ESTABLISHED 42957/frodo
...
</p>
</pre>

<!-- end of ahv storage io section -->
</section>

<section data-type="sect2" id="ahv-microseg">
<h3>Микросегментация</h3>
<p>Микросегментация AHV это распределенный межсетевой экран который обеспечивает гранулярное разделение сетей между запущенными ВМ и внешний доступ к ним, а так же мониторинг их сетей</p>

<p>Конфигурация выполняется через интерфейс Prism Central посредством определения политик и назначения их категориям. 
  Т.е. настройка для всех кластеров Nutanix может выполняться централизовано. Настройки отправляются на кластеры, а затем каждый узел AHV применяет их как правила OpenFlow.</p>
<p>
    Микросегментация использует следующие понятия/компоненты:
</p>

<h5>Категории</h5>
<p>Категории используются для определения групп элементов, к которым применяются политики и правила.</p>
    <ul>
      <li>Key/Value "Tag"</li>
      <li>Examples: app | tier | group | location | subnet | etc.</li>
    </ul>

<h5>Правила безопасности</h5>
<p>Правила безопасности определяют, что разрешено между определенными категориями.</p>

<figure id="id-125221251"><img alt="Micro-segmentation - Rules" src="imagesv2/microseg_rule.png">
<figcaption><span class="label">Рисунок. </span>Микросегментация - правила</figcaption>
</figure>

<p>Есть несколько основных типов правил безопасности:</p>
<ul>
  <li>App Rule
    <ul>
      <li>Это основное правило, позволяющее определить транспорт (TCP/UDP), порт, источник/назначение и разрешение/запрет.</li>
      <li>[Allow|Deny] Transport: Port(s) [To|From]</li>
      <li>Пример: Разрешить трафик на порт TCP 8080 от Category:Tier:Web к Category:Tier:App</li>
    </ul>
  <li>Isolation Rule
  <ul>
    <li>Запрещение любого трафика между двумя категориями, разрешение трафика внутри категории</li>
    <li>Пример: Разделение пользователя A от пользователя B, клонируйте настройки и позвольте им работать параллельно без какого либо взаимного влияния на сеть.</li>
  </ul>
  </li>

  <li>Quarantine Rule
  <ul>
    <li>Запрещение всего трафика для специфических ВМ/категорий</li>
    <li>Пример: ВМ A,B,C заражены вирусом, их можно изолировать для исключения влияния на остальные ВМ в сети и на саму сеть</li>
  </ul>
  </li>
</ul>

<p>Ниже приведен пример использования микросегментации для управления трафиком приложения:</p>

<figure id="id125251"><img alt="Micro-segmentation - Example Application" src="imagesv2/microseg_example.png">
<figcaption><span class="label">Рисунок. </span>Микросегментация - Пример приложения</figcaption>
</figure>

<h5>Применение</h5>
<p>Применение определяет, какое действие выполняется при применении правила. Для AHV, микросегментация имеет два типа исполнения:</p>

<ul>
  <li>
    Apply
    <ul>
      <li>Если есть совпадения с правилом - применять его</li>
    </ul>
  </li>
  <li>
    Monitor
    <ul>
      <li>Наблюдение за взаимодействием, в том числе за соответствием правилам</li>
    </ul>
  </li>
</ul>

<p>Правила микросегментации применяются применяются к пакету как только он покидает пользовательскую ВМ:</p>

<figure id="id-12591251"><img alt="Micro-segmentation - Flow" src="imagesv2/network_flow.png">
<figcaption><span class="label">Рисунок. </span>Микросегментация - Потоки</figcaption>
</figure>

</section>

<section data-type="sect2" id="ahv-service-chain">
<h3>Цепочка обслуживания</h3>
<p>Цепочки обслуживания позволяют нам перехватывать весь трафик и направлять его обработчику пакетов (NFV, аппаратный или виртуальный апплаенс), это делается прозрачно и выглядит как часть сетевого пути.</p>

<p>Стандартные сценарии использования:</p>
<ul>
  <li>Межсетевые экраны (Palo Alto и пр.)</li>
  <li>Балансировщики трафика (F5, Netscaler и пр.)</li>
  <li>IDS/IPS/Сетевой мониторинг</li>
</ul>

<p>В цепочке обслуживания существует два типа пакетных процессоров:</p>

<figure id="id-125214151"><img alt="Service chain - Packet Processors" src="imagesv2/servicechain_both.png">
<figcaption><span class="label">Рисунок. </span>Цепочка обслуживания - Обработчик пакетов</figcaption>
</figure>

<ul>
  <li>Inline обработчик пакетов
  <ul>
    <li>Перехватывает пакеты на лету, как поток через OVS</li>
    <li>Может изменять, пропускать и отбрасывать пакеты</li>
    <li>Пример использования: Межсетевые экраны и балансировщики трафика</li>
  </ul>
  </li>
  <li>
    Tap обработчик пакетов
    <ul>
      <li>Может исследовать содержимое пакеты, т.е. имеет возможность только чтения пакетов в потоке</li>
      <li>Пример использования: IDS/IPS/Сетевой мониторинг</li>
    </ul>
  </li>
</ul>

<p>Ниже показано высокоуровневое представление сетевого пути и цепочки обслуживания:</p>

<p>Все цепочки обслуживания применяются после применения правил микросегментации, и до того как покинут локальный OVS:</p>

<figure id="id-12591251"><img alt="Service Chain - Flow" src="imagesv2/network_flow.png">
<figcaption><span class="label">Рисунок. </span>Цепочка обслуживания - Поток</figcaption>
</figure>

<p>Также возможно объединить несколько обработчиков пакетов в одну цепочку:</p>

<figure id="id-12591a251"><img alt="Service Chain - Multi Packet Processor" src="imagesv2/servicechain_multi.png">
<figcaption><span class="label">Рисунок. </span>Цепочка обслуживания - Несколько обработчиков пакетов</figcaption>
</figure>

</section>

<section data-type="sect2" id="ip-address-management-ONIyujHgHJ">
<h3>Управление IP-адресами</h3>

<p>Встроенная система управления ip-адресами (IPAM) предоставляет возможность определения диапазонов DHCP и назначения IP-адресов виртуальным машинам.&nbsp; Данная система использует правила VXLAN и OpenFlow для перехвата запросов DHCP от ВМ и формирует ответы DHCP для них.</p>

<p>Ниже показан пример запроса DHCP обработанного при помощи встроенного IPAM, когда сервис Acropolis Master размещен локально:</p>

<figure id="id-A0tmT4u9HgHo"><img alt="IPAM - Local Acropolis Master" class="iimagesv2acrop_ipam_1png" src="imagesv2/acrop_ipam_1.png">
<figcaption><span class="label">Рисунок 13-7. </span>IPAM - Локальный сервис Acropolis Master</figcaption>
</figure>

<p>Если Acropolis Master запущен на удаленном узле, тот же тоннель VXLAN будет использован для передачи запроса по сети.&nbsp;</p>

<figure id="id-k1tjH0u5HXHG"><img alt="IPAM - Remote Acropolis Master" class="iimagesv2acrop_ipam_2png" src="imagesv2/acrop_ipam_2.png">
<figcaption><span class="label">Рисунок 13-8. </span>IPAM - Удаленный сервис Acropolis Master</figcaption>
</figure>

<p>Классические решения DHCP / IPAM могут использоваться в "неуправляемых" сетях.</p>
</section>

<section data-type="sect2" id="vm-high-availability-ha-nrIkT2HrHN">
<h3>Высокая доступность ВМ (HA)</h3>
<p>
  Функция HA реализована для того, чтобы обеспечить работу ВМ в случае выхода из строя узла или блока. В случае такого события, ВМ будет перезапущена на одном из функционирующих узлов в кластере. За перезапуск ВМ  отвечает сервис Acropolis Master.
</p>

<p>
  Мастер Acropolis отслеживает работоспособность узла, отслеживая его подключения к libvirt на всех узлах кластера::
</p>

<figure id="id-DktlTvTnHrHZ"><img alt="HA - Host Monitoring" src="imagesv2/ha_hostmonitoring.png">
<figcaption><span class="label">Рисунок 13-9. </span>HA - Мониторинг узлов</figcaption>
</figure>

<p>
  Если текущий Acropolis Master изолирован или отказал - будет запущен новый мастер-сервис на одном из узлов кластера. Если вдруг одна часть узлов кластера оказалась изолированной - функционирующей будет считаться та часть узлов, где может быть набран кворум. 
  Как только кворум набран - будет предпринята попытка перезапуска ВМ на этих узлах.
</p>

<p>
	Существует два типа режимов HA для ВМ:
</p>

<ul>
	<li>
		Стандартный
		<ul>
			<li>
        Этот режим не требует настройки и включается по умолчанию при установке кластера Nutanix на основе AHV.
        Когда узел AHV становится недоступным, виртуальные машины, работающие на отказавшем узле AHV, перезапускаются на оставшихся узлах в зависимости от доступных ресурсов.
        Не все сбойные виртуальные машины перезапускаются, если на оставшихся узлах недостаточно ресурсов.
			</li>
		</ul>
	</li>
	<li>
		Гарантированный
		<ul>
			<li>
        Эта конфигурация по умолчанию резервирует место на узлах AHV в кластере, чтобы гарантировать, что все отказавшие виртуальные машины могут перезапуститься на других узлах в кластере AHV во время сбоя узла. Чтобы включить гарантийный режим, установите флажок 'Enable HA', как показано на рисунке ниже. После этого появляется сообщение, отображающее объем зарезервированной памяти и допустимое количество сбоев узла AHV.
			</li>
		</ul>
	</li>
</ul>

<h4>Резервирование ресурсов</h4>

<p>При использовании Гарантированного режима обеспечения высокой доступности ВМ система резервирует ресурсы узла для переезда виртуальных машин. 
  Объем зарезервированных ресурсов суммируется следующим образом:</p>

<ul>
  <li>Если все контейнеры настроены в режиме RF2 (FT1)
    <ul><li>резервируются ресурсы одного узла</li></ul>
  </li>
  <li>Если любые контейнеры настроены в режиме RF3 (FT2)
    <ul><li>резервируются ресурсы двух узлов</li></ul>
  </li>
</ul>

<p>Когда узлы в кластере имеют разный объем памяти, система будет использовать наибольший объем памяти узла при расчете резервируемых ресурсов.</p>

<div data-type="note" class="note" id="pro-tip-mEiEtPTrHdHM"><h6>Примечание</h6>
<h5>Резервирование ресурсов после версии 5.0</h5>

<p>До версии 5.0 мы поддерживали и резервирование узлов и резервирование сегментов ресурсов.
  После версии 5.0 доступно только резервирование на базе сегментов, которое начинает применяться сразу после включения Гарантированного режима обеспечения HA.</p>

<p></p>
</div>

<p>
  Cегменты резервирования распределяют резервирование ресурсов по всем узлам кластера. В этом случае - каждый узел обеспечивает какие-то ресурсы для машин к которым применяется сценарий HA. Это гарантирует, что общая емкость зарезервированного пространства кластера будет достаточной для перезапуска виртуальных машин в случае сбоя узла.
</p>

<p>
  На рисунке показан пример сценария с зарезервированными сегментами:
</p>

<figure id="id-m2tqiPTrHdHM"><img alt="HA - Reserved Segment" src="imagesv2/ha_reservedsegment1.png">
<figcaption><span class="label">Рисунок 13-13. </span>HA - Резервирование сегментов ресурсов</figcaption>
</figure>

<p>
  В случае сбоя узла виртуальные машины будут перезапущены в кластере на оставшихся работоспособных узлах:
</p>

<figure id="id-vOt4INT9HEH5"><img alt="HA - Reserved Segment - Fail Over" src="imagesv2/ha_reservedsegment2.png">
<figcaption><span class="label">Рисунок 13-14. </span>HA - Резервирование сегментов ресурсов - обработка отказа</figcaption>
</figure>

<div data-type="note" class="note" id="reserved-segments-calculation-eriXUNTQh3H8"><h6>Примечание</h6>
<h5>Расчет резервируемого пространства при использовании резервирования сегментов ресурсов</h5>
<p>
  Система автоматически рассчитает общее количество зарезервированных сегментов и резервирование узла.
</p>

<p>
  Поиск ресурсов сводится к хорошо известному набору проблем, называемому Knapsack. Оптимальным решением является - экспоненциальное (NP-hard), но эвристические решения могут быть близки к оптимальным в большинстве случаев. Мы реализуем алгоритм, называемый MTHM. Nutanix продолжит совершенствовать свои алгоритмы для размещения ВМ и резервирования ресурсов.
</p>

</div>

</section>
</section>

<section data-type="sect1" id="administration-BNIyFRHZ">
<h2>Администрирование</h2>

<p>Больше в ближайшее время!</p>
</section>

<section data-type="sect1" id="important-pages-ONIPtjHA">
<h3>Important Pages</h3>

<p>Больше в ближайшее время!</p>
</section>

<section data-type="sect1" id="command-reference-nrIef2Hx">
<h3>Справочник по командам</h3>

<h5>Включение только 10GbE интерфейсов в OVS</h5>

<p class="codedescription">Описание: Включение только 10g интерфейсов в bond0 на локальном узле</p>

<p class="codetext">manage_ovs --interfaces 10g update_uplinks</p>
<p class="codedescription">Описание: отображение аплинков OVS для всего кластера</p>
<p class="codetext">
allssh "manage_ovs --interfaces 10g update_uplinks"
</p>

<h5>Отображение аплинков OVS</h5>

 <p class="codedescription">Описание: Отображение аплинк интерфейсов OVS для локального узла</p>

<p class="codetext">manage_ovs show_uplinks</p>

 <p class="codedescription">Описание: Отображение аплинк интерфейсов OVS для всего кластера</p>

<p class="codetext">allssh "manage_ovs show_uplinks"</p>

<h5>Отображение интерфейсов OVS</h5>

 <p class="codedescription">Описание: Отображение интерфейсов OVS локального узла</p>

<p class="codetext">manage_ovs show_interfaces</p>

<p>Отображение интерфейсов для всего кластера</p>

<p class="codetext">allssh "manage_ovs show_interfaces"</p>

<h5>Отображение конфигурации OVS</h5>

 <p class="codedescription">Описание: Отображение конфигурации виртульного коммутатора</p>

<p class="codetext">ovs-vsctl show</p>

<h5>Список бридж-интерфейсов OVS </h5>

 <p class="codedescription">Описание: Список бридж-интерфейсов/p>

<p class="codetext">ovs-vsctl list br</p>

<h5>Отображение информации о бридж-интерфейсе OVS</h5>

 <p class="codedescription">Описание: Отображение информации о бридж-интерфейсе OVS</p>

<p class="codetext">ovs-vsctl list port br0
<br>
ovs-vsctl list port &lt;bond&gt;</p>

<h5>Отображение информации о интерфейсах OVS</h5>

 <p class="codedescription">Описание: Отображение информации о интерфейсе</p>

<p class="codetext">ovs-vsctl list interface br0</p>

<h5>Отображение портов/интерфейсов на бридже</h5>

<p class="codedescription">Описание: Отображение портов на бридже</p>

<p class="codetext">ovs-vsctl list-ports br0</p>

<p class="codedescription">Описание: Отображение интерфейсов на бридже</p>

<p class="codetext">ovs-vsctl list-ifaces br0</p>

<h5>Создание бридж-интерфейса OVS</h5>

<p class="codedescription">Описание: Создание бридж-интерфейса</p>

<p class="codetext">ovs-vsctl add-br &lt;bridge&gt;</p>

<h5>Добавление порта в бридж</h5>

 <p class="codedescription">Описание: Добавление порта в бридж</p>

<p class="codetext">ovs-vsctl add-port &lt;bridge&gt; &lt;port&gt;</p>

<p class="codedescription">Описание: Добавление порта в бридж</p>

<p class="codetext">ovs-vsctl add-bond &lt;bridge&gt; &lt;port&gt; &lt;iface&gt;</p>

<h5>Отображение детальной информации о bond-интерфейсе OVS</h5>

<p class="codedescription">Описание: Отображение детальной информации о bond-интерфейсе</p>

<p class="codetext">ovs-appctl bond/show &lt;bond&gt;</p>

<p>Пример:</p>

<p class="codetext">ovs-appctl bond/show bond0</p>

<h5>Настройка режима bond и LACP на bond-интерфейсе</h5>

<p class="codedescription">Описание: Включение LACP на портах</p>

<p class="codetext">ovs-vsctl set port &lt;bond&gt; lacp=&lt;active/passive&gt;</p>

<p class="codedescription">Описание: Настройка всех узлов на использование bond0</p>

<p class="codetext">for i in `hostips`;do echo $i; ssh $i source /etc/profile &gt; /dev/null 2&gt;&amp;1; ovs-vsctl set port bond0 lacp=active;done</p>

<h5>Отображение настроек LACP для bond-интерфейса</h5>

<p class="codedescription">Описание: Отображение настроек LACP </p>

<p class="codetext">ovs-appctl lacp/show &lt;bond&gt;</p>

<h5>Установка режима bond-интерфейса</h5>

<p class="codedescription">Описание: Настройка режимов bond-интерфейса</p>

<p class="codetext">ovs-vsctl set port &lt;bond&gt; bond_mode=&lt;active-backup, balance-slb, balance-tcp&gt;</p>

<h5>Отображение информации OpenFlow</h5>

<p class="codedescription">Описание: Отображение детальных настроек openflow для OVS</p>

<p class="codetext">ovs-ofctl show br0</p>

<p class="codedescription">Описание: Отображение правил OpenFlow</p>

<p class="codetext">ovs-ofctl dump-flows br0</p>

<h5>Получение PID процессов QEMU и информации по ним</h5>

<p class="codedescription">Описание: Получение ID процессов QEMU</p>

<p class="codetext">ps aux | grep qemu | awk '{print $2}'</p>

<p class="codedescription">Описание: Получение основных метрик для конкретных PID</p>

<p class="codetext">top -p &lt;PID&gt;</p>

<h5>Получение активного Stargate для процессов QEMU</h5>

<p class="codedescription">Описание: Получение активного Stargate для процессов QEMU</p>

<p class="codetext">netstat –np | egrep tcp.*qemu</p>
</section>

<section data-type="sect1" id="metrics-and-thresholds-b1IPiXH5">
<h3>Метрики и пороговые значения</h3>

<p>Этот раздел скоро будет дополнен!</p>
</section>

<section data-type="sect1" id="troubleshooting-andamp-advanced-administration-rkIAcyHO">
<h3>Поиск и устранение неисправностей</h3>

<h5>Проверка журналов iSCSI редиректора</h5>

<p class="codedescription">Описание: Проверка журнало iSCSI редиректора на всех узлах</p>

<p class="codetext">for i in `hostips`; do echo $i; ssh root@$i cat /var/log/iscsi_redirector;done</p>

<p>Пример для одного узла</p>

<p class="codetext">Ssh root@&lt;HOST IP&gt;
<br>
Cat /var/log/iscsi_redirector</p>

<h5>Мониторинг украденного процессорного времени CPU (stolen CPU)</h5>

<p class="codedescription">Описание: Мониторинг украденного процессорного времени CPU (stolen CPU)</p>

<p>Запустить утилиту top и искать %st (выделено ниже)</p>

<p class="codetext">Cpu(s):&nbsp; 0.0%us, 0.0%sy,&nbsp; 0.0%ni, 96.4%id,&nbsp; 0.0%wa,&nbsp; 0.0%hi,&nbsp; 0.1%si,&nbsp; <strong>0.0%st</strong></p>

<h5>Мониторинг статистики сетевых интерфейсов виртуальных машин</h5>

<p class="codedescription">Описание: Мониторинг статистики ресурсов виртуальных машин</p>

<p>Запустить virt-top</p>

<p class="codetext">Virt-top</p>

<p>Перейти на страницу о сети</p>

<p>2 – Networking</p>
</section>
</section>

<!-- End of AHV section -->

<div data-type="part" id="book-of-vsphere-7aBig">
<h1><span class="label">Часть V. </span>Книга vSphere</h1>

<section data-type="chapter" id="architecture-NYIMsy">
<h2>Архитектура</h2>

<section data-type="sect1" id="node-architecture-22IRsYsQ">
<h3>Архитектура узла</h3>

<p>При использовании ESXi в качестве гипервизора, CVM запускается как ВМ, а диски презентуются ей с использованием функции VMDirectPath I/O.&nbsp; Таким образом контроллер PCI и все диски работают напрямую с CVM, минуя уровень гипервизора.</p>

<figure id="id-9otduEsDsX"><img alt="ESXi Node Architecture" class="iimagesv2esx_nodepng" src="imagesv2/esx_node.png">
<figcaption><span class="label">Рисунок 15-1. </span>Архитектура узла с ESXi</figcaption>
</figure>
</section>

<section data-type="sect1" id="configuration-maximums-and-scalability-3jIAuksQ">
    <h3>Максимумы и масштабируемость</h3>

    <p>Применяются следующие максимумы конфигурации и ограничения масштабируемости:</p>
    
    <ul>
      <li>Максимальный размер кластера: <strong>64</strong></li>
      <li>Максимальное кол-во ЦПУ на ВМ: <strong>128</strong></li>
      <li>Максимальное кол-во ОЗУ на ВМ: <strong>4TB</strong></li>
      <li>
        Максимальный размер дисков ВМ: <strong>62TB</strong>
      </li>
      <li>Максимальное кол-во ВМ на узел: <strong>1,024</strong></li>
      <li>Максимальное кол-во ВМ на кластер: <strong>8,000 (по 2,048 на Datastore, когда HA активирован)</strong></li>
    </ul>

<p>Примечание: Начиная с версии vSphere 6.0</p>

<div data-type="note" class="note" id="pro-tip-05i5c2349"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>При выполнениии нагрузочного тестирования на узлах с ESXi убедитесь, что настройки питания гипервизора установлены в 'High performance'. Этот режим отключает статусы P- и C- и гарантирует, что никакие лимиты не влияют на выполнение тестирования.</p>
</div>
</section>

<section data-type="sect1">
<h3>Сеть</h3>

<p>Каждый узел ESXi имеет локальный vSwitch, который используется для связи локальной CVM Nutanix и узла ESXi. Для внешней связи и сети виртуальных машин используется стандартный vSwitch (по умолчанию) или dvSwitch.</p>

<p>
  Локальный vSwitch (vSwitchNutanix) предназначен для локальной связи между CVM Nutanix и узлом ESXi. Узел имеет интерфейс vmkernel на vSwitch (vmk1 - 192.168.5.1) и CVM имеет интерфейс, привязанный к группе портов на этом внутреннем коммутаторе (svm-iscsi-pg - 192.168.5.2). Это основной канал связи с хранилищем.
</p>

<p>Внешний vSwitch может быть стандартным vSwitch или dvSwitch. Он будет отвечать за внешние интерфейсы для узла ESXi и CVM, а также группы портов, используемые виртуальными машинами на данном узле.
  Внешний интерфейс vmkernel используется для управления узлом, процессов vMotion и так далее.  Внешний интерфейс CVM используется для связи с другими CVM Nutanix.
  Групп портов может быть создано столько, сколько требуется. При этом предполагается, что VLAN на узел поданы в транке.</p>

<p>На следующем рисунке показана концептуальная схема использования vSwitch:</p>

<figure><img alt="ESXi vSwitch Network Overview" src="imagesv2/esxi_net.png">
<figcaption><span class="label">Рисунок. </span>ESXi vSwitch - сеть</figcaption>
</figure>

<div data-type="note" class="note" id="pro-tip-vAiPIXC9HxF5"><h6>Примечание</h6>
<h5>Аплинки и политики группировки инитерфейсов</h5>

<p>Для обеспечения высокой доступности рекомендуется использования двух ToR коммутаторов, и распределения аплинков узлов между ними. По умолчанию аплинки настроены в режиме активный/пассивный. Для некоторых коммутаторов можно использовать конфигурацию в режиме активный/активный, для обеспечения большей пропускной способности.</p>
</div>
</section>

</section>

<section data-type="chapter" id="how-it-works-22IBuk">
<h2>Как это работает</h2>

<section data-type="sect1" id="array-offloads-vaai-3jImsMuQ">
<h3>VAAI</h3>

<p>Платформа Nutanix поддерживает API VMWare для интеграции с массивами (VAAI), это позволяет перенести часть задач с гипервизора на хранилище данных.&nbsp; 
  Это обеспечивает высокую эффективность, так как гипервизору не надо быть "человеком по середине".
  На текущий момент Nutanix поддерживает примитивы VAAI для хранилищ NAS, включая - ‘full file clone’, ‘fast file clone’, и ‘reserve space’.&nbsp; 
  Вот хороший документ, который описывает некоторые примитивы: http://cormachogan.com/2012/11/08/vaai-comparison-block-versus-nas/</p>

<p>Для всех типов создания клонов дисков РХД будет выполнять 'fast clone', т.е. создает снимок доступный для записи (используя re-direct on write) для каждого созданного клона.&nbsp; 
  Каждый клон имеет свою собственную карту блоков, а значит не стоит беспокоится о глубине цепочки снимков. 
  Ниже описаны события, для которых будет или нет использоваться VAAI:</p>

<ul>
	<li>Клонирование ВМ с созданием снимка –&gt; VAAI НЕ используется</li>
	<li>Клонирование выключенной ВМ без создания снимка –&gt; VAAI используется</li>
	<li>Клонирование ВМ на другой контейнер –&gt;  VAAI НЕ используется</li>
	<li>Клонирование выключенной ВМ &nbsp; –&gt;  VAAI НЕ используется</li>
</ul>

<p>Следующие сценарии применимы для VMware View:</p>

<ul>
	<li>Просмотр полного клона (Шаблон со снимком) –&gt;  VAAI НЕ используется</li>
	<li>Просмотр полного клона (Шаблон без снимка) –&gt; VAAI используется</li>
	<li>Просмотр линкованного клона (VCAI) –&gt; VAAI используется</li>
</ul>

<p>Можно проверить, что операции VAAI выполняются в разделе "NFS Adapter" страницы "Activity Traces".</p>
</section>

<section data-type="sect1" id="cvm-autopathing-aka-hapy-PgIJuQub">
<h3>Переключение путей данных CVM он же Ha.py</h3>

<p>В этом разделе я расскажу, как обрабатываются "сбои" CVM (я расскажу, как мы будем обрабатываем сбои компонентов в будущем).&nbsp; Сбой CVM может включать выключение CVM пользователем, установка обновления на CVM или любое событие, которое могло бы выключить CVM.
  РХД имеет возможности по автоматизации переключения путей данных, в случае недоступности локальной CVM. Операции I/O будет перенаправлены на другу CVM в кластере.
  Гипервизор и CVM взаимодействуют используя приватную сеть 192.168.5.0 и выделенный vSwitch.&nbsp; Т.е. все операции I/O с хранилищем, будут отправлены локально на адрес CVM (192.168.5.2).&nbsp; 
  Внешний IP-адрес CVM будет использоваться для операций репликации или взаимодействия с другими CVM.</p>

<p>На следующем рисунке показан пример того, как это выглядит::</p>

<figure id="id-ZptjTxupuG"><img alt="ESXi Host Networking" class="iimagesv2esx_hapy_1png" src="imagesv2/esx_hapy_1.png">
<figcaption><span class="label">Рисунок 16-1. </span>ESXi Сеть</figcaption>
</figure>

<p>В случае, если локальная CVM недоступна, ее адрес 192.168.5.2 также станет недоступным.&nbsp; 
    РХД автоматически определит, что это случилось и перенаправит все операции I/Os другим CVM кластера через сеть 10GbE.&nbsp; 
    Перенаправление операций прозрачно для гипервизора и ВМ на узле.&nbsp; 
    Это значит, если CVM выключена, ВМ продолжают работать и могут писать и читать данные с РХД.
    Как только локальная CVM вернулась в рабочее состояние - трафик будет перенаправлен обратно на локальную CVM.</p>
  
  <p>На следующем рисунке показано графическое представление того, как это выглядит выход из строя локальной CVM:</p>
  

<figure class="large" id="id-zmtMFEuPuR"><img alt="ESXi Host Networking - Local CVM Down" class="iimagesv2esx_hapy_2png" src="imagesv2/esx_hapy_2.png">
<figcaption><span class="label">Рисунок 16-2. </span>ESXi Сеть - Отказ локальной CVM</figcaption>
</figure>
</section>
</section>

<section data-type="chapter" id="administration-3jI3Tg">
<h2>Администрирование</h2>

<section data-type="sect1" id="important-pages-PgIys4Tb">
<h3>Важные страницы</h3>

<p>Скоро!</p>
</section>

<section data-type="sect1" id="command-reference-lkIBuxTJ">
<h3>Справочник по командам</h3>

<h5>Обновление кластера ESXi</h5>

 <p class="codedescription">Описание: Выполнение автоматизированного обновления узлов ESXi через CLI и специализированный набор ПО
<br>
# Загрузка набора ПО на Nutanix CVM
<br>
# Подключение к Nutanix CVM
<br>
# Выполнение обновления</p>

<p class="codetext">cluster --md5sum=&lt;bundle_checksum&gt; --bundle=&lt;/path/to/offline_bundle&gt; host_upgrade</p>

<p># Например</p>

<p class="codetext">cluster --md5sum=bff0b5558ad226ad395f6a4dc2b28597 --bundle=/tmp/VMware-ESXi-5.5.0-1331820-depot.zip host_upgrade</p>

<h5>Перезапуск сервисов узла ESXi</h5>

 <p class="codedescription">Описание: Перезапуск сервисов всех узлов ESXi</p>

<p class="codetext">for i in `hostips`;do ssh root@$i "services.sh restart";done</p>

<h5>Отображение состояний сетевых интерфейсов ESXi в статусе ‘Up’</h5>

 <p class="codedescription">Описание: Отображение состояний сетевых интерфейсов ESXi в статусе ‘Up’</p>

<p class="codetext">for i in `hostips`;do echo $i &amp;&amp; ssh root@$i esxcfg-nics -l | grep Up;done</p>

<h5>Отображение статуса интерфейсов 10GbE узлов ESXi </h5>

 <p class="codedescription">Описание: Отображение статуса интерфейсов 10GbE узлов ESXi</p>

<p class="codetext">for i in `hostips`;do echo $i &amp;&amp; ssh root@$i esxcfg-nics -l | grep ixgbe;done</p>

<h5>Отбражение активных сетевых адаптеров ESXi</h5>

 <p class="codedescription">Описание: Отбражение активных, неактивных и отключенных сетевых адаптеров ESXi</p>

<p class="codetext">for i in `hostips`;do echo $i &amp;&amp;&nbsp; ssh root@$i "esxcli network vswitch standard policy failover get --vswitch-name vSwitch0";done</p>

<h5>Отбражение таблицы маршрутизации узлов ESXi</h5>

 <p class="codedescription">Описание: Отбражение таблицы маршрутизации узлов ESXi</p>

<p class="codetext">for i in `hostips`;do ssh root@$i 'esxcfg-route -l';done</p>

<h5>Проверка статуса VAAI</h5>

 <p class="codedescription">Описание: Проверка статуса VAAI</p>

<p class="codetext">vmkfstools -Ph /vmfs/volumes/&lt;Datastore Name&gt;</p>

<h5>Разрешение работы с VIB от сообщества</h5>

 <p class="codedescription">Описание: Разрешение работы с VIB от сообщества</p>

<p class="codetext">esxcli software acceptance set --level CommunitySupported</p>

<h5>Установка VIB</h5>

 <p class="codedescription">Описание: Установка VIB без цифровой подписи</p>

<p class="codetext">esxcli software vib install --viburl=/&lt;VIB directory&gt;/&lt;VIB name&gt; --no-sig-check</p>

<p># Или</p>

<p class="codetext">esxcli software vib install --depoturl=/&lt;VIB directory&gt;/&lt;VIB name&gt; --no-sig-check</p>

<h5>Проверка емкости рамдиска ESXi</h5>

 <p class="codedescription">Описание: Проверка емкости рамдиска ESXi</p>

<p class="codetext">for i in `hostips`;do echo $i; ssh root@$i 'vdf -h';done</p>

<h5>Очистка журналов pynfs</h5>

 <p class="codedescription">Описание: Очистка журналов pynfs на каждом узле ESXi</p>

<p class="codetext">for i in `hostips`;do echo $i; ssh root@$i '&gt; /pynfs/pynfs.log';done</p>
</section>

<section data-type="sect1" id="metrics-and-thresholds-M2I5TRTv">
<h3>Метрики и пороговые значения</h3>

<p>Скоро!</p>
</section>
</section>

<section data-type="sect1" id="troubleshooting-andamp-advanced-administration-PgIPS5">
<h3>Поиск и устранение неисправностей</h3>

<p>Скоро!</p>
</section>
</div>

<div data-type="part" id="book-of-hyper-v-7pdi1">
<h1><span class="label">Часть VI. </span>Книга Hyper-V</h1>

<section data-type="chapter" id="architecture-22IRsV">
<h2>Архитектура</h2>

<p>
При создании кластера Nutanix на базе Hyper-V, узлы Hyper-V автоматически присоединяются к указанному домену Windows Active Directory. Затем эти узлы помещаются в отказоустойчивый кластер для обеспечения высокой доступности ВМ. После этого для каждого отдельного узла Hyper-V и отказоустойчивого кластера будут созданы объекты AD.
</p>

<section data-type="sect1" id="node-architecture-3jImsksn">
<h3>Архитектура узла</h3>

<p>При использовании гипервизра Hyper-V, CVM запускается как ВМ, диски к ней подключаются напрямую (passthrough).</p>

<figure id="id-oPtruYsWsB"><img alt="Hyper-V Node Architecture" class="iimagesv2hyperv_nodepng" src="imagesv2/hyperv_node.png">
<figcaption><span class="label">Рисунок 18-1. </span>Архитектура узла Hyper-V</figcaption>
</figure>
</section>

<section data-type="sect1" id="configuration-maximums-and-scalability-PgIJu8s2">
<h3>Configuration Maximums and Scalability</h3>

<h3>Максимумы и масштабируемость</h3>

<p>Применяются следующие максимумы конфигурации и ограничения масштабируемости:</p>

<ul>
  <li>Максимальный размер кластера: <strong>64</strong></li>
  <li>Максимальное кол-во ЦПУ на ВМ: <strong>64</strong></li>
  <li>Максимальное кол-во ОЗУ на ВМ: <strong>1TB</strong></li>
  <li>
    Максимальный размер дисков ВМ: <strong>64TB</strong>
  </li>
  <li>Максимальное кол-во ВМ на узел: <strong>1,024</strong></li>
  <li>Максимальное кол-во ВМ на кластер: <strong>8,000</strong></li>
</ul>

<p>Примечание: Начиная с версии Hyper-V 2012 R2</p>

<section data-type="sect1">
<h3>Сеть</h3>

<p>Каждый узел Hyper-V имеет внутренний виртуальный коммутатор, который используется для связи узла и локальной CVM. Для внешней связи и виртуальных машин используется внешний виртуальный коммутатор (по умолчанию) или логический коммутатор.</p>

<p>
    Внутренний коммутатор (InternalSwitch) предназначен для локальной связи между CVM Nutanix и узлом Hyper-V.
    Узел имеет виртуальный интерфейс Ethernet (vEth) на внутреннем коммутаторе (192.168.5.1), CVM подключена сюда же (192.168.5.2).
    Это основной канал связи с хранилищем.
</p>

<p>Внешний vSwitch может являться стандартным виртуальным коммутатором или логическим коммутатором. 
  Здесь будут размещены внешние интерфейсы для узла Hyper-V и CVM, а также логические сети и сети виртуальных машин, используемые виртуальными машинами на данном узле.
  Внешний vEth интерфейс используется для управления узлов, операций живой миграции ВМ и так далее. Внешний интерфейс CVM используется для взаимодействия с другими CVM кластера.
  Логических сетей может быть создано столько, сколько требуется. При этом предполагается, что VLAN поданы на узлы в режиме транка.</p>

<p>На следующем рисунке показана концептуальная схема архитектуры виртуального коммутатора :</p>

<figure><img alt="Hyper-V Virtual Switch Network Overview" src="imagesv2/hyperv_net.png">
<figcaption><span class="label">Рисунок. </span>Виртуальный коммутатор Hyper-V</figcaption>
</figure>

<div data-type="note" class="note"><h6>Примечание</h6>
  <h5>Аплинки и политики группировки интерфейсов</h5>

  <p>Рекомендуется использовать два коммутатора ToR, и подключать каждый узел кластера к каждому из них для обеспечения бесперебойной работы сети.  
    По умолчанию система имеет группу интерфейсов LBFO, в зависимости от режима коммутаторы должны быть настроены соответствующим образом</p>  
</div>
</section>
</section>

<section data-type="chapter" id="how-it-works-3jIAul">
<h2>Как это работает</h2>

<section data-type="sect1" id="array-offloads-odx-PgIysQu2">
<h3>ODX</h3>

<p>Платформа Nutanix поддерживает функции Microsoft Offloaded Data Transfers (ODX), они позволяют перенести часть нагрузки с гипервизора на хранилище.&nbsp; 
    Это намного эффективнее, так как гипервизор исключен из цепочки при работе с данными хранилища.
    На текущий момент Nutanix поддерживает примитивы ODX для работы с SMB, которые включают в себя операции 'full copy' и 'zeroing'.&nbsp; 
    Однако, в отличие от VAAI, который имеет функцию 'fast file' для работы с клонами (использует снимки доступные для записи), примитивы ODX не имеют альтернатив для выполнения полных копий данных.&nbsp; 
    Исходя из этого - наиболее эффективно выполнять клонирование встроенными механизмами, вызывая их через  nCLI, REST, или PowerShell CMDlets.   
  
  ODX используется для следующих операций:</p>

<ul>
	<li>Копирование файлов внутри ВМ или между ВМ, когда они расположены на общем ресурсе SMB РХД</li>
	<li>Копирование файлов на/с общего ресурса SMB</li>
</ul>

<p>Развертывание шаблона из библиотеки SCVMM (общий ресурс SMB РХД) – Примечание: Общие ресурсы должны быть добавлены в кластер SCVMM с помощью коротких имен (не FQDN).&nbsp; 
   Простой способ - принудительно добавить запись в файл hosts для кластера (например: 10.10.10.10&nbsp;&nbsp;&nbsp;&nbsp; nutanix-130).</p>

<p>ODX не используется для следующих операций:</p>

<ul>
	<li>Клонирование VM через SCVMM</li>
	<li>Развертывание шаблона из библиотеки SCVMM (если хранилище не на РХД)</li>
	<li>Клонирование в рамках XenDesktop</li>
</ul>

<p>Вы можете увидеть операции ODX на странице Activity Traces, в секции ‘NFS Adapter’. (да, секция называется ‘NFS Adapter’, несмотря на то, что в случае HyperV это SMB).&nbsp; 
   Данные по операциям копирования - ‘NfsSlaveVaaiCopyDataOp‘, данные по операциям 'zeroing' - ‘NfsSlaveVaaiWriteZerosOp‘ .</p>
</section>
</section>

<section data-type="chapter" id="administration-PgIYTe">
<h2>Администрирование</h2>

<section data-type="sect1" id="important-pages-lkImsxTa">
<h3>Важные страницы</h3>

<p>Скоро!</p>
</section>

<section data-type="sect1" id="command-reference-M2I0uRT0">
<h3>Справочник по командам</h3>

<h5>Выполнение команды на нескольких узлах</h5>

<p class="codedescription">Описание: Выполнение команд PowerShell на одном или нескольких узлах</p>

<p class="codetext">$targetServers = "Host1","Host2","Etc"
<br>
Invoke-Command -ComputerName&nbsp; $targetServers {
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;COMMAND or SCRIPT BLOCK&gt;
<br>
}</p>

<h5>Проверка доступных VMQ Offloads</h5>

<p class="codedescription">Описание: Проверка доступных VMQ offloads для конкретного узла</p>

<p class="codetext">gwmi –Namespace "root\virtualization\v2" –Class Msvm_VirtualEthernetSwitch | select elementname, MaxVMQOffloads</p>

<h5>Отключение VMQ для конкретных ВМ по префиксу</h5>

<p class="codedescription">Описание: Отключение VMQ для конкретных ВМ</p>

<p class="codetext">$vmPrefix = "myVMs"
<br>
Get-VM | Where {$_.Name -match $vmPrefix} | Get-VMNetworkAdapter | Set-VMNetworkAdapter -VmqWeight 0</p>

<h5>Включение VMQ для конкретных ВМ по префиксу</h5>

<p class="codedescription">Описание: Включение VMQ для конкретных ВМ</p>

<p class="codetext">$vmPrefix = "myVMs"
<br>
Get-VM | Where {$_.Name -match $vmPrefix} | Get-VMNetworkAdapter | Set-VMNetworkAdapter -VmqWeight 1</p>

<h5>Включение питания ВМ по префиксу</h5>

<p class="codedescription">Описание: Включение ВМ по префиксу</p>

<p class="codetext">$vmPrefix = "myVMs"
<br>
Get-VM | Where {$_.Name -match $vmPrefix -and $_.StatusString -eq "Stopped"} | Start-VM</p>

<h5>Выключение питания ВМ по префиксу</h5>

<p class="codedescription">Описание: Выключение ВМ по префиксу</p>

<p class="codetext">$vmPrefix = "myVMs"
<br>
Get-VM | Where {$_.Name -match $vmPrefix -and $_.StatusString -eq "Running"}} | Shutdown-VM -RunAsynchronously</p>

<h5>Остановка ВМ по префиксу</h5>

<p class="codedescription">Описание: Остановка ВМ по префиксу</p>

<p class="codetext">$vmPrefix = "myVMs"
<br>
Get-VM | Where {$_.Name -match $vmPrefix} | Stop-VM</p>

<h5>Получение настроек RSS на узле Hyper-V</h5>

<p class="codedescription">Описание: Получение настроек RSS (recieve side scaling) на узле Hyper-V</p>

<p class="codetext">Get-NetAdapterRss</p>

<h5>Проверка соединений Winsh и WinRM</h5>

<p class="codedescription">Описание: Проверка соединений Winsh и WinRM</p>

<p class="codetext">allssh "winsh "get-wmiobject win32_computersystem"</p>
</section>

<section data-type="sect1" id="metrics-and-thresholds-QMImTzTx">
<h3>Метрики и пороговые значения</h3>

<p>Скоро!</p>
</section>
</section>

<section data-type="sect1" id="troubleshooting-andamp-advanced-administration-lkI8SA">
<h3>Поиск и устранение неисправностей</h3>

<p>Скоро!</p>
</section>
</div>

<div data-type="part" id="book-of-xs">
<h1><span class="label">Часть VII. </span>Книга XenServer</h1>
<p>
    Скоро!
</p>
</div>

<div data-type="part" id="book-of-foundation">
<h1><span class="label">Часть VIII. </span>Книга Foundation</h1>

<section data-type="chapter">
<h2>Архитектура</h2>

<p>
  Foundation - утилита для первоначального развертывания кластера, она установит выбранный гипервизор, требуемую версию AOS и произведет первоначальную настройку.
</p>

<p>
  По умолчанию, узлы Nutanix поставляются с предустановленным AHV. Для установки другого гипервизора - используйте foundation.
  Примечание: Некоторые OEM поставщики могут поставлять оборудование с гипервизором отличным от AHV.
</p>

<p>
    На рисунке показана высокоуровневая архитектура Foundation:
</p>

<figure><img alt="Foundation - Architecture" src="imagesv2/Foundation/foundation_arch.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Архитектура</figcaption>
</figure>

<p>
  Начиная с версии 4.5, утилита Foundation встроена в CVM для упрощения конфигурации.
  Хранилище установщика - каталог для хранения загруженных образов, которые можно использовать как для первичной инсталляции, так и для расширения кластера.
</p>

<p>
  Утилита Foundation имеет компоненту Discovery Applet (доступна по <a href="https://portal.nutanix.com/#/page/foundation/list" target=blank>ссылке</a>), она отвечает за поиск узлов и позволяет пользователю выбрать к какому подключиться.
  Как только пользователь выбирает узел - данный компонента обеспечит перенаправления трафика с локального узла, где запущен аплет localhost:9442 IPv4 на IPv6-адрес CVM по порту 8000.
</p>

<p>
    На рисунке показана высокоуровневая архитектура данного апплета:
</p>

<figure><img alt="Foundation - Applet Architecture" src="imagesv2/Foundation/applet_arch.png">
<figcaption><span class="label">Рисунок. </span>Foundation - архитектура компоненты Discovery</figcaption>
</figure>

<p>
  Примечание: Компонента Discovery - средство обнаружения и прокси-сервер для службы Foundation, которая работает на всех узлах. Все задачи по конфигурации кластера выполняет Foundation.
</p>

<div data-type="note" class="note" id="pro-tip-05i5cRT9"><h6>Примечание</h6>
<h5>Совет от создателей</h5>

<p>Если вы находитесь в сети отличной от сети улов Nutanix вы можете подключиться напрямую к сервису Foundation, если CVM на котором он расположен выдан IPv4 адрес.</p>

<p>
  Для прямого подключения используйте &lt;CVM_IP&gt;:8000/gui/index.html
</p>
</div>


</section>


<h5>Поля ввода</h5>
<p>
    В интерфейсе Foundation представлены следующие поля для ввода пользовательских данных (см. ниже).  
    Стандартная инсталляция требует указать 3 IP-адреса на узел (гипервизор, CVM, IPMI/iDRAC).  
    Кроме того, нужно указать адрес кластера и адреса сервиса данных.  
</p>

<ul>
  <li>
    Cluster
    <ul>
      <li>
        Name
      </li>
      <li>
        IP*
      </li>
      <li>
        NTP*
      </li>
      <li>
        DNS*
      </li>
    </ul>
  </li>
  <li>
    CVM
    <ul>
      <li>
        IP per CVM
      </li>
      <li>
        Netmask
      </li>
      <li>
        Gateway
      </li>
      <li>
        Memory
      </li>
    </ul>
  </li>
  <li>
    Hypervisor
    <ul>
      <li>
        IP per hypervisor host
      </li>
      <li>
        Netmask
      </li>
      <li>
        Gateway
      </li>
      <li>
        DNS*
      </li>
      <li>
        Hostname prefix
      </li>
    </ul>
  </li>
  <li>
    IPMI*
    <ul>
      <li>
        IP per node
      </li>
      <li>
        Netmask
      </li>
      <li>
        Gateway
      </li>
    </ul>
  </li>
</ul>

<p>
  Примечание: Элементы помеченные '*' опциональны, но рекомендуемые
</p>

<section data-type="chapter">
<h2>Инсталляция платформы</h2>

<p>
  Первый шаг - подключение к веб-интерфейсу Foundation UI. Это можно сделать при помощи компоненты Discovery:
</p>

<figure><img alt="Foundation - Discovery Applet" src="imagesv2/Foundation/foundation_0.png">
<figcaption><span class="label">Рисунок. </span>Foundation - компонента Discovery</figcaption>
</figure>

<p>
  Если вы не можете найти необходимый узел - убедитесь, что вы в одной L2 сети.
</p>

<p>
  После подключения к Foundation выбранного узла вы увидите стандартный веб-интерфейс данного сервиса:
</p>

<figure><img alt="Foundation - Discovery Page" src="imagesv2/Foundation/foundation_1.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Страница обнаружения</figcaption>
</figure>

<p>
  Тут вы увидите найденные узлы и шасси в которых они размещены. Выберите нужные узлы, из которых хотите сформировать кластер, затем нажмите 'Next'.
</p>

<figure><img alt="Foundation - Node Selection" src="imagesv2/Foundation/foundation_2.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Выбор узлов</figcaption>
</figure>

<p>
  На следующей странице введите данные по кластеру и сетевые настройки:
</p>

<figure><img alt="Foundation - Cluster Information" src="imagesv2/Foundation/foundation_3.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Информация о кластере</figcaption>
</figure>

<figure><img alt="Foundation - Network Applet" src="imagesv2/Foundation/foundation_4.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Информация о сети</figcaption>
</figure>

<p>
  Проверьте верность введенных настроек и нажмите 'Next'
</p>

<p>
  Далее требуется ввести данные по узлу и его IP-адреса:
</p>

<figure><img alt="Foundation - Node Setup" src="imagesv2/Foundation/foundation_5.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Настройка узла</figcaption>
</figure>

<p>
  Вы можете заменить имя выданное по умолчанию, а так же IP-адреса:
</p>

<figure><img alt="Foundation - Hostname and IP" src="imagesv2/Foundation/foundation_6.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Имя узла и IP</figcaption>
</figure>

<p>
  Нажмите на кнопку 'Validate Network' для проверки верности введенных данных по сети.  Будет выполнена проверка IP-адреса на конфликты, а так же проверено подключение.
</p>

<figure><img alt="Foundation - Network Validation" src="imagesv2/Foundation/foundation_7.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Проверка сети</figcaption>
</figure>

<p>
  После того, как сеть будет проверена - ПО приступит к подготовке образов для инсталляции.
</p>

<p>
  Для обновления ПО Acropolis до последней версии - скачайте ее с пользовательского портала и загрузите Tarball в Foundation. 
  Как только вам станет доступна желаемая версия AOS - можно выбрать желаемый гипервизор.
</p>

<p>
  Гипервизор AHV встроен в образ Acropolis.  Все другие гипервизоры должны быть предварительно загружены. 
  Примечание: проверьте совместимость версий AOS и гипервизоров по (<a href="https://portal.nutanix.com/#/page/compatibilitymatrix" target=blank>ссылке</a>).
</p>

<p>
  После выбора образов нажмите 'Create':
</p>

<figure><img alt="Foundation - Select Images" src="imagesv2/Foundation/foundation_9.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Выбор образов</figcaption>
</figure>

<p>
  Если создание образа не требуется, можно также нажать кнопку "пропустить", чтобы пропустить процесс создания образа. Таким образом переустановка ПО и гипервизоров не будет выполняться, будет выполнена настройка кластера.
</p>

<p>
  Foundation начнет создание и разливку образов, а затем начнет инсталляцию и настройку кластера.
</p>

<figure><img alt="Foundation - Cluster Creation Process" src="imagesv2/Foundation/foundation_10.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Создание кластера выполянется</figcaption>
</figure>

<p>
  Как только настройка и инсталляция завершены вы увидите подтверждающую страницу:
</p>

<figure><img alt="Foundation - Cluster Creation Complete" src="imagesv2/Foundation/foundation_11.png">
<figcaption><span class="label">Рисунок. </span>Foundation - Создание кластера завершено</figcaption>
</figure>

<p>
  На этом этапе вы можете войти в на любую CVM или IP-адрес кластера и начать пользоваться платформой Nutanix!
</p>

</section>

</div>

<section data-type="afterword" id="afterword-622I2">
<h1>Послесловие</h1>

<p>Спасибо, что читаете Библию Nutanix!&nbsp; Следите за обновлениями и наслаждайтесь платформой Nutanix!</p>
</section>

    </div>
    <!-- START: Ken Chen 11-11-2015-->
    <script src="js/menu.js"></script>
    <!-- END: Ken Chen 11-11-2015-->
  </body>
</html>
